{"11pests1disease.pdf": ["Kentucky Pesticide Education Program\ncopyright \u00a9 2016 University of Kentucky Department of Entomology\nAgricultural Plant Diseases\nPlant Diseases\nA plant disease is any harmful condition that affects a plant\u2019s appearance or function. Common pathogens that cause diseases include: fungi,\nbacteria, and viruses. Some nematodes are plant disease agents. Temperature extremes or nutrient deficiencies are examples of disorders caused by\nnon-infectious factors. The disease triangle is a fundamental concept in plant pathology. Disease occurs only when all three sides of the triangle are present: a susceptible\nhost, apathogen (the agent that causes disease), and an environment favorable for disease to develop. Plant diseases are managed by manipulating\nthe disease triangle: the plant, the pathogen, and/or the environment. Infection begins when the pathogen enters the plant. The disease process starts when it arrives at a part of a plant where infection can occur. If\nenvironmental conditions are favorable, the pathogen begins to develop. The plant is diseased when it responds. Plants respond to disease in 3 main ways:\n1. overdevelopment of tissue - galls, swellings, or leaf curls;\n2.", "underdevelopment of tissue - stunting, lack of chlorophyll, or incomplete development of organs; or\n3. tissue death - blight, leaf spot, wilting, and cankers. Plant disease pathogens may be spread in many ways:\n\uf0b7 by wind;\n\uf0b7 rain;\n\uf0b7 animals;\n\uf0b7 soil;\n\uf0b7 nursery grafts;\n\uf0b7 vegetative propagation;\n\uf0b7 contaminated equipment and tools;\n\uf0b7 infected seed stock;\n\uf0b7 pollen;\n\uf0b7 dust storms;\n\uf0b7 irrigation water; and\n\uf0b7 people. ", "Infectious Organisms that Cause Diseases\nFungi are the most common infectious organisms causing plant disease. They do not have chlorophyll so they cannot make their own food.", "They must\nget it by living on another organism. Most fungi are beneficial. They contribute to soil fertility by releasing nutrients from dead plants or animals. Those that cause plant diseases are\nparasites on living plants. They may attack plants and plant products both above and below ground. Some fungal pathogens attack many plant\nspecies; others have to only one host species. Most fungi reproduce by spores, which function like seeds. Fungus spores are often microscopic and are produced in\ntremendous numbers. Often spores can survive for weeks, months, or even years without a host plant. Excessive water or high humidity is nearly\nalways needed for spore germination and active fungal growth. Spores can spread from plant to plant and crop to crop through wind, rain, irrigation water, insects, and by people through infected clothing and\nequipment. Fungal infections frequently are identified by the vegetative body of the fungus (mycelium) and the fruiting bodies that produce the spores. Often,\nthey are large enough to see. Symptoms of fungal infections include\n\uf0b7 soft rot of fruits,\n\uf0b7 plant stunting,\n\uf0b7 smuts,\n\uf0b7 rusts,\n\uf0b7 leaf spots,\n\uf0b7 wilting, or\n\uf0b7 thickening and curling of leaves. Powdery and downy mildew, smut, root and stem rots, and sooty and slime molds are examples of fungus diseases. Bacteria are single-celled organisms that usually reproduce by simple cell division. Some divide as often as every 30 minutes.", "They can build up\nquickly under warm, humid weather conditions. Leaf, growing shoots, and fruit diseases are the most common types in Kentucky. Bacteria can be carried from plant to plant in water droplets, by wind, rain splash, insects, or on equipment. They often survive between growing\nseasons in crop residue, in seeds or cuttings, or in weeds. ", "Leaf Diseases Caused By Fungi and Bacteria\nLeaf Spots\nBacteria or fungi can cause leaf spots that vary in size, shape, and color. Usually the spot has a distinct margin and may be surrounded by a yellow\nhalo. A fungal leaf spot nearly always has a growth of some type in the spot, particularly in damp weather. It may be a tiny pimple-like structure or a\nmoldy growth of spores.", "Often the structures are visible through a hand lens. Nearby diseased areas may join to form irregular \"blotches.\"\nphoto: Paul Bachi, University of Kentucky Research and Education Center, Bugwood.org\nSeptoria brown spot is a common fungal disease of soybeans. It causes small angular red-brown spots to develop on upper and lower surfaces of\ntrifoliate leaves 2 to 3 weeks after planting. Numerous spots will cause leaves to yellow and drop. The disease will develop many irregular, tan lesions\non trifoliate leaves that later turn dark brown. Individual spots will frequently coalesce to form large blackish-brown blotches. Defoliation from the bottom to the top of severely diseased trifoliate leaves is common during wet seasons. Early season brown spot will appear\nannually in almost every field in Kentucky. Late-season brown spot is much more variable in occurrence and severity. The fungus survives from season to season in crop debris and seed. Warm, moist weather promotes the sporulation of the fungus; the spores are\nspread by wind and rain. Hot, dry weather can stop disease development. Leaf Blights\nLeaf blights generally affect larger leaf areas and are more irregular than leaf spots. photo: Margaret McGrath, Cornell University, Bugwood.org\nNorthern corn leaf blight (NCLB), caused by a fungus, first develops on the lower leaves of corn plants that are from waist to shoulder high. The\ntelltale sign of northern corn leaf blight is the 1-to-6 inch long cigar-shaped gray-green to tan-colored lesions on the lower leaves. As the disease\ndevelops, the lesions spread to all leafy structures.", "Wet weather and moderate temperatures favor NCLB. Symptoms can be confused with bacterial wilt, especially late in the season. ", "NCLB will be more severe in fields with corn following corn under reduced tillage. Other hosts include sorghum, Johnsongrass, and some other\ngrass species.", "The pathogen overwinters in plant debris and is transferred by wind to new plants.Severe yield loss can occur when leaves become\nblighted during early grain fill. Rusts\nRust fungi often produce bright yellow, orange-red, reddish-brown or black \"pustules\". The pustules are usually raised above the leaf surface. Rust can\nbe rubbed off the leaf surface. photo: Donald Groth, Louisiana State University AgCenter, Bugwood.org\nWheat leaf rust first appears as small yellow flecks on the upper leaf surface which turn to orange pustules. The disease reduces plant vigor, seed fill,\nand root growth.", "Losses are greatest when the disease is active before or during flowering. Leaf rust is a potentially explosive disease; it requires a\nvery short time to go from low to epidemic levels on a susceptible variety. In severe cases, infected leaves wither and die rapidly. Warm\ntemperatures and high humidity or rain favors its development. Rust is present almost every year in some part of Kentucky, however most years it\ndevelops too late to cause extensive damage. Powdery Mildews\nPowdery mildews are diseases caused by some closely related fungi. The mildew can be diagnosed by a grayish white powder mat leaves and in some\ncases stems. Affected leaves usually turn yellow, wither and die rapidly. photo: Gerald Holmes, California Polytechnic State University at San Luis Obispo, Bugwood.org\nPowdery mildew of wheat is easily diagnosed by the white, powdery patches that form on the upper surfaces of lower leaves and stems. The\npatches turn a dull gray-brown and may have small black embedded specks. This disease can spread to all aboveground parts of the plant. The fungus responsible for powdery mildew can persist between seasons in infested wheat stubble and in overwintering wheat. Spores then infect\nplants during periods of high moisture (not necessarily rain) and cool to moderate temperatures. Low light intensity, which accompanies dreary\nweather, and a dense, lush crop canopy favor this disease. Hot daytime temperatures (80\u00b0F plus) and moderate nighttime temperatures will stop\npowdery mildew development. ", "Downy Mildews\nphoto: Gerald Holmes, California Polytechnic State University at San Luis Obispo, Bugwood.org\nDowny mildews are caused by organisms similar to fungi. Colonies often appear first on the underside of leaves, and they sometimes have a bluish\ntinge. Many can grow systemically throughout the plant. Downy mildews are generally favored by cool temperatures ( 58-72\u00b0 F) and relative\nhumidity above 85% at the leaf surface. Blue mold of tobacco is a downy mildew disease. Deformed plant growth (\"crazy top\") may result from\ndowny mildew as in the case of sorghum downy mildew of corn or grain sorghum. Wilts, Root and Crown Rots\nThe main symptoms of these three diseases are wilting, stunting, and death. The causal organisms are usually soil-borne, that is they are already\npresent in the soil when the host crop is planted.", "Most other diseases are usually spread through the air. Some of these diseases may be controlled\nthrough the use of soil fungicides and/or soil fumigants but most are controlled with resistant varieties and cultural practices. Good controls are not\navailable for many of these diseases. Wilts\nMost wilt diseases are caused by fungi -- Fusarium and Verticillium -- and a bacterium, Erwinia. Each parasite causes wilts on a wide range of crop\nplants. A light to dark brown streaking often can be seen when the stem of a diseased plant is cut lengthwise. However, it is often difficult to determine\nwhich of these wilt diseases a plant may have. Plants with Fusarium or Verticillium wilt grow slower than healthy plants and may show stunting effects before wilting occurs. The lower leaves\nusually turn yellow and wilt first, then yellowing and wilting slowly progress up the plant. It may take several days to a few weeks from first wilting\nand plant death.", "photo: J.K. Pataky, University of Illinois at Urbana-Champaign, Bugwood.org\nStewart\u2019s wilt of corn is a bacterial disease with two phases. The seedling wilt stage occurs when young plants have a systemic infection. The leaf\nblight stage occurs when plants are infected after the seedling stage. The bacterium spends the winter in the corn flea beetle. The bacterium enters the\nplant at wounds made when an infected flea beetle feeds on a susceptible plant. ", "Stewart's wilt is controlled effectively by planting resistant corn hybrids. Resistance restricts the movement of bacteria in the vascular system of\nplants and prevents plants from becoming infected systemically. Most field corn hybrids are moderately to highly resistant; sweet corn hybrids range\nfrom highly susceptible to resistant. Crown Rots\nphoto: Ernesto Moya, Bugwood.org\nCrown rots include causal organisms that attack the plant at/or near the soil line. Crown rots are called various names such as collar rot, stem blight,\nstalk rot and southern blight. Affected plants are generally, at first, unthrifty with leaves smaller and lighter green than normal. Leaves usually turn\nyellow and, in advanced stages of disease, wilt and die. The crown or base of the stem will be water-soaked, discolored or decayed. With some\ndiseases, this area may dry rot and become shredded. A moldy growth and various colored fungus fruiting bodies often form in this diseased area. Most crops are affected by one or more of this type of disease. Root Rots\nphoto: Mary Burrows, Montana State University, Bugwood.org\nAbove-ground symptoms of root rot diseases are variable. Some plants may show wilt type symptoms and die rapidly; others may become yellow,\nstunted, slow growing and may not die for some time after the first symptoms are seen. Roots are reduced in size and will be light brown to black in\ncolor, with both taproots and feeder roots decayed.", "Most plants are susceptible to root rots. A great variety of fungi can infect the crown of the alfalfa plant, which is the portion of the taproot just below the soil line. The alfalfa plant stores food\nreserves for winter in the crown. Crown rot diseases interfere with this process, making plants more susceptible to winter injury. ", "Seed and Seedling Diseases\nSeed and seedling diseases occur on plants from after germination until about 1 or 2 weeks after seedling emergence. Fungi that are on the seed at\nplanting or are present in the soil are the cause. They often are responsible for poor emergence and thin stands. photo: Paul Bertrand, University of Georgia, Bugwood.org\nIf the disease occurs before emergence, the seedling may rot before it has a chance to get out of the soil. This is referred to as pre-emergence damping-\noff. After emergence, seedling stems may be attacked at or below the soil line in what is termed post-emergence damping-off. Two general symptoms are brown to red-brown or black cankers at the soil line and a soft watery rot. If the canker girdles the stem, the seedling falls\nover and dies. The plant may continue to live but will be stunted with partial girdling, In the second instance, the soft watery rot continues until the\nentire seedling decays.", "Seed and seedling diseases are most common in cool, wet soils. They are controlled by planting crops in warm soil, by fungicide seed treatments and\nby use of in-furrow, broadcast, or band-applied fungicides. Smuts\nphoto: Clemson University - USDA Cooperative Extension Slide Series, Bugwood.org\nSmut disease fungi attack grasses and cereal tops. The most destructive smuts attack small grains, often causing the kernels of grain in the head to be\nreplaced by a mass of dark powdery fungus spores. Corn smut disease results in a swelling of the affected plant part with the galls produced on the\nplant containing a mass of dark, powdery spores. ", "Viruses\nViruses are too small to be seen with a microscope. They are generally recognized by their effects on plants. There can be a variety of responses:\nstunted growth; change in plant color; abnormal formation of infected roots, stems, leaves, or fruit. Mosaic diseases, characterized by light and dark\nblotchy patterns, usually are caused by viruses. It can be difficult to distinguish between diseases caused by viruses and those caused by fungi and bacteria.", "A positive diagnosis requires\nsophisticated testing, such as inoculating indicator plants and observing the results or using specifically identified antibodies to test for the presence of\nthe organism. Viruses depend upon living organisms for food and reproduction; they cannot exist very long outside a host. Viruses are commonly spread from\nplant to plant by mites, aphids, leafhoppers, or whiteflies. A few are spread in the seeds of the infected plant. photo: Mary Ann Hansen, Virginia Polytechnic Institute and State University, Bugwood.org\nMaize Dwarf Mosaic Virus (MDMV) and Maize Chlorotic Dwarf Virus (MCDV) can survive between crops in underground rhizomes. MCDV is\nmoved between corn, Johnsongrass, and sorghum by leafhoppers. Symptoms only appear in corn. MDMV, carried by aphids, infects over 250 species\nof grasses. Nematodes\nNematodes are small, usually microscopic roundworms with mouthparts like hollow needles \u2013 called stylets. Stylets are used to puncture and feed on\nthe contents of plant cells. Nematodes may develop and feed inside or outside of a plant.", "They are easy spread on footwear, tools, and equipment. The life cycle of a nematode includes an egg, several larval stages, and an adult. Most larvae look like small adults. In unfavorable conditions, females\nof some species form inactive, resistant forms called cysts. The cyst is the hard, leathery, egg-filled body of the dead female, which is difficult to\npenetrate with pesticides. A cyst may protect eggs for as long as 10 years. Typical aboveground symptoms are stunting, yellowing, loss of vigor, general decline, and eventual death of plants. Since many other problems can\ncause symptoms root and soil samples need to be examined when nematode injury is suspected. Nematodes are controlled with cultural practices such\nas crop rotation, resistant varieties and nematicides. ", "Cysts (bodies of female nematodes) are visible on diseased roots from four weeks after planting through the rest of the season\nSCN-affected plants (left) vs. unaffected plants (right)\nphotos: Paul Bachi, University of Kentucky Research and Education Center, Bugwood.org\nIn Kentucky, the soybean cyst nematode (SCN) is the most damaging nematode disease. It feeds on the roots of soybean and other host plants. Feeding removes nutrients and disrupts nutrient and water movement. An infestation also reduces the production of nitrogen-fixing nodules and\nencourages other root diseases. SCN in Kentucky\n\uf0b7 Causes up to 30% yield loss without any obvious problem until harvest. \uf0b7 Eggs can survive in the soil for many years even without host plants.", "\uf0b7 Reproduction occurs on resistant soybeans. \uf0b7 Moves every way that soil moves. \uf0b7 SCN can be present in a field for many years before it is detected. \uf0b7 Symptoms may look like those due to other causes. Diagnosis of Plant Diseases\nA correct diagnosis is the first step in disease management. To recognize a disease condition, you must know the plant's normal growth habits. When\nyou are trying to identify the cause of a plant disease, you need to look for symptoms - the host plant's reaction to the disease agent, and signs - visible\npresence of the disease agent. Many different plant diseases cause similar symptoms. Different pathogens and agents that are not pathogens can cause leaf spots, wilts, root galls,\nor stunted growth. For example, similar symptoms may be a result of mechanical injury, improperly applied fertilizers and pesticides, or frost. Often,\nthe only way to pinpoint the cause is to find the observable signs that the particular disease agent is present -such as fungal spores and mycelium or\nbacterial ooze. "], "2404.03622v3.pdf": ["Mind\u2019s Eye of LLMs: Visualization-of-Thought Elicits\nSpatial Reasoning in Large Language Models\nWenshanWu\u2020 ShaoguangMao\u2020 YadongZhang\u2020,\u2021,\u2217\nYanXia\u2020 LiDong\u2020 LeiCui\u2020 FuruWei\u2020\n\u2020MicrosoftResearch \u2021EastChinaNormalUniversity\nAbstract\nLargelanguagemodels(LLMs)haveexhibitedimpressiveperformanceinlanguage\ncomprehension and various reasoning tasks. However, their abilities in spatial\nreasoning, a crucial aspect of human cognition, remain relatively unexplored. Humanpossessaremarkableabilitytocreatementalimagesofunseenobjectsand\nactionsthroughaprocessknownastheMind\u2019sEye,enablingtheimaginationof\ntheunseenworld. Inspiredbythiscognitivecapacity,weproposeVisualization-\nof-Thought(VoT)prompting. VoTaimstoelicitspatialreasoningofLLMsby\nvisualizingtheirreasoningtraces,therebyguidingsubsequentreasoningsteps. We\nemployedVoTformulti-hopspatialreasoningtasks,includingnaturallanguage\nnavigation,visualnavigation,andvisualtilingin2Dgridworlds. Experimental\nresultsdemonstratedthatVoTsignificantlyenhancesthespatialreasoningabilities\nofLLMs. Notably,VoToutperformedexistingmultimodallargelanguagemodels\n(MLLMs)inthesetasks. WhileVoTworkssurprisinglywellonLLMs,theability\ntogeneratementalimagestofacilitatespatialreasoningresemblesthemind\u2019seye\nprocess,suggestingitspotentialviabilityinMLLMs. Pleasefindthedatasetand\ncodesinourprojectpage. Behindyouisthemarket.", "Verbal\nTurnrighttothecemetery. Mental Images\nTurnlefttothelibrary. Mental Images\nGostraighttothepostoffice. \u2026 garage post office\n\u2026 cinema Chemist\u2019s library Text \u2026\nMind\u2019s Eye market cemetery Mind\u2019s Eye\nVisual\nMind\u2019s Eye of Humans Mind\u2019s Eye of LLMs\nConventional Prompting\nInput Output\nChain-of-Thought\nInput Thought Thought \u2026 Output\nVisualization-of-Thought\nInput Thought Thought \u2026 Output\nVisualization Visualization\nFigure 1: Humans can enhance their spatial awareness and inform decisions by creating mental\nimagesduringthespatialreasoningprocess. Similarly,largelanguagemodels(LLMs)cancreate\ninternalmentalimages. WeproposetheVoTpromptingtoelicitthe\"mind\u2019seye\"ofLLMsforspatial\nreasoningbyvisualizingtheirthoughtsateachintermediatestep. \u2217ContributionduringinternshipatMicrosoftResearch. 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202\ntcO\n32\n]LC.sc[\n3v22630.4042:viXra\n", "1 Introduction\nRecently, largelanguagemodels(LLMs)[BCE+23,BMR+20,TLI+23,JSM+23]haveachieved\nremarkableperformanceonvariouslanguage-relatedtasks. However,despitetheirsuccessinmath\nreasoning [KGR+23], common sense reasoning [LKH+22], and other reasoning tasks such as\nsymbolicreasoningorlogicreasoning[KGR+23], theirabilitiesinspatialreasoningstillremain\nunderexplored[RFD+21,YBL+23,MHV+24]. Spatialreasoningisanessentialfunctionofhumancognition,allowingustointeractwiththeenvi-\nronment. Itfacilitatestasksthatrequireunderstandingandreasoningaboutthespatialrelationships\nbetween objects and their motions. The spatial reasoning of language models largely relies on\nlanguagetoreasonaboutspatialinformation,whereashumancognitivecapabilitiesextendfarbeyond\nverbal reasoning. Humans can not only create task-relevant abstract representations from visual\nperception [BK18, KC22], but also imagine unseen scenes through their mind\u2019s eye. It remains\na research topic called mental image [She78] in domains of neuroscience, philosophy of mind,\nandcognitivescience. Buildinguponthiscognitivefunction, humansfacilitatespatialreasoning\nbymentalimagemanipulation,suchasnavigation[Tol48],mentalrotation[SM71],mentalpaper\nfolding[SF72],andmentalsimulation[MK09]. Figure1illustratesthehumanprocessinvolvedin\nanavigationtask. Humansenhancetheirspatialawarenessandinformtheirdecisionsbycreating\nmentalimagesofaroute,utilizingvarioussensoryinputssuchasnavigationinstructionsoramap\nimage. Subsequently,theysimulaterouteplanningthroughthemind\u2019seye. Inspiredbythiscognitivemechanism, weconjecturethatLLMspossesstheabilitytocreateand\nmanipulatementalimagesinthemind\u2019seyeforspatialreasoning. AsillustratedinFigure1,LLMs\ncould potentially process and understand spatial information in various formats. They might be\ncapableofvisualizinginternalstatesandmanipulatingthesementalimagesthroughtheirmind\u2019seye,\ntherebyguidingsubsequentreasoningstepstoenhancespatialreasoning. Therefore,weproposethe\nVisualization-of-Thought(VoT)promptingtoelicitthisability. ThismethodleverageLLMsto\nvisualizetheirreasoningstepsandinformsubsequentsteps,implementingtheconceptofvisuospatial\nsketchpad [Bad92]. VoTadoptszero-shotpromptinginsteadofrelyingonfew-shotdemonstrations\nor text-to-image visualization with CLIP [RKH+21]. This choice stems from LLMs\u2019 ability to\nacquirevariousmentalimagesfromtext-basedvisualart[SB14,SMM21,Reg19]. ToevaluatetheeffectivenessofVoTinspatialreasoning,weselectedthreetasksthatrequirespatial\nawarenessinLLMs,includingnatural-languagenavigation[YBL+23],visualnavigation,andvisual\ntiling. Thesetasksrequireanunderstandingofspace,direction,andgeometricshapereasoning. To\nemulatehuman-likemultisensoryperception,wedesigned2Dgridworldsusingspecialcharacters\nasenrichedinputformatsfortheLLMsinvisualnavigationandvisualtilingtasks. Wecompared\ndifferentmodels(GPT-4,GPT-4V)andpromptingtechniquesacrossthesethreetasks. Thefindings\nrevealthattheVoTpromptingproposedinthispaperconsistentlyinducesLLMstovisualizetheir\nreasoning steps and inform subsequent steps. Consequently, this approach achieved significant\nperformanceimprovementsonthecorrespondingtasks.", "Themaincontributionsofthispaperinclude:\n1. We shed light on LLMs\u2019 mental image for spatial reasoning from a cognitive perspective,\nconductingquantitativeandqualitativeanalysesonthemind\u2019seyeofLLMsanditslimitations. We\nalsoexplorecuesabouttheoriginofthisgeneralizedabilityfromcodepre-training. 2. Wedeveloptwotasksof\"visualnavigation\"and\"visualtiling\", alongwithcorresponding\nsyntheticdatasets,emulatingvarioussensoryinputsforLLMs. Thesetasksarestructuredtosupport\nvaryinglevelsofdifficulty,offeringawell-designedtestbedfortheresearchonspatialreasoning.", "3. WeproposeVisualization-of-Thought(VoT)promptingtoelicitthemind\u2019seyeofLLMs\nforspatialreasoningandprovideempiricalevaluationsonthreetasks. Experimentresultsprovethe\neffectivenessofVoTpromptingcomparedwithotherpromptingmethodsandexistingMLLMs. This\nabilitytogeneratementalimagestofacilitatespatialreasoningresemblesthemind\u2019seyeprocess,\nsuggestingitspotentialviabilityinMLLMs. 2\n", "2 SpatialReasoning\nSpatialreasoningreferstotheabilitytocomprehendandreasonaboutthespatialrelationshipsamong\nobjects,theirmovements,andinteractionswiththeenvironment. Thisskillisvitalforawiderangeof\nreal-worldapplicationssuchasnavigation,robotics,andautonomousdriving. Thesefieldsnecessitate\nactionplanningbasedonvisualperceptionandaconcreteunderstandingofspatialdimensions. Althoughseveraltasksanddatasets[WBC+15,SZL22,MK22,LB18,RAB+20]havebeendevel-\nopedtoprobethespatialsemanticsembeddedintext,existingresearcheffortsoftenfocusonhow\nspatialtermsarelinguisticallystructured. Recently,significantachievementsandimpressiveresults\nhavebeenachievedinthesebenchmarksbyconvertingspatialtermstologicalformsthroughLLMs\nandadoptinglogicprogramming[YIL23]. Thisimpliesthatexcellinginthesetasksdoesnotnec-\nessarilyequatetoagenuineunderstandingofspatialinformationbyLLMs,nordoesitprovidean\naccuratemeasureoftheirspatialawareness. Spatialawarenessinvolvesunderstandingspatialrelationships,directions,distances,andgeometric\nshapes,allofwhichareessentialforactionplanninginthephysicalworld. Toevaluatethespatial\nawarenessandspatialreasoningabilitiesofLLMs,wehaveselectedtasksthattestnavigationand\ngeometricreasoningskills,includingnaturallanguagenavigation,visualnavigationandvisualtiling. 2.1 NaturalLanguageNavigation\nNatural language navigation task [YBL+23] was inspired by prior research on human cogni-\ntion[GDB17]presentingparticipantswithsequentialtransitionssampledfromagraphstructure. Inthiscontext,asquaremapisdefinedbyasequenceofrandomwalkinstructionsandassociated\nobjectsateachlocation,denotedasW ={(l ,o ),(l ,o ),...,(l ,o )}.", "GivenasquaremapW,\n1 1 2 2 n n\nandsequenceofnavigationinstructionsI = {i ,...,i },thetaskforthemodelistoidentifythe\n1 k\nassociatedobjecto\u2208W atthespecifiedlocationlwhichisdeterminedbythenavigationinstructions,\nasdetailedinEquation1andexemplifiedinAppendixB.2. o\u223cp(o\u2208W|W ={(l ,o ),(l ,o ),...,(l ,o )},I) (1)\n1 1 2 2 n n\n2.2 VisualNavigation\nVisualnavigationtaskpresentsasynthetic2DgridworldtoLLM,challengingittonavigateusing\nvisualcues. Themodelmustgeneratenavigationinstructionstomoveinfourdirections(left,right,\nup,down)toreachthedestinationfromthestartingpointwhileavoidingobstacles. Thisinvolvestwo\nsub-tasks: routeplanningandnextstepprediction,requiringmulti-hopspatialreasoning,whilethe\nformerismorecomplex. TaskinstructionsareavailableinFigure6inappendix. Formulation The model is presented with a grid map M consisting of k consecutive edges\nE ={e(s ,s ),e(s ,s ),\u00b7\u00b7\u00b7 ,e(s ,s )},wherethestartingpointanddestinationares ands\n0 1 1 2 k\u22121 k 0 k\nrespectively,asshowninFigure2. Routeplanningtaskistogenerateasequenceofcorrectdirections\nD = {d(s ,s ),d(s ,s ),\u00b7\u00b7\u00b7 ,d(s ,s )},asdefinedinEquation2. GivenM andtnavigation\n0 1 1 2 k\u22121 k\ninstructions D = {d(s ,s ),\u00b7\u00b7\u00b7 ,d(s ,s )}, next step prediction task is to identify the\nt,0<t<k 0 1 t\u22121 t\ncorrectdirectiond(s ,s )ofthenextstep,asdefinedinEquation3. t t+1\nD \u223cp({d(s ,s ),d(s ,s ),\u00b7\u00b7\u00b7 ,d(s ,s )}|M) (2)\n0 1 1 2 k\u22121 k\n(a)k=2 (b)k=3 (c)k=4 (d)k=5 (e)k=6 (f)k=7\nFigure2: Examplesofanavigationmapunderdifferentsettingsofk,withemojiofhouseindicating\nthestartingpoint,andemojiofofficeindicatingthedestination. 3\n", "d\u223cp(d(s ,s )|M,D ) (3)\nt t+1 t,0<t<k\nImplementation Thenavigationmap\u2019sunderlyinggraphissemi-Eulerian,alternatingbetween\nhorizontalandverticaledges,with2k+1possiblespatialconfigurationsforak-hopnavigationmap. Foreachmapandsetofk navigationinstructions,k\u22121question-and-answer(QA)instances,i.e. \"whatisthenextstep?\"arecreated.", "FurtherimplementationdetailsareinAppendixA.1. 2.3 VisualTiling\nIntroduced by [Gol66], polyomino tiling is a classic spatial reasoning challenge. We extend this\nconcepttotesttheLLM\u2019sabilitytocomprehend,organize,andreasonwithshapesinaconfinedarea,\nthusenhancingtheevaluationofspatialreasoningskills. AsdepictedinFigure3,thetaskinvolvesa\nrectanglewithunfilledcellsandvariouspolyominopieces,liketheI-tetrominomadeoffouraligned\nsquares. Themodelmustselecttheappropriatepolyominovariant,suchaschoosingtheorientation\nfortheI-tetromino,tosolvetheQApuzzle. TaskinstructionsareprovidedinFigure7inappendix. Formulation The model is presented with a rectangle R masked with k unique polyominoes\nMP = {mp ,\u00b7\u00b7\u00b7 ,mp },2correspondingvariantsofeachpolyominov = {v ,v },anda\n1 k i<=k i1 i2\npolyomino query q \u2208MP. Visual tiling task is to identify the correct variant of q, as defined in\nEquation4. v \u223cp(v |R,{mp ,\u00b7\u00b7\u00b7 ,mp },{v ,v \u00b7\u00b7\u00b7 ,v ,v },q) (4)\nq 1 k 11 12 k1 k2\nImplementation The dataset comprises valid spatial arrangements generated through existing\nalgorithms[ES03,GN07],withrandommaskingofpolyominoestocreateQApuzzles. Detailsare\nprovidedinAppendixA.2. (a)Fit2piecesintoamaskedrectangle (b)Fit3piecesintoamaskedrectangle\nFigure 3: Example of visual tiling with masked polyomino pieces. Variants of those polyomino\npiecesincludingrotationandreflectionarenotshowninthisfigure. 3 Visualization-of-ThoughtPrompting\nConsideringthewayhumansprocessspatialinformationduringtaskslikenavigation,it\u2019scommon\ntocreatementalimages,suchasmaps,toenhancespatialawarenessorsimulatingmovementsto\ninformdecision-making. OurobjectiveistoelicitthespatialawarenessofLLMsandgroundtheir\nreasoningbyvisualizingtheconsequenceoftheirintermediatereasoningsteps. WeintroduceVisualization-of-Thought(VoT)prompting: \"Visualizethestateaftereachrea-\nsoning step.\" This new paradigm for spatial reasoning aims to generate reasoning traces and\nvisualizationsinaninterleavedmanner. QualitativeresultsofthisapproacharepresentedinFigure4.", "Weusep todenoteapre-trainedLMwithparameters\u03b8,x,y,ztodenotealanguagesequence,and\n\u03b8\nvtodenoteavisualizationsequenceintextform. Inamulti-hopspatialreasoningtaskwithinputx,\nCoTpromptinggeneratesaseriesofintermediatestepsz ,\u00b7\u00b7\u00b7 ,z ,eachstepz \u223cp (z |x,z )\n1 n i \u03b8 i 1\u00b7\u00b7\u00b7i\u22121\nis sampled sequentially, followed by the output y \u223c p (y|x,z ). As shown in Figure 1, VoT\n\u03b8 1\u00b7\u00b7\u00b7n\npromptingenhancesthisprocessbyaddingavisuospatialsketchpadtoeachintermediatestepz ,\ni\nthenthesubsequentstepz issampledconditionedonpriorstepsz andvisualizationsv . i+1 1\u00b7\u00b7\u00b7i 1\u00b7\u00b7\u00b7i\nAs defined in the Equation 5 and 6, it forms interleaved reasoning traces and visualizations. A\nqualitativecomparisonbetweenoutputsofVoTandCoTisprovidedinFigure8ainappendix. v \u223cp (v |prompt ,x,z ,v ) (5)\ni \u03b8 i VoT 1\u00b7\u00b7\u00b7i 1\u00b7\u00b7\u00b7i\u22121\n4\n", "Visual Navigation Visual Tiling Natural Language Navigation\nYou have been given a 3 by 3 square grid. Initially,\nyou are at the bottom-left corner\u2026find a cassette\nplayer\u2026go right\u2026a wool, go right\u2026a conch, go\nup\u2026a moving van, go left\u2026a confectionery store,\ngo left\u2026a pot pie, go up\u2026a siamang, go right\u2026a\nblack-and-white colobus, go right\u2026a minivan. Now you have all the information on the map. You\nProvided: I T L start at where the cassette player is located, then\nTo fit all the provided polyominoes into the you go right by one step, go right\u2026go up\u2026go\nStarting from , provide the steps to navigate empty squares, what's the correct variation of left\u2026go left\u2026go up\u2026go right\u2026go down by one\nto . Tetromino T?", "step. What will you find? Visualize the state after each reasoning step. Visualize the state after each reasoning step. Visualize the state after each reasoning step. Analyze I Analyze L Analyze T\n1. Move right 2. Move down 3. Move left \u2026\n4. Move down 5. Move left 6. Move down 1. Place I 2.", "Place L 3. Place T\nFigure4: ExamplesofVoTpromptinginthreetasks,whereLLMgenerates2Dgridsastext-form\nmentalimages. Thegeneratedreasoningtracesandvisualizationsformaninterleavedsequenceto\ntrackthestateovertime. The2Dgridsintheinputandresponsesarecomposedofspecialcharacters. FullresponsescouldbefoundinAppendixB. z \u223cp (z |prompt ,x,z ,v ) (6)\ni+1 \u03b8 i+1 VoT 1\u00b7\u00b7\u00b7i 1\u00b7\u00b7\u00b7i\nThisreasoningparadigmenablesLLMswithvisualstatetracking. Weintroducetheconceptofa\nstate,denotedass =[x,z ,v ]representingapartialsolutionatstepiwiththeinput,the\ni 1\u00b7\u00b7\u00b7i 1\u00b7\u00b7\u00b7i\u22121\nsequenceofintermediatestepsz andthesequenceofvisualizationsv . 1\u00b7\u00b7\u00b7i 1\u00b7\u00b7\u00b7i\u22121\nv \u223cp (v |prompt ,x,z ,v )\ni \u03b8 i VoT 1\u00b7\u00b7\u00b7i 1\u00b7\u00b7\u00b7i\u22121\n(7)\n\u223cp (v |prompt ,s )\n\u03b8 i VoT i\nAs shown in Equation 7, visual state tracking is implemented by generating the visualization v\ni\nasrepresentationoftheinternalstates aftereachreasoningstepz (e.g.v couldbeagridofthe\ni i i\nnavigationmapmarkedwithpathorafilledrectangle).Groundedbythevisualstatetrackingsequence,\nthesubsequentstateisderivedbys \u223cp (s |prompt ,x,s ,v ). Thismechanismallows\ni+1 \u03b8 i+1 VoT i i\nforthederivationofsubsequentstates,reflectingspatiotemporalcausalityandenhancingthespatial\nreasoningcapabilitiesofLLMsinagroundedcontext. 4 Experiment\n4.1 Setup\nForthevisualtaskswhereacounterpartimageexistsforeachtextinput,weconductadditionalexper-\nimentswithamultimodalmodel. Specifically,weadoptGPT-4[OA+23]andGPT-4Vision[Ope23]\nvia Azure OpenAI API as they\u2019re state of the art LLM and multimodal model respectively. API\nsettingsaretemperature0asgreedydecodingandtopp1,withmodelversionsof1106-previewand\nvision-preview. Forallexperimentsweadoptzero-shotprompting. DependingonwhethertheLLMisexplicitlypromptedtovisualizeintermediatesteps,weexperiment\nwiththreesettingsofGPT-4,includingzero-shotCoTprompting(GPT-4CoT),GPT-4w/oVizwhere\nvisualizationisexplicitlydisabledduringreasoning,andVoTprompting(GPT-4VoT).Additional\nsettingofGPT-4VisionwithcounterpartimageinputisGPT-4VCoT.Promptsareasfollowing:\n5\n", "\u2022 GPT-4CoT:Let\u2019sthinkstepbystep. \u2022 GPT-4w/oViz: Don\u2019tusevisualization. Let\u2019sthinkstepbystep. \u2022 GPT-4VCoT:Let\u2019sthinkstepbystep. \u2022 GPT-4VoT:Visualizethestateaftereachreasoningstep. TaskinstructionsandexamplescouldbefoundinAppendixB. 4.2 Dataset\nNaturalLanguageNavigation Wegenerate200squaremapsofsize3x3whichisdescribedby9\nlandmarksinsnakeordertraversal,andasetofnavigationinstructions. VisualNavigation Wegenerate496navigationmapsand2520QAinstancesintotal,covering\nvariousmapsizes,upto7\u00d79and9\u00d77.", "ThedatadistributionisprovidedinTable4inappendix. Visual Tiling We first generate multiple unique configurations to fill a 5 x 4 rectangle with 5\npolyominopiecesincludingtwoItetrominoes,twoTtetrominoesandoneLtetromino. Thenwe\nrandomlymaskedtwoorthreepiecesofdifferenttypesandgenerateQAinstanceforeachmasked\npieces. ThetotalnumberofQAinstancesis796,andweshowdatasetdetailsinTable5inappendix. 4.3 Metric\nWeextracttheanswerfrommodeloutputbypatternmatching. Fortasksexceptforrouteplanning,\nwe calculate accuracy by Equation 8. We adopted sub-string matching\u2020 as f to determine\ncorrect\ncorrectness. n\n(cid:88)\nacc= f (extracted_answer,ground_truth)/n (8)\ncorrect\ni\nFor the route planning task which predicts a sequence of navigation instructions, we reject any\nsequencesexceeding100instructions,consideringthemtoberandomguesses. Wethennormalize\nthenavigationinstructionsbyexecutingeachnavigationinstruction. Thoseinstructionswhichviolate\nnavigationruleswillbeignored. Thelengthtofnormalizedinstructionsequenceisconsideredasthe\ntemporaldistanceagainstthestartingpoint. Giventheground-truthofknavigationinstructions,the\ncompletingrateofrouteplanningist/k. Forthedatasetofnmaps,wereporttwometricsincluding:\n1. Average completing rate:\n(cid:80)nt\n/k /n. Average completing rate among all instruction\ni i i\nsequences,reflectingLLM\u2019seffectivenessofrouteplanning. 2. Successrate:\n(cid:80)n(t\n==k )/n. Thismetricrepresentstheproportionofinstructionse-\ni i i\nquenceswitht=k,i.e.,reachingthedestination. 4.4 Results\nAs illustrated in Table 1, GPT-4 VoT significantly outperforms other settings in all tasks across\nall metrics. The significant gap when comparing GPT-4 VoT with GPT-4V CoT and GPT-4 w/o\nVizdemonstratesthateffectivenessofvisualstatetracking,whichallowsLLMsvisuallyinterpret\ntheiractionswithinangroundedworld. Andinthenaturallanguagenavigationtask,GPT-4VoT\noutperformsGPT-4w/oVizby23.5%. Inthevisualtasks,thenoticeableperformancegapbetween\nGPT-4CoTandGPT-4VCoTindicatesthatLLMgroundedwith2Dgridcouldpossiblyoutperform\naMLLMinchallengingspatialreasoningtasks. Ontheotherhand,performanceofGPT-4VoTisstillfarfromperfectinalltasks,especiallyinthe\nmostchallengingrouteplanningtask. Despitethesetasksarerelativelyeasyforhumans,performance\nof LLMs drops significantly as task difficulty increases. Details on performance trends across\ndifficultylevelsareprovidedinfigure9andtable6inappendix. \u2020Weusethistermforsimplicity.Innaturallanguagenavigationtasks,LLMsoftenoutputadditionalwords\nintheextractedanswerbesidestheexpectedobjectname.", "Forexample,\"Answer: Youwillfind...\". Inthis\ncase,sub-stringmatchingisadoptedwithoutaffectingthecorrectness.Otherwise,exactmatchingisadoptedfor\nmultiplechoicequestionsinvisualtasks. 6\n", "VisualNavigation\nNatural-Language\nSettings VisualTiling\nRoutePlanning NextStep Navigation\nPrediction\nCompletingRate SuccRate\nGPT-4CoT 37.02 9.48 48.61 54.15 54.00\nGPT-4w/oViz 37.17 10.28 48.49 46.98 35.50\nGPT-4VCoT 33.36 5.65 46.59 49.62 /\nGPT-4VoT 40.77 14.72 55.28 63.94 59.00\nTable 1: Performance of different GPT-4/4V settings in all tasks. Underline denotes statistical\nsignificancewithp<0.05whencomparingGPT-4VoTagainstallbaselinesusingtwo-samplez-test,\nwhilep<0.16isobservedcomparedwithGPT-4CoTinnaturallanguagenavigationtask.", "5 Analysis\nAsexplainedinsection3,oneofthecoreaspectsofVoTliesinenablingLLMswithvisualstate\ntracking. Duringtheexperiments,itwasobservedthatGPT-4CoToccasionallyexhibitedthisreason-\ningpatternacrossseveraltaskswithexceptionofrouteplanning. Besides,incorrectvisualizationsof\nVoTarecommonlyobservedinmodeloutputs. Inthissection,ouranalysisofVoTprimarilyfocuses\nonthreequestions: (1)Dovisualstatetrackingbehaviorsdifferamongpromptingmethods? (2)How\nvisualizationsenhancefinalanswers? (3)CanVoTbenefitlesspowerfullanguagemodels? 5.1 Dovisualstatetrackingbehaviorsdifferamongpromptingmethods? Foreachmodeloutput,weextractthesequenceofvisualizationssampledpriortogeneratingthefinal\nansweranddiscardanyvisualizationsgeneratedthereafter. Thenwecomparethesequencelengthl\nv\nwiththenumberofreasoningstepsl . WecalculateComplete\nTracking(cid:80)n(l\n==l )/nwhena\ns i v s\nvisualizationv correspondstoeachstates . Similarly,wecalculatethePartial Trackingmetric\ni i\nas\n(cid:80)n(l\n> 0)/n whenat leastone visualization ispresent beforethefinal answerisgenerated. i v\nFigure 5 shows the significant differences between these settings. In the GPT-4 CoT setting, it\ndemonstratednoticeabletrackingrateacrossalmostalltasksexceptrouteplanning. Thisobservation\nimpliesthatLLMsinherentlyexhibitthecapabilityofvisualstatetrackingwhenspatiotemporal\nsimulationisintegraltoreasoning. Ontheotherhand,thevisualstatetrackingbehaviorissensitivetopromptstovaryingdegrees. As\nshowcasedinFigure8inappendix,afterremoving\"reasoning\"fromthepromptofVoT,thevisualiza-\ntionsaresampledafterGPT-4generatesthewronganswer. Consequently,explicitlypromptingLLMs\ntovisualizetheirreasoningtraceswithVoTmarkedlyimprovesthevisualtrackingrate,thereby\nenhancing overall performance. The potential contribution of code pre-training to this emergent\ncapabilityisfurtherexploredinAppendixC. 100 96.2 92.9 87.1\n59.3 57.4\n50\n18.1 20\n0 1.20 5 0.50\nRoute NextStep Visual Natural\nPlanningPrediction Tiling language\nNavigation\n%\nGPT-4VoT GPT-4CoT GPT-4w/oViz\n100 99.2 95.391 87.4\n80.5\n59.2\n50\n30.5\n18.3\n0 1.80 5.5 0\nRoute NextStep Visual Natural\nPlanningPrediction Tiling language\nNavigation\n(a)Completetrackingrate\n%\nGPT-4VoT GPT-4CoT GPT-4w/oViz\n(b)Partialtrackingrate\nFigure5: trackingrateofdifferentsettingsacrossalltasksk. 5.2 Howvisualizationsenhancefinalanswers?", "Ideally,VoTissupposedtogenerateanaccuratevisualizationv ateachstep,sothatsubsequentstep\ni\nz couldbedeterminedcorrectly. Thisreliesonthespatialvisualizationandspatialunderstanding\ni+1\ncapability of LLMs. To evaluate these capabilities of LLMs in these tasks, we extract the final\nvisualizationfromeachmodeloutputunderthesettingGPT-4VoTinvisualnavigationandpolyomino\n7\n", "tiling task. Specifically, for visual navigation task, we extract the visualized map where LLM\ncompleted all navigation instructions. For polyomino tiling, we extract the rectangle filled with\ncorrespondingpolyominopiece. Thespatialvisualizationcapabilityismeasuredbytwocriteria:\n(1)Compliance,indicatingwhetherthemanipulationofmentalimagesatisfiesrequirementssuch\nasavoidingoverlapandnavigatingaroundobstacles.", "(2)Accuracy,indicatingwhetherthemental\nimagealignswiththecorrespondingstate. Thespatialunderstandingcapabilityismeasuredbythe\nproportionofcorrectanswerswhenthecorrespondingvisualizationisgeneratedaccurately. As could be seen from Table 2, LLMs demonstrate promising potential in performing multi-hop\nvisualizationwhileadheringtospatialconstraints,withcomplianceratesofapproximately51-52%. However,therelativelylowaccuracyofstatevisualization(around24%-26%)indicatesaneedfor\nsignificant improvements in this area. Despite this limitation, LLMs are able to make correct\ndecisionsin65%-77%ofthecaseswhenaccurateinternalstatevisualizationsaregenerated,\nwhichenhancesgroundednessandcontributestonotableperformancegains. Severalcasestudiesare\nprovidedinAppendixEforinterestedreaders. SpatialVisualization SpatialUnderstanding\nTask\nCompliance Accuracy Accuracy\nVisualNavigation 51.14 26.48 65.16\nVisualTilling 52.01 24.25 77.20\nTable2: Spatialvisualization/understandingevaluationinvisualnavigationandvisualtilingtask. Ontheotherhand,VoTpromptingmightunderperforminthosetaskswhereLLMscanleverage\nlogicalreasoningwithoutvisualizinginternalstates.Weconductedexperimentsinnaturallanguage\nnavigationwithinaring[YBL+23],wherenavigationinstructionsareeitherclockwiseorcounter-\nclockwisemovements. Bynormalizingeachinstructiontoasignednumber,GPT-4convertsthistask\ntomathematicalcalculationofaddingandmodulusoperation. Forexample,instructionsof15steps\nclockwiseand3stepscounter-clockwisearenormalizedto(15-3)%12. ResultsshowthatGPT-4\nCoToutperformsGPT-4VoTwith52.5%VS49.5%among200testinstanceswithringsizeof12. 5.3 CanVoTbenefitlesspowerfullanguagemodels? ToevaluatetheefficacyofVoTonlesspowerfullanguagemodels,weconductedexperimentsacross\nvariousmodelfamilies[BMR+20,OA+23,TLI+23]andmodelsizes, includingGPT-3.5turbo,\nLLAMA3-8B-InstructandLLAMA3-70B-Instruct. WeaccessGPT-3.5viaAzureOpenAIAPI\nwithmodelversion1106-previewandapplygreedydecodingtoallmodels. AsshowninTable3, withinthesamemodelfamily, performanceimprovesacrossalltaskswith\nincreasesinmodelsize. LLAMA3-70BVoTsignificantlyoutperformsthebaselineacrossalltasks\nexceptforvisualtiling, whereitalignscloselywithresultsobservedinGPT-4. Thisconsistency\nsuggests that VoT offers a scaling advantage when applied to more advanced models, markedly\nenhancingperformanceinlargermodels. Incontrast,lesscapablemodelstendtorelyonrandom\nguessing, especiallyinspatialreasoningtasks. Forinstance, intherouteplanningtask, GPT-3.5\nCoToftenresortstospeculativeresponses,randomguessinginnearlyhalfoftheinstances,which\nleadstoexhaustionofoutputtokens. WhileGPT-3.5VoTeffectivelyminimizesrandomguesses,\nsuch occurrences become increasingly rare with GPT-4 CoT as the model size expands. On the\notherhand,therelianceonrandomguessingintroducesunpredictabilityinperformancetrendsfor\nlesspowerfulmodels. Itsuggeststheirlimitationsinsustainingreliablereasoningprocessesacross\ndifferentdifficultylevels.", "DetailsonperformancetrendsareprovidedinAppendixD. 6 RelatedWorks\nSpatialReasoningoverText Spatialreasoningandspatiallanguageunderstanding[KPM20]in\nNLPdomainmainlyfocusonsemanticrepresentation[CBGG97,Bat10,HK11],spatialinformation\nextraction[RMK18,KVOM11],learningandreasoning[KM15,SLYA17,KFP19]. Recentadvance-\nmentshavefurtherexploredspatialreasoningwithinthecontextoflargelanguagemodels(LLMs). Toimprovemulti-hopspatialreasoningskillsoflanguagemodels,severalworks[MFNK21,MK22]\nproposedtopretrainlanguagemodelswithsyntheticdatasets. Anincreasingnumberofdatasetwere\nthendevelopedtocoversvarioustypeofspatialrelationsin2Dvisualscenes[WBC+15,SZL22],\n8\n", "VisualNavigation\nNatural-Language\nSettings VisualTiling\nRoutePlanning NextStep Navigation\nPrediction\nCompletingRate SuccRate\nGPT-3.5CoT 16.10 2.62 17.42 44.10 8.50\nGPT-3.5VoT 19.02 1.61 13.10 47.99 9.00\nLLAMA3-8BCoT 4.65 0 28.73 47.24 16.50\nLLAMA3-8BVoT 4.97 0.2 26.75 46.73 15.50\nLLAMA3-70BCoT 19.90 2.62 49.01 56.41 26.00\nLLAMA3-70BVoT 30.24 5.85 54.09 56.03 32.50\nTable 3: Performance of VoT in GPT-3.5 and LLAMA3 models. Underline denotes statistical\nsignificancewithp<0.05comparedtocorrespondingCoTbaselineusingtwo-samplez-test.", "geometricpatterns[Cho19]and3Dspatialinformation[AMKK21,HZC+23]. [FML+22]investi-\ngatedspatialreasoningcapabilitiesoftransformer-basedmodelsintheUIgroundingsetting. Onthe\notherhand,someworksadoptedin-contextlearning,leveragingLLMsforgeneralpurposereasoning\ntoconvertspatialinformationtologicforms[YIL23],orasageneralpatternmachineforsequence\ntransformation[MXF+23]. Recently,severalworksfocusedonevaluatingspatialreasoningofLLMs\nascognitivecapabilityonnavigation[YBL+23]andplanningtasks[MHV+24]amongvariousspatial\nstructures. Whilemostexistingworksrelyonlinguisticsemanticsandverbalreasoning,andmight\nnotalwaysnecessitatespatialawareness,weproposetoelicitmind\u2019seyeofLLMsinspatialreasoning\ntaskswithvariousformatsfromacognitiveperspective. TheVoTpromptinginducesLLMstocreate\nmentalimagesforvisualizingtheirinternalstatesandinformsubsequentreasoningstep. WorldModelsofLLMs WhiletherehavebeenmanytheoreticaldebatesaboutwhetherLLMs\ncaneffectivelylearnaninternalworldmodelfromungroundedformalone[BHT+20,MGSS21],\n[LeC22]advocatedthatworldmodelsshouldrepresentperceptsandactionplansatmultiplelevels\nofabstractionandmultipletimescales,withthecapabilityofplanning,predicting,andreasoning. [LWG+22]proposedtogroundLLMinthephysicalworldbyreasoningovertheexperimentalresults\npredictedbyexternalsimulation. [HGM+23]furtherleveragedLLMsasworldmodelstopredictthe\nsubsequentstatesbyactionsimulation,givenpredefinedstatesandactionspertask. Ontheother\nhand,anincreasingnumberofstudiesfocusoninvestigatinginternalrepresentationsofLLMs.[PP22,\nAKH+21]showedthatbyutilizingin-contextlearning,LLMs\u2019learnedrepresentationscanbemapped\ntogroundedperceptualandconceptualstructureincolorandspatialdomains. Moreover,[GT23]\nand[NLW23]discoveredlinearrepresentationsofspace,timeandgamestateinspecificallytrained\nLLMs,whichareimportantfordynamiccausalworldmodels. Ourworkdoesnotprobetheinternal\nrepresentations of specialized LLMs, nor does it depend on external simulation engine or state\ndefinitions. WedemonstrateLLMs\u2019zero-shotcapabilityofrepresentingtheirpreceptsatanabstract\nlevel,predictingandtrackingtheinternalstatesovertimetogenerateactionplansinmulti-hopspatial\nreasoningtasks,whichpossiblymirrorsthecausalworldmodelwithinLLMs. 7 Conclusion\nThisstudyintroducesVisualization-of-ThoughtPrompting(VoT),inspiredbythehumancognitive\nfunctionofvisualizingandmanipulatingmentalimagesthroughthemind\u2019seye. Wehavedemon-\nstratedthatVoTenablesLLMstoexhibitthemechanismof\"themind\u2019seye\",asevidencedbytheir\nperformanceinmulti-hopspatialreasoningtasksandourcomprehensiveanalysisofthereasoning\ntraces. Remarkably, VoTenableLLMstooutperformstate-of-the-artmultimodallargelanguage\nmodels(MLLMs)inthetestedvisualtasks. WhileVoTdemonstratesimpressiveefficacyinLLMs,\nthisemergentcapabilitytocreatementalimagestoenhancespatialreasoningresemblesthemind\u2019s\neyeprocess,suggestingitspromiseinMLLMs. BuildingonthesuccessofexperimentswithGPT-4,weplantoinvestigatehowVoTcanfutherelicit\n\"themind\u2019seye\"inMLLMstoenhancetheirspatialawareness. Additionally,ourfutureeffortswill\nfocusonautomaticdataaugmentationfromreal-worldscenarios,aimingtoidentifyeffectivemethods\nforlearninggeneralizedinternalrepresentationsofmentalimages. Thiswillfurtherimprovethe\nmind\u2019seyeofLLMs,ultimatelycontributingtotheadvancementoftheircognitiveandreasoning\nabilities.", "9\n", "Limitations\nThisworkonlyscratchesthesurfaceofspatialreasoningofLLMs. Bothmentalimagesandvisual\nstatetrackingrelyontheemergentabilityofadvancedLLMs. Therefore,itmightcauseperformance\ndeteriorationinlessadvancedlanguagemodelsormorechallengingtasks. Besides,duetothelimited\ndataexposureandalackofexplicitinstructiontuning, visualstatetrackingofcurrentLLMsare\nsensitivetoprompts. Forexample,whenexplicitlypromptedwith\"useascii-art\",thetrackingrate\nwillsignificantlyincreasetherebyboostingperformance,whileremoving\"reasoning\"fromtheVoT\npromptwillcauseadecreaseoftrackingrate. Moreover,thementalimagestestedinourworkare\nlimitedto2Dgrid. Tostrengththemind\u2019seyeofLLMs,morediverseandcomplicatedrepresentation\nshouldbeexploredinthefuture,suchascomplexgeometricshapesandeven3Dsemanticsshownin\nFigure11inappendix. References\n[AKH+21] MostafaAbdou,ArturKulmizev,DanielHershcovich,StellaFrank,ElliePavlick,and\nAndersS\u00f8gaard. Canlanguagemodelsencodeperceptualstructurewithoutgrounding? acasestudyincolor. arXivpreprintarXiv:2109.06129,2021. [AMKK21] DaichAzuma,TaikiMiyanishi,ShuheiKurita,andMotoakiKawanabe. Scanqa: 3d\nquestionansweringforspatialsceneunderstanding. 2022IEEE/CVFConferenceon\nComputerVisionandPatternRecognition(CVPR),pages19107\u201319117,2021. [Bad92] AlanBaddeley. Workingmemory. Science,255(5044):556\u2013559,1992. [Bat10] JohnABateman. Languageandspace: atwo-levelsemanticapproachbasedonprinci-\nplesofontologicalengineering. InternationalJournalofSpeechTechnology,13:29\u201348,\n2010. [BCE+23] S\u00e9bastienBubeck,VarunChandrasekaran,RonenEldan,J.Gehrke,EricHorvitz,Ece\nKamar,PeterLee,Y.Lee,Yuan-FangLi,ScottM.Lundberg,HarshaNori,H.Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early\nexperimentswithgpt-4. arXiv.org,2023. [BHT+20] YonatanBisk,AriHoltzman,JesseThomason,JacobAndreas,YoshuaBengio,Joyce\nChai,MirellaLapata,AngelikiLazaridou,JonathanMay,AleksandrNisnevich,etal. Experiencegroundslanguage. arXivpreprintarXiv:2004.10151,2020. [BK18] NicholasBakerandPhilipJKellman. Abstractshaperepresentationinhumanvisual\nperception. JournalofExperimentalPsychology: General,147(9):1295,2018. [BMR+20] Tom B.", "Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla\nDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,Sandhini\nAgarwal,ArielHerbert-Voss,GretchenKrueger,T.Henighan,RewonChild,A.Ramesh,\nDaniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler,MateuszLitwin,S.Gray,BenjaminChess,JackClark,ChristopherBerner,Sam\nMcCandlish, Alec Radford, I.", "Sutskever, and Dario Amodei. Language models are\nfew-shotlearners. NeuralInformationProcessingSystems,2020. [CBGG97] AnthonyGCohn,BrandonBennett,JohnGooday,andNicholasMGotts. Representing\nandreasoningwithqualitativespatialrelationsaboutregions. InSpatialandtemporal\nreasoning,pages97\u2013134.Springer,1997. [Cho19] Fran\u00e7oisChollet. Onthemeasureofintelligence,2019. [ES03] NiklasE\u00e9nandNiklasS\u00f6rensson. Anextensiblesat-solver. InInternationalconference\nontheoryandapplicationsofsatisfiabilitytesting,pages502\u2013518.Springer,2003. [FML+22] R. Freedman, Joseph B.", "Mueller, Jack Ladwig, Steven Johnston, David McDonald,\nH.Wauck,RutaWheelock,andHayleyBorck. Asymbolicrepresentationofhuman\npostureforinterpretablelearningandreasoning. arXiv.org,2022. 10\n", "[GDB17] Mona M Garvert, Raymond J Dolan, and Timothy EJ Behrens. A map of abstract\nrelational knowledge in the human hippocampal\u2013entorhinal cortex. elife, 6:e17086,\n2017.", "[GN07] EugeneGoldbergandYakovNovikov. Berkmin: Afastandrobustsat-solver. Discrete\nAppliedMathematics,155(12):1549\u20131561,2007. [Gol66] Solomon W Golomb. Tiling with polyominoes. Journal of Combinatorial Theory,\n1(2):280\u2013296,1966. [GT23] WesGurneeandMaxTegmark. Languagemodelsrepresentspaceandtime,2023. [HGM+23] ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,and\nZhitingHu. Reasoningwithlanguagemodelisplanningwithworldmodel,2023. [HK11] JoanaHoisandOliverKutz. Towardslinguistically-groundedspatiallogics. InDagstuhl\nSeminarProceedings.SchlossDagstuhl-Leibniz-Zentrumf\u00c31/4rInformatik,2011. [HZC+23] YiningHong,HaoyuZhen,PeihaoChen,ShuhongZheng,YilunDu,ZhenfangChen,\nandChuangGan. 3d-llm: Injectingthe3dworldintolargelanguagemodels,2023. [JSM+23] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\ndraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,Guillaume\nLample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTevenLeScao,ThibautLavril,ThomasWang,Timoth\u00e9eLacroix,andWilliamElSayed. Mistral7b,2023. [KC22] YunaKwakandClaytonECurtis. Unveilingtheabstractformatofmnemonicrepresen-\ntations. Neuron,110(11):1822\u20131828,2022. [KFP19] NikhilKrishnaswamy,ScottFriedman,andJamesPustejovsky.Combiningdeeplearning\nandqualitativespatialreasoningtolearncomplexstructuresfromsparseexampleswith\nnoise. InProceedingsoftheAAAIConferenceonArtificialIntelligence, volume33,\npages2911\u20132918,2019. [KGR+23] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Largelanguagemodelsarezero-shotreasoners,2023. [KM15] ParisaKordjamshidiandMarie-FrancineMoens. Globalmachinelearningforspatial\nontologypopulation. JournalofWebSemantics,30:3\u201321,2015. [Knu00] DonaldEKnuth. Dancinglinks. arXivpreprintcs/0011047,2000.", "[KPM20] ParisaKordjamshidi,J.Pustejovsky,andMarie-FrancineMoens. Representation,learn-\ningandreasoningonspatiallanguagefordownstreamnlptasks.ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing,2020. [KVOM11] ParisaKordjamshidi,MartijnVanOtterlo,andMarie-FrancineMoens. Spatialrolela-\nbeling:Towardsextractionofspatialrelationsfromnaturallanguage. ACMTransactions\nonSpeechandLanguageProcessing(TSLP),8(3):1\u201336,2011. [LB18] Brenden M.", "Lake and Marco Baroni. Generalization without systematicity: On the\ncompositionalskillsofsequence-to-sequencerecurrentnetworks,2018. [LeC22] YannLeCun. Apathtowardsautonomousmachineintelligenceversion0.9.2,2022-06-\n27. OpenReview,62(1),2022. [LKH+22] XiangLorraineLi,AdhigunaKuncoro,JordanHoffmann,CypriendeMassond\u2019Autume,\nPhil Blunsom, and Aida Nematzadeh. A systematic investigation of commonsense\nknowledgeinlargelanguagemodels,2022. [LWG+22] RuiboLiu,JasonWei,ShixiangShaneGu,Te-YenWu,SoroushVosoughi,ClaireCui,\nDennyZhou,andAndrewM.Dai. Mind\u2019seye: Groundedlanguagemodelreasoning\nthroughsimulation,2022. 11\n", "[MFNK21] Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjmashidi. Spartqa:Atextualquestionansweringbenchmarkforspatialreasoning. NorthAmerican\nChapteroftheAssociationforComputationalLinguistics,2021. [MGSS21] WilliamMerrill,YoavGoldberg,RoySchwartz,andNoahASmith. Provablelimitations\nofacquiringmeaningfromungroundedform: Whatwillfuturelanguagemodelsunder-\nstand? TransactionsoftheAssociationforComputationalLinguistics,9:1047\u20131060,\n2021. [MHV+24] Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Os-\nazuwa Ness, Nebojsa Jojic, Hamid Palangi, and Jonathan Larson. Evaluating\ncognitive maps and planning in large language models with cogeval. Arxiv:\nhttp://arxiv.org/abs/2309.15129v1,2024.", "[MK09] SamuelTMoultonandStephenMKosslyn. Imaginingpredictions: mentalimagery\nasmentalemulation. PhilosophicalTransactionsoftheRoyalSocietyB:Biological\nSciences,364(1521):1273\u20131280,2009. [MK22] RoshanakMirzaeeandParisaKordjamshidi. Transferlearningwithsyntheticcorpora\nforspatialrolelabelingandreasoning. ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,2022. [MXF+23] SuvirMirchandani,FeiXia,PeteFlorence,BrianIchter,DannyDriess,MontserratGon-\nzalezArenas,KanishkaRao,DorsaSadigh,andAndyZeng. Largelanguagemodelsas\ngeneralpatternmachines,2023. [NLW23] NeelNanda,AndrewLee,andMartinWattenberg. Emergentlinearrepresentationsin\nworldmodelsofself-supervisedsequencemodels. arXivpreprintarXiv:2309.00941,\n2023. [OA+23] OpenAI,:,JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,\nFlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,Shyamal\nAnadkat, RedAvila, IgorBabuschkin, SuchirBalaji, ValerieBalcom, PaulBaltescu,\nHaimingBao,MoBavarian,JeffBelgum,IrwanBello,JakeBerdine,GabrielBernadett-\nShapiro,ChristopherBerner,LennyBogdonoff,OlegBoiko,MadelaineBoyd,Anna-\nLuisaBrakman,GregBrockman,TimBrooks,MilesBrundage,KevinButton,Trevor\nCai,RosieCampbell,AndrewCann,BrittanyCarey,ChelseaCarlson,RoryCarmichael,\nBrookeChan,CheChang,FotisChantzis,DerekChen,SullyChen,RubyChen,Jason\nChen,MarkChen,BenChess,ChesterCho,CaseyChu,HyungWonChung,DaveCum-\nmings,JeremiahCurrier,YunxingDai,CoryDecareaux,ThomasDegry,NoahDeutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien\nEcoffet,AttyEleti,TynaEloundou,DavidFarhi,LiamFedus,NikoFelix,Sim\u00f3nPosada\nFishman,JustonForte,IsabellaFulford,LeoGao,ElieGeorges,ChristianGibson,Vik\nGoel,TarunGogineni,GabrielGoh,RaphaGontijo-Lopes,JonathanGordon,Morgan\nGrafstein,ScottGray,RyanGreene,JoshuaGross,ShixiangShaneGu,YufeiGuo,Chris\nHallacy,JesseHan,JeffHarris,YuchenHe,MikeHeaton,JohannesHeidecke,Chris\nHesse,AlanHickey,WadeHickey,PeterHoeschele,BrandonHoughton,KennyHsu,\nShengliHu,XinHu,JoostHuizinga,ShantanuJain,ShawnJain,JoanneJang,Angela\nJiang,RogerJiang,HaozhunJin,DennyJin,ShinoJomoto,BillieJonn,HeewooJun,\nTomerKaftan,\u0141ukaszKaiser,AliKamali,IngmarKanitscheider,NitishShirishKeskar,\nTabarakKhan,LoganKilpatrick,JongWookKim,ChristinaKim,YongjikKim,Hendrik\nKirchner,JamieKiros,MattKnight,DanielKokotajlo,\u0141ukaszKondraciuk,Andrew\nKondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael\nLampe,IkaiLan,TeddyLee,JanLeike,JadeLeung,DanielLevy,ChakMingLi,Rachel\nLim,MollyLin,StephanieLin,MateuszLitwin,TheresaLopez,RyanLowe,Patricia\nLue,AnnaMakanju,KimMalfacini,SamManning,TodorMarkov,YanivMarkovski,\nBiancaMartin,KatieMayer,AndrewMayne,BobMcGrew,ScottMayerMcKinney,\nChristineMcLeavey,PaulMcMillan,JakeMcNeil,DavidMedina,AalokMehta,Ja-\ncobMenick,LukeMetz,AndreyMishchenko,PamelaMishkin,VinnieMonaco,Evan\nMorikawa,DanielMossing,TongMu,MiraMurati,OlegMurk,DavidM\u00e9ly,Ashvin\nNair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwooNoh,LongOuyang,CullenO\u2019Keefe,JakubPachocki,AlexPaino,JoePalermo,\n12\n", "AshleyPantuliano,GiambattistaParascandolo,JoelParish,EmyParparita,AlexPas-\nsos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\nMichaelPetrov,HenriquePondedeOliveiraPinto,Michael,Pokorny,MichellePokrass,\nVitchyrPong,TollyPowell,AletheaPower,BorisPower,ElizabethProehl,RaulPuri,\nAlec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra\nRimbach,CarlRoss,BobRotsted,HenriRoussez,NickRyder,MarioSaltarelli,Ted\nSanders,ShibaniSanturkar,GirishSastry,HeatherSchmidt,DavidSchnurr,JohnSchul-\nman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,\nPranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina\nSlama,IanSohl,BenjaminSokolowsky,YangSong,NatalieStaudacher,FelipePetroski\nSuch,NatalieSummers,IlyaSutskever,JieTang,NikolasTezak,MadeleineThomp-\nson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley,\nJerryTworek,JuanFelipeCer\u00f3nUribe,AndreaVallone,ArunVijayvergiya,Chelsea\nVoss,CarrollWainwright,JustinJayWang,AlvinWang,BenWang,JonathanWard,\nJasonWei,CJWeinmann,AkilaWelihinda,PeterWelinder,JiayiWeng,LilianWeng,\nMattWiethoff,DaveWillner,ClemensWinter,SamuelWolrich,HannahWong,Lauren\nWorkman,SherwinWu,JeffWu,MichaelWu,KaiXiao,TaoXu,SarahYoo,Kevin\nYu,QimingYuan,WojciechZaremba,RowanZellers,ChongZhang,MarvinZhang,\nShengjiaZhao,TianhaoZheng,JuntangZhuang,WilliamZhuk,andBarretZoph. Gpt-4\ntechnicalreport,2023.", "[Ope23] OpenAI. Gpt-4v(ision)systemcard. 2023. [PP22] RomaPatelandElliePavlick. Mappinglanguagemodelstogroundedconceptualspaces. InInternationalConferenceonLearningRepresentations,2022. [RAB+20] LauraRuis,JacobAndreas,MarcoBaroni,DianeBouchacourt,andBrendenMLake. A\nbenchmarkforsystematicgeneralizationingroundedlanguageunderstanding. Advances\ninneuralinformationprocessingsystems,33:19861\u201319872,2020. [Reg19] JohnRegehr. Explainingcodeusingasciiart,2019. [RFD+21] Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei Cheng, Dell Zhang, and\nAndreFreitas. Groundingnaturallanguageinstructions: Canlargelanguagemodels\ncapturespatialinformation?,2021. [RKH+21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,Gretchen\nKrueger,andIlyaSutskever. Learningtransferablevisualmodelsfromnaturallanguage\nsupervision,2021. [RMK18] Taher Rahgooy, Umar Manzoor, and Parisa Kordjamshidi. Visually guided spatial\nrelation extraction from text. In Proceedings of the 2018 Conference of the North\nAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage\nTechnologies,Volume2(ShortPapers),pages788\u2013794,2018. [SB14] AnthonyJSmithandJoannaJBryson.Alogicalapproachtobuildingdungeons:Answer\nsetprogrammingforhierarchicalproceduralcontentgenerationinroguelikegames. In\nProceedingsofthe50thAnniversaryConventionoftheAISB,2014.", "[SF72] RogerNShepardandChristineFeng. Achronometricstudyofmentalpaperfolding. Cognitivepsychology,3(2):228\u2013243,1972. [She78] RogerNShepard. Thementalimage. Americanpsychologist,33(2):125,1978. [SLYA17] AlaneSuhr, MikeLewis, JamesYeh, andYoavArtzi. Acorpusofnaturallanguage\nforvisualreasoning. InReginaBarzilayandMin-YenKan,editors,Proceedingsofthe\n55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2: Short\nPapers),pages217\u2013223,Vancouver,Canada,July2017.AssociationforComputational\nLinguistics. [SM71] RogerN.ShepardandJacquelineMetzler. Mentalrotationofthree-dimensionalobjects. Science,171:701\u2013703,1971. 13\n", "[SMM21] HariniSampath,AliceMerrick,andAndrewMacvean. Accessibilityofcommandline\ninterfaces. InProceedingsofthe2021CHIConferenceonHumanFactorsinComputing\nSystems,pages1\u201310,2021. [SZL22] Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. Stepgame: A new benchmark for\nrobustmulti-hopspatialreasoningintexts. AAAIConferenceonArtificialIntelligence,\n2022. [TLI+23] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,\nTimoth\u00e9eLacroix,BaptisteRozi\u00e8re,NamanGoyal,EricHambro,FaisalAzhar,Aurelien\nRodriguez,ArmandJoulin,EdouardGrave,andGuillaumeLample. Llama: Openand\nefficientfoundationlanguagemodels. arXiv.org,2023.", "[Tol48] EdwardCTolman. Cognitivemapsinratsandmen. Psychologicalreview,55(4):189,\n1948. [WBC+15] JasonWeston,AntoineBordes,SumitChopra,AlexanderM.Rush,BartvanMerri\u00ebnboer,\nArmandJoulin,andTomasMikolov. Towardsai-completequestionanswering: Asetof\nprerequisitetoytasks,2015. [YBL+23] Yutaro Yamada, Yihan Bao, Andrew K. Lampinen, Jungo Kasai, and Ilker Yildirim. Evaluatingspatialunderstandingoflargelanguagemodels,2023. [YIL23] ZhunYang,AdamIshay,andJoohyungLee. Couplinglargelanguagemodelswithlogic\nprogrammingforrobustandgeneralreasoningfromtext,2023. 14\n", "A SyntheticData\nA.1 VisualNavigation\nAsdepictedin1, givenaspecifick, theprocessofgeneratinga2Dnavigationmapiscomposed\nof3steps,whichareinstructiongeneration,instructionsimulation,maprendering. Ininstruction\ngenerationstep,weenumerateallpossibleinstructionsetsnavigatingfromthestartingpointtothe\ndestination(e.gmoveup,thenmoveright). Duringthisstep,onlythedirectionofeachinstructionis\ndetermined,whilethemovingdistanceisundetermineduntilnextstep. Ininstructionsimulationstep,\nsimulationisappliedinthe2Dcoordinatesystemwithorigin(0,0)asthestartingpoint. Toguarantee\nanuniqueanswerineachnavigationmap,themovingdistanceofeachinstructionisdynamically\ncalculatedtoavoidoverlapping. Eachtimewhenanoverlappingisdetected,themovingdistance\nof previous instruction will be increased by 1 unit recursively until overlapping is resolved. As\nthedistanceisdetermined,thosecorrespondingpointsareaddedtothenavigatingpath. Afterall\ninstructionsarecompleted,thefinalpointismarkedasthedestination. Inthemaprenderingstep,the\nboundingboxofthosepointsisadoptedandnormalizedtoa2Dsquaregrid. Thestartingpointand\ndestinationaremarkedwithdedicatedsquares,andcellsalongthepatharemarkedbyemptysquares,\nwhileotheruntouchedcellsaremarkedbyobstaclesquares. Algorithm1:NavigationMapGeneration\nInput :k\nOutput:C ={s ,s ,...,s },C ={t ,t ,...,t },\nsolution 1 2 n textual_map 1 2 n\nC ={v ,v ,...,v };wheren=2k+1\nvisual_map 1 2 n\n1 dirs\u2190[up,left,down,right]\n2 instruction_sets\u2190gen_instruction(dirs,k)\n3 fordir_instructsininstruction_setsdo\n4 cur_pos,path_points,moves\u2190initialize() // origin as the starting point\n5 fordirection in dir_instructsdo\n// instruction simulation\n6 ifnotvalidate_plan(cur_pos,direction,path_points)then\n7 increase_previous_move(cur_pos,moves,path_points)\n8 end\n9 cur_pos,path_points\u2190step_forward(cur_pos,direction,path_points)\n10 moves\u2190moves\u222adirection\n11 end\n12 s i \u2190moves\n13 t i,v i \u2190render_map(extract_bounding_box(path_points))\n14 C solution \u2190C solution\u222as i\n15 C textual_map \u2190C textual_map\u222at i\n16 C visual_map \u2190C visual_map\u222av i\n17 end\nSincethedirectionofeachnavigationinstructionisalternating,thereare4\u22172k\u22121 = 2k+1 kinds\nofspatialconfigurationsforak-hopnavigationmap. Duringtheimplementation,wesimplifythe\nrecursiveimplementationwithanearlyquitwhenpathoverlappingcouldnotberesolvedwithinone\niteration,themainconsiderationofwhichisthesizeofthemap.", "Sothenumberofgeneratedmapis\nslightlylowerthan2k+1asthenavigatingstepkincreases. A.2 VisualTiling\nThedatagenerationprocesscomprises3stages,includingconfigurationgeneration,questiongen-\nerationandpolyominorendering. Intheconfigurationgenerationstage, togeneratevalidspatial\nconfigurationsofarectangleandthecorrespondingpolyominoset,weconvertatilingproblemto\nexistingformalizedproblems. Oneoftheproblemsisanexactcoverproblemleveragingdancing\nlinkalgorithm[Knu00],whichcouldbedescribedas: givenamatrixof0sand1s,findasetofrows\ncontainingexactlyone1ineachcolumn. Theconversionistoconstructamatrixof0sand1s,each\nrowofwhichrepresentsapossiblearrangementofplacingaspecificpolyominoinarectangle. As\nillustratedinEquation9,givenkpolyominopieces,andarectangleofnunitstobefilled,thefirstk\n15\n", "columnscomposeanone-hotvectorindicatingthecorrespondingpolyomino,andthelastncolumns\naremarkedwith0or1dependingonwhetherthecorrespondingunitisfilledbythatpolyomino. Then\nfindingasetofpolyominoarrangementsinarectangleequalstofindasetofrowscontainingexactly\none1ineachcolumn. Anotheradaptableproblemisthebooleansatisfiabilityproblem(commonly\nknownasSAT),forwhichefficientsolversexist[ES03,GN07]. Atilingproblemcanbeconvertedto\nSATbyintroducingabooleanvariableforeachpossiblearrangementofeachpiece,andthenadding\nclausescomprisingofthosebooleanvariablesthatensureatleastonearrangementofeachpieceis\nachieved,whileavoidingconflictsbetweenarrangementsofonepieceortwodifferentpieces.", "Giventhesizeofarectangleandpolyominoestobefit,multiplecorrespondingsolutionsaregenerated\nbyapplyingthosealgorithms. Theninthequestiongenerationstage, werandomlymaskseveral\npolyomino pieces in the rectangle, and generate a question answer(QA) pair for each masked\npolyomino. Finallytherectangleandeachpolyominopiecearerenderedwithemojisquares. C C C C\n1 k k+1 n+k\n\uf8ee 1 0 1 0 0 1 \uf8f9\nP1\n1 0 0 1 1 0\n\uf8ef \uf8fa\n\uf8ef \uf8fa\n\uf8ef \uf8fa (9)\n\uf8ef \uf8fa\n\uf8ef \uf8fa\n\uf8f0 0 1 1 1 0 0 \uf8fb\nPk\n0 1 0 0 1 1\nA.3 VisualDataRendering\nAftergatheringthetextualdatasetof2Dsquaregrid,wegeneratethecorrespondingvisualdatasetby\ndrawingtextontoanimage. Specificallyweadoptcoloremojisforafaircomparisonasthey\u2019remore\nvisualfriendlytoamultimodalmodel. A.4 DatasetDetails\nDatadistributionamongvariousdifficultylevelsforvisualnavigationtasksandvisualtilingtasksare\nprovidedinTable4and5.Itprovidesflexibledifficultycontrolacrossdifferenttasks.Forvisualtiling\ntask,thedifficultyiscontrolledbythenumberofmaskedpolyominopieces. Asthenumberincreases,\nthe more spatial arrangements LLMs need to consider. Regarding the visual navigation task, as\nillustratedinfigure2,weusethenumberofroadsktocontroldifficulty,whichiscorrespondingto\nthesizeofthemap. KStep Maskcount\nTask Total Total\n2 3 4 5 6 7 2 3\nRoutePlanning 8 16 32 64 128 248 496 Configuration 248 124 376\nNextStepPrediction 8 32 96 256 640 1488 2520 QAInstance 489 307 796\nTable4: Datadistributionofvisualnavigationdatasetwith Table 5: Details of visual tiling\nthetotalnavigatingstepofkindicatingdifficultylevel. The dataset. Some QA instances are\nreasonwhythenumberofgeneratedmapisslightlylower discardedwhenmultiplesolutions\nthan2k+1fork >5isexplainedinAppendixA.1. existandallanswersarecorrect. B Examples\nFor visual navigation and visual tiling tasks, the structured input template is comprised of task\ninstruction,inputparametersandpromptofspecificsetting. B.1 VisualTasks\nTaskinstructionsandresponsesofeachvisualtaskundersettingGPT-4VoTareprovidedasfollowing:\n\u2022 RoutePlanningTaskinstructioninFigure6,responseinFigure12. 16\n", "\u2022 NextStepPredictionTaskinstructioninFigure6,responseinFigure13. \u2022 VisualTilingTaskinstructioninFigure7,responseinFigure14. Route Planning Next Step Prediction\nNavigation Task: for a provided map, is the home as starting point, is the Navigation Task: for a provided map, is the home as starting point, is the\noffice as the destination. means the road, means the obstacle.", "There exists office as the destination. means the road, means the obstacle. There exists\none and only one viable route for each map. Each step you choose a direction and one and only one viable route for each map. Each step you choose a direction and\nmove to the end of the continuous road or the destination. move to the end of the continuous road or the destination. map: map:\n``` ```\n```\n``` Starting from , to navigate to , you made following movements:\nStarting from , provide the steps to navigate to . 1. Move right to the end of continuous road. 2. Move down to the end of continuous road. Visualize the state after each reasoning step. 3. Move left to the end of continuous road. What's the direction of next movement? A. Up\nB. Left\nC. Down\nD. Right\nVisualize the state after each reasoning step. Figure6: TaskInstructionofvisualnavigation. Task: given a set of polyominoes and corresponding Variation 2 fitting into its bounding box: Variation 3 fitting into its bounding box:\nvariations of each polyomino, fit them into the empty ``` ```\nsquares ( ) in the target rectangle without overlapping any\nexisting polyominoes or going outside the rectangle. The\nvariations allow only translation, not rotation or reflection. It's guaranteed that there always exists a solution. ```\n------------------------- -------------------------\nTarget rectangle with 12 empty squares: Variations for Tetromino L:\n``` Variation 3 fitting into its bounding box: ```\n``` -------------------------\nTo fit all the provided polyominoes into the empty squares\n( ), what's the correct variation of Tetromino T? A. 2\n``` B. 3\nVariation 7 fitting into its bounding box: C. Neither\n```\nVisualize the state after each reasoning step. ```\nProvided polyominoes:\n1. Tetromino I ( )\n2.", "Tetromino L ( )\n```\n3. Tetromino T ( )\n-------------------------\n-------------------------\nVariations for Tetromino T:\nVariations for Tetromino I:\nVariation 2 fitting into its bounding box:\nVariation 1 fitting into its bounding box:\n```\n```\n```\n```\nFigure7: TaskInstructionofvisualtiling. B.2 NaturalLanguageNavigation\nPromptExample Youhavebeengivena3by3squaregrid. Startingfromavertex,youwillmove\nalongtheedgesofthegrid. Initially,youarepositionedatthebottom-leftcornerofthegrid,where\nyouwillfindatorch,thenyougoright,whereyouwillfindaninfantbed,thenyougoright,where\nyouwillfindanAmericandipper. Thenyougoup,whereyouwillfindajay,thenyougoleft,where\nyouwillfindaterrapin,thenyougoleft,whereyouwillfindamicrowaveoven. Thenyougoup,\n17\n", "Visualize the state after each reasoning step Let\u2019s think step by step. First, let's look at the target rectangle and identify the empty squares ( ) First, let's look at the empty spaces in the target rectangle. We have a 4x1 space at the\nwhere we need to fit the polyominoes: bottom and three single squares above it. The Tetromino I ( ) can only fit in the 4x1\nspace at the bottom, so we'll use Variation 2 of the Tetromino I to fill that space. Now, let's consider the Tetromino T ( ) variations. We have three single empty squares\nthat are not in a straight line, so Variation 2 of the Tetromino T, which is a straight line\nWe have two polyominoes to fit into this rectangle: Tetromino I ( ) and with a protrusion, will not fit. However, Variation 3 of the Tetromino T has a\nTetromino T ( ). The Tetromino I can only fit in the bottom row since it's configuration that can fit into the three remaining empty squares with one square\nthe only place with four consecutive empty squares, either horizontally or protruding to the side, which matches the layout of the empty squares. vertically. Let's place the Tetromino I ( ) in the bottom row:\nTherefore, the correct variation of TetrominoT to fit into the remaining empty squares is:\nB. 3\nThe final configuration would look like this:\n\u2026.. We can see that Variation 2 of Tetromino T ( ) fits perfectly into the\nremaining empty squares, as it matches the T shape formed by the empty\nsquares. Let's place Variation 2 of Tetromino T ( ) into the target rectangle:\nThe correct variation of TetrominoT to fit into the empty squares is Variation 2. Answer: A.", "2\n(a)CommonbehaviorsofVoTandCoTpromptinginvisualtilingtask,withtheoveralltrackrateof87.1%and\n57.4%respectively. Visualize the state after each reasoning step Visualize the state after each reasoning step\nTo navigate from to , we need to find the path that avoids obstacles To navigate from to on the provided map, follow these instructions:\n( ) and follows the roads ( ). Here's the step-by-step navigation: 1. Move down to the end of the continuous road.", "1. Move down from to the end of the continuous road. 2. Move right to the end of the continuous road. 3. Move up to reach the destination . Here's the visualization after each step:\nStarting point:\nAfter the move:\nAfter Step 1 (Move down):\n2. Move left from the current position to the office . After Step 2 (Move right):\nAfter the move:\nAfter Step 3 (Move up):\nThe has now reached the , completing the navigation task. (b)Routeplanningtaskissensitivetoprompts.Afterdeletingtheword\"reasoning\"fromtheVoTprompt,final\nanswerandstateisderivedwithoutconditioningonstatevisualizationinmanycases.Theleftexampleprovides\nacorrectsolutionwithimperfectintermediatevisualization,whiletherightexampleprovidesawrongsolution. Figure8: Examplecasesofvisualstatetrackingbehaviorsindifferenttasks. Weprovideside-by-side\ncomparisonstoshowhowvisualstatetrackingisinfluencedbyprompt,whereboldtextsonthetop\nare prompts to compare. Case on the left exhibits visual state tracking in an interleaved manner,\nwherenextstateisconditionedonvisualizationofpreviousstates. Caseontherightdisentangles\nverbalreasoningandstatevisualization,whichleadstoadropintrackingrateandtaskperformance. 18\n", "whereyouwillfindabaseballplayer,thenyougoright,whereyouwillfindaharvestman,thenyou\ngoright,whereyouwillfindaneckbrace. Nowyouhavealltheinformationonthemap. Youstartat\nthepositionwherethetorchislocated,thenyougorightbyonestep,thenyougorightbyonestep,\nthenyougoupbyonestep,thenyougoupbyonestep,thenyougoleftbyonestep,thenyougo\ndownbyonestep,andthenyougodownbyonestep. Whatwillyoufind?", "ResponseExample SeeFigure15. C VisualStateTracking\nAsforwherethisemergentabilitystemsfrom,itmightderivefromtabulardata,citygridnaviga-\ntion,mazeexplorationrelatedcodingproblems[YBL+23]. Thesetasksinvolvesunderstandingand\nmanipulating objects in a 2D square grid. Besides, we conjecture the exposure of ascii-art com-\nments[Reg19]duringLLMs\u2019codepre-trainingpossiblyenhancesthisgeneralizedability. Asafact\ntosupportthisconjecture,thevisualtilingtaskisdifferentfromnavigationtasksbecauseitrequires\nshapeunderstandingandspatialmanipulationability. Whiletabulardataandsquaregridnavigation\ndata boost row-wise or column-wise attention, ascii-art supplements intricate spatial attention to\nunderstand and manipulate 2D shapes. Additionally, ascii-art in code comments is presented in\nvarious formats, one of which is interleaved ascii diagrams, natural language and programming\nlanguage. It require LLMs to generate the interleaved mental images and text sequence, thereby\nenhancing spatial visualization ability and spatiotemporal causality. Interestingly in the natural\nlanguagenavigationtask,whenGPT-4ispromptedwith\"useascii-arttovisualize\",thecomplete\ntrackingrateincreasesto98.5%(+78.5%),boostingtaskperformanceto62.5%(+3.5%). C.1 Ascii-artinCodeComments\nAscii-artiscommonlyusedincodecommentstorepresentdatastructure,diagram,geometryand\nsoon,whichcouldbenefitLLMs\u2019spatialunderstandingandvisualizationcapability. Besides,it\u2019s\nalsousedtoillustratehowanalgorithmworksorsimulateanoperation,wherereasoningtracesand\ncorrespondingvisualizationarepresentedinaninterleavedmanner.", "Belowareseveralexamplesin\nopen-sourceprojects. \u2022 Spatial Causality:Double-ended queue in Rust, Scrolling web pages and tree rotation\npresenttripletsofpreviousvisualstate,instruction,andupdatedstateofinstructionfollow-\ning. \u2022 TemporalCausality:Undosystemsfromemacsprovidesvarioustemporalstatesoftheundo\nsystemwhenundooperationhappensindifferenttimelinesandcorrespondingvisualizations\ninaninterleavedmanner. Eachvisualizationreflectsthetemporalcasualityofthesystem\nstate. Thiskindofinterleavedsequencetracksthesystemstateovertime,thusreflectingspatiotemporal\ncasuality. D PerformanceTrendsAcrossLevels\nIn this analysis, we examine performance trends across varying difficulty levels in the next-step\nprediction task for models utilizing either CoT or VoT methods. These trends are crucial for\nunderstandingtheinherentunpredictabilityassociatedwithrandomguessing. Askincreasesfrom2\nto7inak-stepnavigationmap,distinctperformancepatternsemergeamongdifferentmodels,as\ndepictedinFigure9. LargerlanguagemodelssuchasGPT-4andLLAMA3-70Bdemonstrateamore\npredictabledecreaseinaccuracywithincreasingk. Thistrendindicatesarobustabilitytohandle\nprogressivelychallengingtasks,despitetheoveralldecreaseinperformance. Detailedstatisticsare\nprovidedintable6. Incontrast, lesspowerfulmodelslikeGPT-3.5andLLAMA3-8Bexhibitan\nirregularperformancetrajectory. Thesemodelsshowvariableaccuracy,withsignificantfluctuations\nathigherdifficultylevels,suggestingarelianceonrandomguessing,particularlyunderconditionsof\nincreasedtaskdifficulty. Thisbehaviorhighlightstheirlimitationsinsustainingreliablereasoning\nprocessesthroughmorecomplexscenarios. Furthermore,theVoTmethodseemstoofferamodest\nimprovementinperformanceforthelesspowerfulmodels,particularlyinscenariosoflowerdifficulty. 19\n", "ThisobservationsuggeststhatVoTmightbeadvantageousforenhancingreliablereasoninginsimpler\nspatialreasoningtasks,potentiallycompensatingfortheinherentweaknessesofsmallerlanguage\nmodels. 100\n80\n60\n40\n20\n0\n2 3 4 5 6 7\nK-stepMap\n)%(ycaruccA\nCoTModelsPerformanceTrend\n100\n80\n60\n40\n20\n0\n2 3 4 5 6 7\nK-stepMap\n)%(ycaruccA\nVoTModelsPerformanceTrend\nGPT-4 GPT-3.5 LLAMA3-70B LLAMA3-8B\nFigure9:PerformanceTrendsofCoTandVoTModelsAcrossdifficultylevelsinnext-step-prediction\ntask. Model K-stepMap MapCount CoTAccuracy(%) VoTAccuracy(%)\n2 8 75.00 75.00\n3 32 68.75 62.50\nGPT-4 4 96 60.42 68.75\n5 256 50.78 64.06\n6 640 52.34 55.16\n7 1488 45.30 52.69\n2 8 62.50 62.50\n3 32 68.75 65.63\nLLama3-70B 4 96 60.42 62.50\n5 256 56.25 57.42\n6 640 48.59 54.84\n7 1488 46.71 52.35\nTable6: CoTandVoTperformanceofadvancedmodelsinnext-step-predictiontaskacrossvarious\ndifficulty levels. While performance drops as difficulty level increases, VoT method generally\nmaintains a higher accuracy compared to CoT, highlighting its robustness in more challenging\nscenarios. E Casestudy\nWeconsidervisualstatetrackingsimilartospatiotemporalsimulation. Duringthesimulationinthose\ntasks,wediscoveredseveralinterestingbehaviorsofLLM. 1. Diverse visualization formats for state tracking: Nearly 30 different symbols found in the\nnavigationtaskstotrackthenavigationprogress,includingmarkingthepath,markingthecurrent\nlocation. Amongthosediverserepresentations,LLMsucceededinsomechallengingcaseswhereit\nuseddirectionalarrowemojistoindicateboththelocationandmovingdirectionateachstep. More\nexamplescouldbefoundinAppendixE.1. 2. Inconsistencybetweenlanguageandvisualization: Thisiscommonlyobservedacrossalltasks. Duetothelimitedvisualizationcapability,sometimesLLMgeneratesaccuratelanguageinstruction\nbutinaccuratevisualization. Andinothercases,LLMgenerateswronganswerseventhevisualization\nisgeneratedcorrectly,whichreflectsitslimitationofspatialunderstandingasdiscussedinprevious\nsection. MoreexamplescouldbefoundinAppendixE.2.", "20\n", "3. Self-refinemechanism:Wefoundseveralcasesinvisualtilingtaskswherespatialhallucination\nhappensduetotheinconsistencyorinaccuratevisualization.Subsequently,LLMrefineditsreasoning,\nresultinginanaccuratevisualizationandthecorrectionofthefinalanswer. Moreexamplescouldbe\nfoundinAppendixE.3. E.1 mentalimagesforStateTracking\nInthevisualnavigationtask,LLMadoptedvarioussymbolsandrepresentationstotrackthestateof\nnavigationprogress. AsshowninFigure10,there\u2019reseveraltrackingstyles.", "\u2022 Markthepath: adoptinganidenticalsymboltomarkcurrentlocationorpartofthepath. \u2022 Markpathanddirection: usingdirectionalarrowstomarkcurrentlocationandindicatethe\nmovingdirectionsimultaneously,whichismorechallengingthansimplymarkingthepath. \u2022 Mark path with temporal steps: using numbers to demonstrate both temporal steps and\ncurrentlocation. \u2022 Remove road: turning roads into obstacles to avoid turning back, instead of adopting\nadditionalsymbolstomarkthepath. E.2 InconsistencybetweenLanguageandVisualization\nInthevisualtilingtask,twoinconsistentstepsarehighlightedinFigure16. Oneistheinconsistent\nvisualizationwiththelanguageinstructionof\"placeVariation6ofTetrominoL\". Anotheristhe\nwrongdecisiontochose\"Variation2ofTetrominoI\"giventhevisualizationofthevalidstate. E.3 Self-refineMechanism\nWefoundvisualizationcouldenhanceLLM\u2019sreasoningbyself-groundingandrefiningsubsequent\nreasoningstepsinsomecases. AsshowninFigure17,despitesuccessfullyidentifyingvariation1of\ntetrominoLasincorrectoption,GPT-4excludedthecorrectoptionofvariation6evenit\u2019splaced\naccuratelyduetospatialhallucination(overlappingwithyellowpieces),whichledtoaimpossible\nsolution. Thenitdetectedthemistakeandre-evaluatetheplacementofvariation6. Finallyitplaced\nthecorrectpieceintothetopleftcornerandvalidatedtheanswerbyfillingtheremainingspace. 21\n", "Use round pin Use checklist\nOriginal map 1. Move down 2. Move right Original map Move down\nRemove road to avoid turning back\n3. Move down 4. Move right 5. Move down Original map Move right\nUse arrows to reflect direction Use numbers for temporal steps\nOriginal map 1. Move right 2. Move down Original map 1. Move left 2. Move down\n3. Move right 4. Move down 5. Move left 3. Move right 4.", "Move down 5. Move right\nFigure10: Mentalimagesforstatetrackinginvisualnavigationtasks. Wefounddiversevisualization\nformatsinLLMstotrackthenavigationprogress,includingmarkingthepath,markingthecurrent\nlocation, marking directional steps, and marking temporal steps. More details are available in\nAppendixE.1\n22\n", "The ASCII art you've provided represents a three-dimensional cube with labeled vertices from 0 to 7. To unfold\nthe front face (which appears to be the face connecting vertices 1, 2, 6, and 5), we would lay it flat. Assuming\nthat the front face is the one with vertices 1, 2, 6, and 5, and we unfold it downwards, the net might look\nsomething like this:\n```\n0 3\n*-------*\n/| /|\n*-------* |\n4| | | |7\nAscii cube: | *----- |-*\n``` |/ |/\n0 3 *-------*\n*-------* 1 2\n1 / | 2 /| ```\n*-------* | Now, let's unfold the front face (1, 2, 6, 5) downwards:\n| |4 | |7 ```\n| *-----|-* 0 3\n|/ | / *-------*\n*-------* / /|\n5 6 *-------* |\n``` 4| | |7\nUnfold the front face | | *\n| |/\n*-------*\n| |\n| |\n*-------*\n1 2\n```\nHere, the face with vertices 1, 2, 6, and 5 has been unfolded downwards. The vertical lines connecting the top\nface to the front face have now become horizontal, indicating that the front face is lying flat. The cube is now\nrepresented as a two-dimensional net, with the front face unfolded. Keep in mind that this is a simplified\nrepresentation and that in an actual net, the faces would be connected at the edges without the gaps shown in the\nASCII art. Figure11: Representationfor3DsemanticsandqualitativeresultsofGPT-4manipulatingthemental\nimagecorrectly. Starting from , let's navigate step by step to the .", "Step 3: Move left to the end of the continuous road. Step 6: Move down to reach the . Initial map: ``` ```\n```\n``` ```\n``` Step 4: Move down to the end of the continuous road. Step 1: Move right from to the end of the continuous road. ``` The steps to navigate from to are:\n```\n1. Right\n2. Down\n3. Left\n4. Down\n5. Left\n```\nStep 5: Move left to the end of the continuous road. 6. Down\n``` ``` We have now reached the destination, . Step 2: Move down to the end of the continuous road. ```\n```\n```\nFigure12: ResponsesofRoutePlanning\n23\n", "Let's visualize the state after each movement: 3. Move left to the end of the continuous road:\nInitial map: ```\n```\n``` ```\n1. Move right to the end of the continuous road: Now, the next movement options are:\n```\nA.Up - This would lead back to a previous position, which is not a viable route to the destination. B.", "Left - This is not possible as there is an obstacle ( ) immediately to the left. C. Down - This would lead to the destination ( ). D. Right - This is not possible as there is an obstacle ( ) immediately to the right. The correct direction for the next movement is:\nC. Down\n``` Moving down will take us to the destination, which is the office ( ). Here's the final visualization:\n2. Move down to the end of the continuous road: ```\n```\n```\nFigure13: ResponseofNextStepPrediction\n24\n", "Let's start by examining the empty spaces in the target rectangle and the provided\npolyominoes and their variations. We need to fit the Tetromino I, Tetromino L, and\nTetromino T into the empty spaces without overlapping any existing polyominoes or going\noutside the rectangle. The target rectangle with 12 empty squares looks like this:\n```\n```\nWe have two variations for Tetromino I ( ), which can either be placed vertically or\nhorizontally.", "Given the configuration of the empty spaces, the only way to place the\nTetromino I is horizontally at the top row, as the vertical placement would not fit anywhere. After placing Tetromino I ( ), the target rectangle looks like this:\n```\n```\nNext, we have two variations for Tetromino L ( ). We can see that Variation 3 of Tetromino\nL can fit in the bottom left corner, while Variation 7 would not fit anywhere without\noverlapping or going outside the rectangle. After placing Tetromino L ( ), the target rectangle looks like this:\n```\n(a)ResponseofVisualTiling-Page1\n25\n", "```\nNow, we have two variations for Tetromino T ( ). We need to fit this into the remaining\nempty spaces. Variation 2 of Tetromino T cannot fit because it would require two empty\nspaces next to each other vertically, which we do not have.", "However, Variation 3 of\nTetromino T fits perfectly into the remaining space. After placing Tetromino T ( ), the target rectangle looks like this:\n```\n```\nThe correct variation of Tetromino T that fits into the empty squares is:\nB. 3\n(b)ResponseofVisualTiling-Page2\nFigure14: ResponsesofVisualTiling\n26\n", "(a)ResponseofNaturalLanguageNavigation-Page1\n27\n", "(b)ResponseofNaturalLanguageNavigation-Page2\nFigure15: ResponsesofNaturalLanguageNavigation\n28\n", "(c)ResponseofNaturalLanguageNavigation-Page3\nFigure15: ResponsesofNaturalLanguageNavigation\n29\n", "First, let's look at the target rectangle and identify the empty squares ( ):\n```\n```\nWe have two polyominoes to fit into this rectangle: Tetromino L ( ) and Tetromino I ( ). We need to use the variations provided to fill the empty squares without overlapping the\nexisting polyominoes or going outside the rectangle. Let's start with Tetromino L ( ) because it has a unique shape that can fit into corners or\nprotrusions.", "Looking at the variations of Tetromino L:\nVariation 1:\n```\n```\nVariation 6:\n```\n```\nWe can see that Variation 6 of Tetromino L fits perfectly into the top left corner of the target\nrectangle, filling three empty squares vertically and turning to fill one horizontally. Let's\nplace Variation 6 of Tetromino L into the target rectangle:\n```\n(a)Inconsistencybetweenlanguageandvisualization-Page1\n30\n", "```\nNow, we have four empty squares remaining in a straight line, which matches the shape of\nTetromino I.", "Let's look at the variations of Tetromino I:\nVariation 1:\n```\n```\nVariation 2:\n```\n```\nSince the remaining empty squares are in a horizontal line, Variation 2 of Tetromino I will fit\nperfectly. Let's place Variation 2 of Tetromino I into the target rectangle:\n```\n```\nThe correct variation of Tetromino I that fits into the target rectangle is:\nB. 2\n(b)Inconsistencybetweenlanguageandvisualization-Page2\nFigure16: Inconsistencybetweenlanguageandvisualization\n31\n", "Let's start by visualizing the target rectangle with the empty squares ( ) and the existing\npolyominoes:\n```\n```\nWe have two polyominoes to fit into this rectangle: Tetromino L ( ) and Tetromino I ( ). We need to determine which variation of Tetromino L will fit without overlapping any\nexisting polyominoes or going outside the rectangle. Let's consider the variations for Tetromino L:\nVariation 1:\n```\n```\nVariation 6:\n```\n```\nWe also have the variations for Tetromino I, but since it's a straight line, it can only fit in one\nway in the remaining space after placing Tetromino L.", "Let's try to fit Variation 1 of Tetromino L first:\n```\n(a)ExamplecaseofSelf-refine-Page1\n32\n", "```\nAs we can see, Variation 1 of Tetromino L does not fit because it overlaps with the existing\nyellow ( ) polyomino. Now let's try Variation 6 of Tetromino L:\n```\n```\nVariation 6 of Tetromino L does not fit either because it goes outside the rectangle. Since neither Variation 1 nor Variation 6 of Tetromino L fits into the target rectangle without\noverlapping existing polyominoes or going outside the rectangle, the correct answer is:\nC. Neither\nHowever, there seems to be a mistake because the task guarantees that there always\nexists a solution.", "Let's re-evaluate the placement of Tetromino L. We can see that the Tetromino L must fit in the top left corner because that's the only place\nwith enough space for its shape. Let's try placing Variation 1 of Tetromino L in the top left\ncorner:\n```\n(b)ExamplecaseofSelf-refine-Page2\nFigure17: ExamplecaseofSelf-refine\n33\n", "```\nThis time, Variation 1 of Tetromino L fits perfectly in the top left corner. Now, we can place\nthe Tetromino I in the remaining space:\n```\n```\nThe Tetromino I fits perfectly in the remaining space using Variation 1 (vertical placement). Therefore, the correct variation of Tetromino L that fits into the target rectangle is:\nA. 1\nThe initial attempt to place Variation 1 of Tetromino L was incorrect because it was not\nplaced in the top left corner.", "Upon correcting the placement, we find that Variation 1 does\nindeed fit, and the Tetromino I can be placed afterward to complete the puzzle. (c)ExamplecaseofSelf-refine-Page3\nFigure17: ExamplecaseofSelf-refine\n34\n"], "text_input": ["Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning.", "According to Hotho et al. (2005), there are three perspectives of text mining: information extraction, data mining, and knowledge discovery in databases (KDD).[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities). Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via the application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information. A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted. The document is the basic element when starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections.[3]  Text analytics See also: List of text mining methods Text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation.[4] The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of \"text mining\"[5] in 2004 to describe \"text analytics\".[6] The latter term is now used more frequently in business settings while \"text mining\" is used in some of the earliest application areas, dating to the 1980s,[7] notably life-sciences research and government intelligence. The term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80% of business-relevant information originates in unstructured form, primarily text.[8] These techniques and processes discover and present knowledge \u2013 facts, business rules, and relationships \u2013 that is otherwise locked in textual form, impenetrable to automated processing. Text analysis processes Subtasks\u2014components of a larger text-analytics effort\u2014typically include:  Dimensionality reduction is important technique for pre-processing data. It is used to identify the root word for actual words and reduce the size of the text data.[citation needed] Information retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis. Although some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.[9] Named entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on. Disambiguation\u2014the use of contextual clues\u2014may be required to decide where, for instance, \"Ford\" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.[10] Recognition of pattern-identified entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches. Document clustering: identification of sets of similar text documents.[11] Coreference resolution: identification of noun phrases and other terms that refer to the same object. Extraction of relationships, facts and events: identification of associations among entities and other information in texts. Sentiment analysis: discerning of subjective material and extracting information about attitudes: sentiment, opinion, mood, and emotion. This is done at the entity, concept, or topic level and aims to distinguish opinion holders and objects.[12] Quantitative text analysis: a set of techniques stemming from the social sciences where either a human judge or a computer extracts semantic or grammatical relationships between words in order to find out the meaning or stylistic patterns of, usually, a casual personal text for the purpose of psychological profiling etc.[13] Pre-processing usually involves tasks such as tokenization, filtering and stemming. Applications Text mining technology is now broadly applied to a wide variety of government, research, and business needs. All these groups may use text mining for records management and searching documents relevant to their daily activities. Legal professionals may use text mining for e-discovery, for example. Governments and military groups use text mining for national security and intelligence purposes. Scientific researchers incorporate text mining approaches into efforts to organize large sets of text data (i.e., addressing the problem of unstructured data), to determine ideas communicated through text (e.g., sentiment analysis in social media[14][15][16]) and to support scientific discovery in fields such as the life sciences and bioinformatics. In business, applications are used to support competitive intelligence and automated ad placement, among numerous other activities. Security applications Many text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes.[17] It is also involved in the study of text encryption/decryption. Biomedical applications Main article: Biomedical text mining A flowchart of a text mining protocol. An example of a text mining protocol used in a study of protein-protein complexes, or protein docking.[18] A range of text mining applications in the biomedical literature has been described,[19] including computational approaches to assist with studies in protein docking,[20] protein interactions,[21][22] and protein-disease associations.[23] In addition, with large patient textual datasets in the clinical field, datasets of demographic information in population studies and adverse event reports, text mining can facilitate clinical studies and precision medicine. Text mining algorithms can facilitate the stratification and indexing of specific clinical events in large patient textual datasets of symptoms, side effects, and comorbidities from electronic health records, event reports, and reports from specific diagnostic tests.[24] One online text mining application in the biomedical literature is PubGene, a publicly accessible search engine that combines biomedical text mining with network visualization.[25][26] GoPubMed is a knowledge-based search engine for biomedical texts. Text mining techniques also enable us to extract unknown knowledge from unstructured documents in the clinical domain[27]  Software applications Text mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results. Within the public sector, much effort has been concentrated on creating software for tracking and monitoring terrorist activities.[28] For study purposes, Weka software is one of the most popular options in the scientific world, acting as an excellent entry point for beginners. For Python programmers, there is an excellent toolkit called NLTK for more general purposes.", "For more advanced programmers, there's also the Gensim library, which focuses on word embedding-based text representations. Online media applications Text mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site \"stickiness\" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content. Business and marketing applications Text analytics is being used in business, particularly, in marketing, such as in customer relationship management.[29] Coussement and Van den Poel (2008)[30][31] apply it to improve predictive analytics models for customer churn (customer attrition).[30] Text mining is also being applied in stock returns prediction.[32]  Sentiment analysis Sentiment analysis may involve analysis of products such as movies, books, or hotel reviews for estimating how favorable a review is for the product.[33] Such an analysis may need a labeled data set or labeling of the affectivity of words. Resources for affectivity of words and concepts have been made for WordNet[34] and ConceptNet,[35] respectively. Text has been used to detect emotions in the related area of affective computing.[36] Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories. Scientific literature mining and academic applications The issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within the written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within the text without removing publisher barriers to public access. Academic institutions have also become involved in the text mining initiative:  The National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world. NaCTeM is operated by the University of Manchester[37] in close collaboration with the Tsujii Lab,[38] University of Tokyo.[39] NaCTeM provides customised tools, research facilities and offers advice to the academic community. They are funded by the Joint Information Systems Committee (JISC) and two of the UK research councils (EPSRC & BBSRC). With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences. In the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis. The Text Analysis Portal for Research (TAPoR), currently housed at the University of Alberta, is a scholarly project to catalogue text analysis applications and create a gateway for researchers new to the practice. Methods for scientific literature mining Computational methods have been developed to assist with information retrieval from scientific literature. Published approaches include methods for searching,[40] determining novelty,[41] and clarifying homonyms[42] among technical reports. Digital humanities and computational sociology The automatic analysis of vast textual corpora has created the possibility for scholars to analyze millions of documents in multiple languages with very limited manual intervention. Key enabling technologies have been parsing, machine translation, topic categorization, and machine learning. Narrative network of US Elections 2012[43] The automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data. The resulting networks, which can contain thousands of nodes, are then analyzed by using tools from network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes.[44] This automates the approach introduced by quantitative narrative analysis,[45] whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.[43]  Content analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a \"big data\" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents.[46][47][48][49][50] The analysis of readability, gender bias and topic bias was demonstrated in Flaounas et al.[51] showing how different topics have different gender biases and levels of readability; the possibility to detect mood patterns in a vast population by analyzing Twitter content was demonstrated as well.[52][53]  Software Main article: List of text mining software Text mining computer programs are available from many commercial and open source companies and sources. Intellectual property law Situation in Europe Video by Fix Copyright campaign explaining TDM and its copyright issues in the EU, 2016 [3:51] Under European copyright and database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is illegal.", "In the UK in 2014, on the recommendation of the Hargreaves review, the government amended copyright law[54] to allow text mining as a limitation and exception. It was the second country in the world to do so, following Japan, which introduced a mining-specific exception in 2009. However, owing to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licenses for Europe.[55] The fact that the focus on the solution to this legal issue was licenses, and not limitations and exceptions to copyright law, led representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[56]  Situation in the United States US copyright law, and in particular its fair use provisions, means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea, is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed\u2014one such use being text and data mining.[57]  Situation in Australia There is no exception in copyright law of Australia for text or data mining within the Copyright Act 1968. The Australian Law Reform Commission has noted that it is unlikely that the \"research and study\" fair dealing exception would extend to cover such a topic either, given it would be beyond the \"reasonable portion\" requirement.[58]  Implications Until recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word).", "Additionally, text mining software can be used to build large dossiers of information about specific people and events. For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence. In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment."]}