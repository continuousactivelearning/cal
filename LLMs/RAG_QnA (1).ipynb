{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6XA2jFvpUMf"
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.1.14\n",
    "!pip install langchain-experimental==0.0.56\n",
    "!pip install langchain-community==0.0.31\n",
    "!pip install faiss-cpu==1.8.0\n",
    "!pip install pdfplumber==0.11.0\n",
    "!pip install gradio==4.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8wv6LpRpnLi"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "loader = PDFPlumberLoader(\"llm-research-paper.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"Number of pages in the PDF:\",len(docs))\n",
    "\n",
    "docs[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "id": "shTw5MElqRua"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mjEkiYbWqxhI",
    "outputId": "73746eeb-9b7b-4dfa-9153-0315b6fdc2f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created:  80\n",
      "\n",
      "CHUNK : 1\n",
      "Mind’s Eye of LLMs: Visualization-of-Thought Elicits\n",
      "Spatial Reasoning in Large Language Models\n",
      "WenshanWu† ShaoguangMao† YadongZhang†,‡,∗\n",
      "YanXia† LiDong† LeiCui† FuruWei†\n",
      "†MicrosoftResearch ‡EastChinaNormalUniversity\n",
      "Abstract\n",
      "Largelanguagemodels(LLMs)haveexhibitedimpressiveperformanceinlanguage\n",
      "comprehension and various reasoning tasks. However, their abilities in spatial\n",
      "reasoning, a crucial aspect of human cognition, remain relatively unexplored. Humanpossessaremarkableabilitytocreatementalimagesofunseenobjectsand\n",
      "actionsthroughaprocessknownastheMind’sEye,enablingtheimaginationof\n",
      "theunseenworld. Inspiredbythiscognitivecapacity,weproposeVisualization-\n",
      "of-Thought(VoT)prompting. VoTaimstoelicitspatialreasoningofLLMsby\n",
      "visualizingtheirreasoningtraces,therebyguidingsubsequentreasoningsteps. We\n",
      "employedVoTformulti-hopspatialreasoningtasks,includingnaturallanguage\n",
      "navigation,visualnavigation,andvisualtilingin2Dgridworlds. Experimental\n",
      "resultsdemonstratedthatVoTsignificantlyenhancesthespatialreasoningabilities\n",
      "ofLLMs. Notably,VoToutperformedexistingmultimodallargelanguagemodels\n",
      "(MLLMs)inthesetasks. WhileVoTworkssurprisinglywellonLLMs,theability\n",
      "togeneratementalimagestofacilitatespatialreasoningresemblesthemind’seye\n",
      "process,suggestingitspotentialviabilityinMLLMs. Pleasefindthedatasetand\n",
      "codesinourprojectpage. Behindyouisthemarket.\n",
      "\n",
      "CHUNK : 2\n",
      "Verbal\n",
      "Turnrighttothecemetery. Mental Images\n",
      "Turnlefttothelibrary. Mental Images\n",
      "Gostraighttothepostoffice. … garage post office\n",
      "… cinema Chemist’s library Text …\n",
      "Mind’s Eye market cemetery Mind’s Eye\n",
      "Visual\n",
      "Mind’s Eye of Humans Mind’s Eye of LLMs\n",
      "Conventional Prompting\n",
      "Input Output\n",
      "Chain-of-Thought\n",
      "Input Thought Thought … Output\n",
      "Visualization-of-Thought\n",
      "Input Thought Thought … Output\n",
      "Visualization Visualization\n",
      "Figure 1: Humans can enhance their spatial awareness and inform decisions by creating mental\n",
      "imagesduringthespatialreasoningprocess. Similarly,largelanguagemodels(LLMs)cancreate\n",
      "internalmentalimages. WeproposetheVoTpromptingtoelicitthe\"mind’seye\"ofLLMsforspatial\n",
      "reasoningbyvisualizingtheirthoughtsateachintermediatestep. ∗ContributionduringinternshipatMicrosoftResearch. 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202\n",
      "tcO\n",
      "32\n",
      "]LC.sc[\n",
      "3v22630.4042:viXra\n",
      "\n",
      "\n",
      "CHUNK : 3\n",
      "1 Introduction\n",
      "Recently, largelanguagemodels(LLMs)[BCE+23,BMR+20,TLI+23,JSM+23]haveachieved\n",
      "remarkableperformanceonvariouslanguage-relatedtasks. However,despitetheirsuccessinmath\n",
      "reasoning [KGR+23], common sense reasoning [LKH+22], and other reasoning tasks such as\n",
      "symbolicreasoningorlogicreasoning[KGR+23], theirabilitiesinspatialreasoningstillremain\n",
      "underexplored[RFD+21,YBL+23,MHV+24]. Spatialreasoningisanessentialfunctionofhumancognition,allowingustointeractwiththeenvi-\n",
      "ronment. Itfacilitatestasksthatrequireunderstandingandreasoningaboutthespatialrelationships\n",
      "between objects and their motions. The spatial reasoning of language models largely relies on\n",
      "languagetoreasonaboutspatialinformation,whereashumancognitivecapabilitiesextendfarbeyond\n",
      "verbal reasoning. Humans can not only create task-relevant abstract representations from visual\n",
      "perception [BK18, KC22], but also imagine unseen scenes through their mind’s eye. It remains\n",
      "a research topic called mental image [She78] in domains of neuroscience, philosophy of mind,\n",
      "andcognitivescience. Buildinguponthiscognitivefunction, humansfacilitatespatialreasoning\n",
      "bymentalimagemanipulation,suchasnavigation[Tol48],mentalrotation[SM71],mentalpaper\n",
      "folding[SF72],andmentalsimulation[MK09]. Figure1illustratesthehumanprocessinvolvedin\n",
      "anavigationtask. Humansenhancetheirspatialawarenessandinformtheirdecisionsbycreating\n",
      "mentalimagesofaroute,utilizingvarioussensoryinputssuchasnavigationinstructionsoramap\n",
      "image. Subsequently,theysimulaterouteplanningthroughthemind’seye. Inspiredbythiscognitivemechanism, weconjecturethatLLMspossesstheabilitytocreateand\n",
      "manipulatementalimagesinthemind’seyeforspatialreasoning. AsillustratedinFigure1,LLMs\n",
      "could potentially process and understand spatial information in various formats. They might be\n",
      "capableofvisualizinginternalstatesandmanipulatingthesementalimagesthroughtheirmind’seye,\n",
      "therebyguidingsubsequentreasoningstepstoenhancespatialreasoning. Therefore,weproposethe\n",
      "Visualization-of-Thought(VoT)promptingtoelicitthisability. ThismethodleverageLLMsto\n",
      "visualizetheirreasoningstepsandinformsubsequentsteps,implementingtheconceptofvisuospatial\n",
      "sketchpad [Bad92]. VoTadoptszero-shotpromptinginsteadofrelyingonfew-shotdemonstrations\n",
      "or text-to-image visualization with CLIP [RKH+21]. This choice stems from LLMs’ ability to\n",
      "acquirevariousmentalimagesfromtext-basedvisualart[SB14,SMM21,Reg19]. ToevaluatetheeffectivenessofVoTinspatialreasoning,weselectedthreetasksthatrequirespatial\n",
      "awarenessinLLMs,includingnatural-languagenavigation[YBL+23],visualnavigation,andvisual\n",
      "tiling. Thesetasksrequireanunderstandingofspace,direction,andgeometricshapereasoning. To\n",
      "emulatehuman-likemultisensoryperception,wedesigned2Dgridworldsusingspecialcharacters\n",
      "asenrichedinputformatsfortheLLMsinvisualnavigationandvisualtilingtasks. Wecompared\n",
      "differentmodels(GPT-4,GPT-4V)andpromptingtechniquesacrossthesethreetasks. Thefindings\n",
      "revealthattheVoTpromptingproposedinthispaperconsistentlyinducesLLMstovisualizetheir\n",
      "reasoning steps and inform subsequent steps. Consequently, this approach achieved significant\n",
      "performanceimprovementsonthecorrespondingtasks.\n",
      "\n",
      "CHUNK : 4\n",
      "Themaincontributionsofthispaperinclude:\n",
      "1. We shed light on LLMs’ mental image for spatial reasoning from a cognitive perspective,\n",
      "conductingquantitativeandqualitativeanalysesonthemind’seyeofLLMsanditslimitations. We\n",
      "alsoexplorecuesabouttheoriginofthisgeneralizedabilityfromcodepre-training. 2. Wedeveloptwotasksof\"visualnavigation\"and\"visualtiling\", alongwithcorresponding\n",
      "syntheticdatasets,emulatingvarioussensoryinputsforLLMs. Thesetasksarestructuredtosupport\n",
      "varyinglevelsofdifficulty,offeringawell-designedtestbedfortheresearchonspatialreasoning.\n",
      "\n",
      "CHUNK : 5\n",
      "3. WeproposeVisualization-of-Thought(VoT)promptingtoelicitthemind’seyeofLLMs\n",
      "forspatialreasoningandprovideempiricalevaluationsonthreetasks. Experimentresultsprovethe\n",
      "effectivenessofVoTpromptingcomparedwithotherpromptingmethodsandexistingMLLMs. This\n",
      "abilitytogeneratementalimagestofacilitatespatialreasoningresemblesthemind’seyeprocess,\n",
      "suggestingitspotentialviabilityinMLLMs. 2\n",
      "\n",
      "\n",
      "CHUNK : 6\n",
      "2 SpatialReasoning\n",
      "Spatialreasoningreferstotheabilitytocomprehendandreasonaboutthespatialrelationshipsamong\n",
      "objects,theirmovements,andinteractionswiththeenvironment. Thisskillisvitalforawiderangeof\n",
      "real-worldapplicationssuchasnavigation,robotics,andautonomousdriving. Thesefieldsnecessitate\n",
      "actionplanningbasedonvisualperceptionandaconcreteunderstandingofspatialdimensions. Althoughseveraltasksanddatasets[WBC+15,SZL22,MK22,LB18,RAB+20]havebeendevel-\n",
      "opedtoprobethespatialsemanticsembeddedintext,existingresearcheffortsoftenfocusonhow\n",
      "spatialtermsarelinguisticallystructured. Recently,significantachievementsandimpressiveresults\n",
      "havebeenachievedinthesebenchmarksbyconvertingspatialtermstologicalformsthroughLLMs\n",
      "andadoptinglogicprogramming[YIL23]. Thisimpliesthatexcellinginthesetasksdoesnotnec-\n",
      "essarilyequatetoagenuineunderstandingofspatialinformationbyLLMs,nordoesitprovidean\n",
      "accuratemeasureoftheirspatialawareness. Spatialawarenessinvolvesunderstandingspatialrelationships,directions,distances,andgeometric\n",
      "shapes,allofwhichareessentialforactionplanninginthephysicalworld. Toevaluatethespatial\n",
      "awarenessandspatialreasoningabilitiesofLLMs,wehaveselectedtasksthattestnavigationand\n",
      "geometricreasoningskills,includingnaturallanguagenavigation,visualnavigationandvisualtiling. 2.1 NaturalLanguageNavigation\n",
      "Natural language navigation task [YBL+23] was inspired by prior research on human cogni-\n",
      "tion[GDB17]presentingparticipantswithsequentialtransitionssampledfromagraphstructure. Inthiscontext,asquaremapisdefinedbyasequenceofrandomwalkinstructionsandassociated\n",
      "objectsateachlocation,denotedasW ={(l ,o ),(l ,o ),...,(l ,o )}.\n",
      "\n",
      "CHUNK : 7\n",
      "GivenasquaremapW,\n",
      "1 1 2 2 n n\n",
      "andsequenceofnavigationinstructionsI = {i ,...,i },thetaskforthemodelistoidentifythe\n",
      "1 k\n",
      "associatedobjecto∈W atthespecifiedlocationlwhichisdeterminedbythenavigationinstructions,\n",
      "asdetailedinEquation1andexemplifiedinAppendixB.2. o∼p(o∈W|W ={(l ,o ),(l ,o ),...,(l ,o )},I) (1)\n",
      "1 1 2 2 n n\n",
      "2.2 VisualNavigation\n",
      "Visualnavigationtaskpresentsasynthetic2DgridworldtoLLM,challengingittonavigateusing\n",
      "visualcues. Themodelmustgeneratenavigationinstructionstomoveinfourdirections(left,right,\n",
      "up,down)toreachthedestinationfromthestartingpointwhileavoidingobstacles. Thisinvolvestwo\n",
      "sub-tasks: routeplanningandnextstepprediction,requiringmulti-hopspatialreasoning,whilethe\n",
      "formerismorecomplex. TaskinstructionsareavailableinFigure6inappendix. Formulation The model is presented with a grid map M consisting of k consecutive edges\n",
      "E ={e(s ,s ),e(s ,s ),··· ,e(s ,s )},wherethestartingpointanddestinationares ands\n",
      "0 1 1 2 k−1 k 0 k\n",
      "respectively,asshowninFigure2. Routeplanningtaskistogenerateasequenceofcorrectdirections\n",
      "D = {d(s ,s ),d(s ,s ),··· ,d(s ,s )},asdefinedinEquation2. GivenM andtnavigation\n",
      "0 1 1 2 k−1 k\n",
      "instructions D = {d(s ,s ),··· ,d(s ,s )}, next step prediction task is to identify the\n",
      "t,0<t<k 0 1 t−1 t\n",
      "correctdirectiond(s ,s )ofthenextstep,asdefinedinEquation3. t t+1\n",
      "D ∼p({d(s ,s ),d(s ,s ),··· ,d(s ,s )}|M) (2)\n",
      "0 1 1 2 k−1 k\n",
      "(a)k=2 (b)k=3 (c)k=4 (d)k=5 (e)k=6 (f)k=7\n",
      "Figure2: Examplesofanavigationmapunderdifferentsettingsofk,withemojiofhouseindicating\n",
      "thestartingpoint,andemojiofofficeindicatingthedestination. 3\n",
      "\n",
      "\n",
      "CHUNK : 8\n",
      "d∼p(d(s ,s )|M,D ) (3)\n",
      "t t+1 t,0<t<k\n",
      "Implementation Thenavigationmap’sunderlyinggraphissemi-Eulerian,alternatingbetween\n",
      "horizontalandverticaledges,with2k+1possiblespatialconfigurationsforak-hopnavigationmap. Foreachmapandsetofk navigationinstructions,k−1question-and-answer(QA)instances,i.e. \"whatisthenextstep?\"arecreated.\n",
      "\n",
      "CHUNK : 9\n",
      "FurtherimplementationdetailsareinAppendixA.1. 2.3 VisualTiling\n",
      "Introduced by [Gol66], polyomino tiling is a classic spatial reasoning challenge. We extend this\n",
      "concepttotesttheLLM’sabilitytocomprehend,organize,andreasonwithshapesinaconfinedarea,\n",
      "thusenhancingtheevaluationofspatialreasoningskills. AsdepictedinFigure3,thetaskinvolvesa\n",
      "rectanglewithunfilledcellsandvariouspolyominopieces,liketheI-tetrominomadeoffouraligned\n",
      "squares. Themodelmustselecttheappropriatepolyominovariant,suchaschoosingtheorientation\n",
      "fortheI-tetromino,tosolvetheQApuzzle. TaskinstructionsareprovidedinFigure7inappendix. Formulation The model is presented with a rectangle R masked with k unique polyominoes\n",
      "MP = {mp ,··· ,mp },2correspondingvariantsofeachpolyominov = {v ,v },anda\n",
      "1 k i<=k i1 i2\n",
      "polyomino query q ∈MP. Visual tiling task is to identify the correct variant of q, as defined in\n",
      "Equation4. v ∼p(v |R,{mp ,··· ,mp },{v ,v ··· ,v ,v },q) (4)\n",
      "q 1 k 11 12 k1 k2\n",
      "Implementation The dataset comprises valid spatial arrangements generated through existing\n",
      "algorithms[ES03,GN07],withrandommaskingofpolyominoestocreateQApuzzles. Detailsare\n",
      "providedinAppendixA.2. (a)Fit2piecesintoamaskedrectangle (b)Fit3piecesintoamaskedrectangle\n",
      "Figure 3: Example of visual tiling with masked polyomino pieces. Variants of those polyomino\n",
      "piecesincludingrotationandreflectionarenotshowninthisfigure. 3 Visualization-of-ThoughtPrompting\n",
      "Consideringthewayhumansprocessspatialinformationduringtaskslikenavigation,it’scommon\n",
      "tocreatementalimages,suchasmaps,toenhancespatialawarenessorsimulatingmovementsto\n",
      "informdecision-making. OurobjectiveistoelicitthespatialawarenessofLLMsandgroundtheir\n",
      "reasoningbyvisualizingtheconsequenceoftheirintermediatereasoningsteps. WeintroduceVisualization-of-Thought(VoT)prompting: \"Visualizethestateaftereachrea-\n",
      "soning step.\" This new paradigm for spatial reasoning aims to generate reasoning traces and\n",
      "visualizationsinaninterleavedmanner. QualitativeresultsofthisapproacharepresentedinFigure4.\n",
      "\n",
      "CHUNK : 10\n",
      "Weusep todenoteapre-trainedLMwithparametersθ,x,y,ztodenotealanguagesequence,and\n",
      "θ\n",
      "vtodenoteavisualizationsequenceintextform. Inamulti-hopspatialreasoningtaskwithinputx,\n",
      "CoTpromptinggeneratesaseriesofintermediatestepsz ,··· ,z ,eachstepz ∼p (z |x,z )\n",
      "1 n i θ i 1···i−1\n",
      "is sampled sequentially, followed by the output y ∼ p (y|x,z ). As shown in Figure 1, VoT\n",
      "θ 1···n\n",
      "promptingenhancesthisprocessbyaddingavisuospatialsketchpadtoeachintermediatestepz ,\n",
      "i\n",
      "thenthesubsequentstepz issampledconditionedonpriorstepsz andvisualizationsv . i+1 1···i 1···i\n",
      "As defined in the Equation 5 and 6, it forms interleaved reasoning traces and visualizations. A\n",
      "qualitativecomparisonbetweenoutputsofVoTandCoTisprovidedinFigure8ainappendix. v ∼p (v |prompt ,x,z ,v ) (5)\n",
      "i θ i VoT 1···i 1···i−1\n",
      "4\n",
      "\n",
      "\n",
      "CHUNK : 11\n",
      "Visual Navigation Visual Tiling Natural Language Navigation\n",
      "You have been given a 3 by 3 square grid. Initially,\n",
      "you are at the bottom-left corner…find a cassette\n",
      "player…go right…a wool, go right…a conch, go\n",
      "up…a moving van, go left…a confectionery store,\n",
      "go left…a pot pie, go up…a siamang, go right…a\n",
      "black-and-white colobus, go right…a minivan. Now you have all the information on the map. You\n",
      "Provided: I T L start at where the cassette player is located, then\n",
      "To fit all the provided polyominoes into the you go right by one step, go right…go up…go\n",
      "Starting from , provide the steps to navigate empty squares, what's the correct variation of left…go left…go up…go right…go down by one\n",
      "to . Tetromino T?\n",
      "\n",
      "CHUNK : 12\n",
      "step. What will you find? Visualize the state after each reasoning step. Visualize the state after each reasoning step. Visualize the state after each reasoning step. Analyze I Analyze L Analyze T\n",
      "1. Move right 2. Move down 3. Move left …\n",
      "4. Move down 5. Move left 6. Move down 1. Place I 2.\n",
      "\n",
      "CHUNK : 13\n",
      "Place L 3. Place T\n",
      "Figure4: ExamplesofVoTpromptinginthreetasks,whereLLMgenerates2Dgridsastext-form\n",
      "mentalimages. Thegeneratedreasoningtracesandvisualizationsformaninterleavedsequenceto\n",
      "trackthestateovertime. The2Dgridsintheinputandresponsesarecomposedofspecialcharacters. FullresponsescouldbefoundinAppendixB. z ∼p (z |prompt ,x,z ,v ) (6)\n",
      "i+1 θ i+1 VoT 1···i 1···i\n",
      "ThisreasoningparadigmenablesLLMswithvisualstatetracking. Weintroducetheconceptofa\n",
      "state,denotedass =[x,z ,v ]representingapartialsolutionatstepiwiththeinput,the\n",
      "i 1···i 1···i−1\n",
      "sequenceofintermediatestepsz andthesequenceofvisualizationsv . 1···i 1···i−1\n",
      "v ∼p (v |prompt ,x,z ,v )\n",
      "i θ i VoT 1···i 1···i−1\n",
      "(7)\n",
      "∼p (v |prompt ,s )\n",
      "θ i VoT i\n",
      "As shown in Equation 7, visual state tracking is implemented by generating the visualization v\n",
      "i\n",
      "asrepresentationoftheinternalstates aftereachreasoningstepz (e.g.v couldbeagridofthe\n",
      "i i i\n",
      "navigationmapmarkedwithpathorafilledrectangle).Groundedbythevisualstatetrackingsequence,\n",
      "thesubsequentstateisderivedbys ∼p (s |prompt ,x,s ,v ). Thismechanismallows\n",
      "i+1 θ i+1 VoT i i\n",
      "forthederivationofsubsequentstates,reflectingspatiotemporalcausalityandenhancingthespatial\n",
      "reasoningcapabilitiesofLLMsinagroundedcontext. 4 Experiment\n",
      "4.1 Setup\n",
      "Forthevisualtaskswhereacounterpartimageexistsforeachtextinput,weconductadditionalexper-\n",
      "imentswithamultimodalmodel. Specifically,weadoptGPT-4[OA+23]andGPT-4Vision[Ope23]\n",
      "via Azure OpenAI API as they’re state of the art LLM and multimodal model respectively. API\n",
      "settingsaretemperature0asgreedydecodingandtopp1,withmodelversionsof1106-previewand\n",
      "vision-preview. Forallexperimentsweadoptzero-shotprompting. DependingonwhethertheLLMisexplicitlypromptedtovisualizeintermediatesteps,weexperiment\n",
      "withthreesettingsofGPT-4,includingzero-shotCoTprompting(GPT-4CoT),GPT-4w/oVizwhere\n",
      "visualizationisexplicitlydisabledduringreasoning,andVoTprompting(GPT-4VoT).Additional\n",
      "settingofGPT-4VisionwithcounterpartimageinputisGPT-4VCoT.Promptsareasfollowing:\n",
      "5\n",
      "\n",
      "\n",
      "CHUNK : 14\n",
      "• GPT-4CoT:Let’sthinkstepbystep. • GPT-4w/oViz: Don’tusevisualization. Let’sthinkstepbystep. • GPT-4VCoT:Let’sthinkstepbystep. • GPT-4VoT:Visualizethestateaftereachreasoningstep. TaskinstructionsandexamplescouldbefoundinAppendixB. 4.2 Dataset\n",
      "NaturalLanguageNavigation Wegenerate200squaremapsofsize3x3whichisdescribedby9\n",
      "landmarksinsnakeordertraversal,andasetofnavigationinstructions. VisualNavigation Wegenerate496navigationmapsand2520QAinstancesintotal,covering\n",
      "variousmapsizes,upto7×9and9×7.\n",
      "\n",
      "CHUNK : 15\n",
      "ThedatadistributionisprovidedinTable4inappendix. Visual Tiling We first generate multiple unique configurations to fill a 5 x 4 rectangle with 5\n",
      "polyominopiecesincludingtwoItetrominoes,twoTtetrominoesandoneLtetromino. Thenwe\n",
      "randomlymaskedtwoorthreepiecesofdifferenttypesandgenerateQAinstanceforeachmasked\n",
      "pieces. ThetotalnumberofQAinstancesis796,andweshowdatasetdetailsinTable5inappendix. 4.3 Metric\n",
      "Weextracttheanswerfrommodeloutputbypatternmatching. Fortasksexceptforrouteplanning,\n",
      "we calculate accuracy by Equation 8. We adopted sub-string matching† as f to determine\n",
      "correct\n",
      "correctness. n\n",
      "(cid:88)\n",
      "acc= f (extracted_answer,ground_truth)/n (8)\n",
      "correct\n",
      "i\n",
      "For the route planning task which predicts a sequence of navigation instructions, we reject any\n",
      "sequencesexceeding100instructions,consideringthemtoberandomguesses. Wethennormalize\n",
      "thenavigationinstructionsbyexecutingeachnavigationinstruction. Thoseinstructionswhichviolate\n",
      "navigationruleswillbeignored. Thelengthtofnormalizedinstructionsequenceisconsideredasthe\n",
      "temporaldistanceagainstthestartingpoint. Giventheground-truthofknavigationinstructions,the\n",
      "completingrateofrouteplanningist/k. Forthedatasetofnmaps,wereporttwometricsincluding:\n",
      "1. Average completing rate:\n",
      "(cid:80)nt\n",
      "/k /n. Average completing rate among all instruction\n",
      "i i i\n",
      "sequences,reflectingLLM’seffectivenessofrouteplanning. 2. Successrate:\n",
      "(cid:80)n(t\n",
      "==k )/n. Thismetricrepresentstheproportionofinstructionse-\n",
      "i i i\n",
      "quenceswitht=k,i.e.,reachingthedestination. 4.4 Results\n",
      "As illustrated in Table 1, GPT-4 VoT significantly outperforms other settings in all tasks across\n",
      "all metrics. The significant gap when comparing GPT-4 VoT with GPT-4V CoT and GPT-4 w/o\n",
      "Vizdemonstratesthateffectivenessofvisualstatetracking,whichallowsLLMsvisuallyinterpret\n",
      "theiractionswithinangroundedworld. Andinthenaturallanguagenavigationtask,GPT-4VoT\n",
      "outperformsGPT-4w/oVizby23.5%. Inthevisualtasks,thenoticeableperformancegapbetween\n",
      "GPT-4CoTandGPT-4VCoTindicatesthatLLMgroundedwith2Dgridcouldpossiblyoutperform\n",
      "aMLLMinchallengingspatialreasoningtasks. Ontheotherhand,performanceofGPT-4VoTisstillfarfromperfectinalltasks,especiallyinthe\n",
      "mostchallengingrouteplanningtask. Despitethesetasksarerelativelyeasyforhumans,performance\n",
      "of LLMs drops significantly as task difficulty increases. Details on performance trends across\n",
      "difficultylevelsareprovidedinfigure9andtable6inappendix. †Weusethistermforsimplicity.Innaturallanguagenavigationtasks,LLMsoftenoutputadditionalwords\n",
      "intheextractedanswerbesidestheexpectedobjectname.\n",
      "\n",
      "CHUNK : 16\n",
      "Forexample,\"Answer: Youwillfind...\". Inthis\n",
      "case,sub-stringmatchingisadoptedwithoutaffectingthecorrectness.Otherwise,exactmatchingisadoptedfor\n",
      "multiplechoicequestionsinvisualtasks. 6\n",
      "\n",
      "\n",
      "CHUNK : 17\n",
      "VisualNavigation\n",
      "Natural-Language\n",
      "Settings VisualTiling\n",
      "RoutePlanning NextStep Navigation\n",
      "Prediction\n",
      "CompletingRate SuccRate\n",
      "GPT-4CoT 37.02 9.48 48.61 54.15 54.00\n",
      "GPT-4w/oViz 37.17 10.28 48.49 46.98 35.50\n",
      "GPT-4VCoT 33.36 5.65 46.59 49.62 /\n",
      "GPT-4VoT 40.77 14.72 55.28 63.94 59.00\n",
      "Table 1: Performance of different GPT-4/4V settings in all tasks. Underline denotes statistical\n",
      "significancewithp<0.05whencomparingGPT-4VoTagainstallbaselinesusingtwo-samplez-test,\n",
      "whilep<0.16isobservedcomparedwithGPT-4CoTinnaturallanguagenavigationtask.\n",
      "\n",
      "CHUNK : 18\n",
      "5 Analysis\n",
      "Asexplainedinsection3,oneofthecoreaspectsofVoTliesinenablingLLMswithvisualstate\n",
      "tracking. Duringtheexperiments,itwasobservedthatGPT-4CoToccasionallyexhibitedthisreason-\n",
      "ingpatternacrossseveraltaskswithexceptionofrouteplanning. Besides,incorrectvisualizationsof\n",
      "VoTarecommonlyobservedinmodeloutputs. Inthissection,ouranalysisofVoTprimarilyfocuses\n",
      "onthreequestions: (1)Dovisualstatetrackingbehaviorsdifferamongpromptingmethods? (2)How\n",
      "visualizationsenhancefinalanswers? (3)CanVoTbenefitlesspowerfullanguagemodels? 5.1 Dovisualstatetrackingbehaviorsdifferamongpromptingmethods? Foreachmodeloutput,weextractthesequenceofvisualizationssampledpriortogeneratingthefinal\n",
      "answeranddiscardanyvisualizationsgeneratedthereafter. Thenwecomparethesequencelengthl\n",
      "v\n",
      "withthenumberofreasoningstepsl . WecalculateComplete\n",
      "Tracking(cid:80)n(l\n",
      "==l )/nwhena\n",
      "s i v s\n",
      "visualizationv correspondstoeachstates . Similarly,wecalculatethePartial Trackingmetric\n",
      "i i\n",
      "as\n",
      "(cid:80)n(l\n",
      "> 0)/n whenat leastone visualization ispresent beforethefinal answerisgenerated. i v\n",
      "Figure 5 shows the significant differences between these settings. In the GPT-4 CoT setting, it\n",
      "demonstratednoticeabletrackingrateacrossalmostalltasksexceptrouteplanning. Thisobservation\n",
      "impliesthatLLMsinherentlyexhibitthecapabilityofvisualstatetrackingwhenspatiotemporal\n",
      "simulationisintegraltoreasoning. Ontheotherhand,thevisualstatetrackingbehaviorissensitivetopromptstovaryingdegrees. As\n",
      "showcasedinFigure8inappendix,afterremoving\"reasoning\"fromthepromptofVoT,thevisualiza-\n",
      "tionsaresampledafterGPT-4generatesthewronganswer. Consequently,explicitlypromptingLLMs\n",
      "tovisualizetheirreasoningtraceswithVoTmarkedlyimprovesthevisualtrackingrate,thereby\n",
      "enhancing overall performance. The potential contribution of code pre-training to this emergent\n",
      "capabilityisfurtherexploredinAppendixC. 100 96.2 92.9 87.1\n",
      "59.3 57.4\n",
      "50\n",
      "18.1 20\n",
      "0 1.20 5 0.50\n",
      "Route NextStep Visual Natural\n",
      "PlanningPrediction Tiling language\n",
      "Navigation\n",
      "%\n",
      "GPT-4VoT GPT-4CoT GPT-4w/oViz\n",
      "100 99.2 95.391 87.4\n",
      "80.5\n",
      "59.2\n",
      "50\n",
      "30.5\n",
      "18.3\n",
      "0 1.80 5.5 0\n",
      "Route NextStep Visual Natural\n",
      "PlanningPrediction Tiling language\n",
      "Navigation\n",
      "(a)Completetrackingrate\n",
      "%\n",
      "GPT-4VoT GPT-4CoT GPT-4w/oViz\n",
      "(b)Partialtrackingrate\n",
      "Figure5: trackingrateofdifferentsettingsacrossalltasksk. 5.2 Howvisualizationsenhancefinalanswers?\n",
      "\n",
      "CHUNK : 19\n",
      "Ideally,VoTissupposedtogenerateanaccuratevisualizationv ateachstep,sothatsubsequentstep\n",
      "i\n",
      "z couldbedeterminedcorrectly. Thisreliesonthespatialvisualizationandspatialunderstanding\n",
      "i+1\n",
      "capability of LLMs. To evaluate these capabilities of LLMs in these tasks, we extract the final\n",
      "visualizationfromeachmodeloutputunderthesettingGPT-4VoTinvisualnavigationandpolyomino\n",
      "7\n",
      "\n",
      "\n",
      "CHUNK : 20\n",
      "tiling task. Specifically, for visual navigation task, we extract the visualized map where LLM\n",
      "completed all navigation instructions. For polyomino tiling, we extract the rectangle filled with\n",
      "correspondingpolyominopiece. Thespatialvisualizationcapabilityismeasuredbytwocriteria:\n",
      "(1)Compliance,indicatingwhetherthemanipulationofmentalimagesatisfiesrequirementssuch\n",
      "asavoidingoverlapandnavigatingaroundobstacles.\n",
      "\n",
      "CHUNK : 21\n",
      "(2)Accuracy,indicatingwhetherthemental\n",
      "imagealignswiththecorrespondingstate. Thespatialunderstandingcapabilityismeasuredbythe\n",
      "proportionofcorrectanswerswhenthecorrespondingvisualizationisgeneratedaccurately. As could be seen from Table 2, LLMs demonstrate promising potential in performing multi-hop\n",
      "visualizationwhileadheringtospatialconstraints,withcomplianceratesofapproximately51-52%. However,therelativelylowaccuracyofstatevisualization(around24%-26%)indicatesaneedfor\n",
      "significant improvements in this area. Despite this limitation, LLMs are able to make correct\n",
      "decisionsin65%-77%ofthecaseswhenaccurateinternalstatevisualizationsaregenerated,\n",
      "whichenhancesgroundednessandcontributestonotableperformancegains. Severalcasestudiesare\n",
      "providedinAppendixEforinterestedreaders. SpatialVisualization SpatialUnderstanding\n",
      "Task\n",
      "Compliance Accuracy Accuracy\n",
      "VisualNavigation 51.14 26.48 65.16\n",
      "VisualTilling 52.01 24.25 77.20\n",
      "Table2: Spatialvisualization/understandingevaluationinvisualnavigationandvisualtilingtask. Ontheotherhand,VoTpromptingmightunderperforminthosetaskswhereLLMscanleverage\n",
      "logicalreasoningwithoutvisualizinginternalstates.Weconductedexperimentsinnaturallanguage\n",
      "navigationwithinaring[YBL+23],wherenavigationinstructionsareeitherclockwiseorcounter-\n",
      "clockwisemovements. Bynormalizingeachinstructiontoasignednumber,GPT-4convertsthistask\n",
      "tomathematicalcalculationofaddingandmodulusoperation. Forexample,instructionsof15steps\n",
      "clockwiseand3stepscounter-clockwisearenormalizedto(15-3)%12. ResultsshowthatGPT-4\n",
      "CoToutperformsGPT-4VoTwith52.5%VS49.5%among200testinstanceswithringsizeof12. 5.3 CanVoTbenefitlesspowerfullanguagemodels? ToevaluatetheefficacyofVoTonlesspowerfullanguagemodels,weconductedexperimentsacross\n",
      "variousmodelfamilies[BMR+20,OA+23,TLI+23]andmodelsizes, includingGPT-3.5turbo,\n",
      "LLAMA3-8B-InstructandLLAMA3-70B-Instruct. WeaccessGPT-3.5viaAzureOpenAIAPI\n",
      "withmodelversion1106-previewandapplygreedydecodingtoallmodels. AsshowninTable3, withinthesamemodelfamily, performanceimprovesacrossalltaskswith\n",
      "increasesinmodelsize. LLAMA3-70BVoTsignificantlyoutperformsthebaselineacrossalltasks\n",
      "exceptforvisualtiling, whereitalignscloselywithresultsobservedinGPT-4. Thisconsistency\n",
      "suggests that VoT offers a scaling advantage when applied to more advanced models, markedly\n",
      "enhancingperformanceinlargermodels. Incontrast,lesscapablemodelstendtorelyonrandom\n",
      "guessing, especiallyinspatialreasoningtasks. Forinstance, intherouteplanningtask, GPT-3.5\n",
      "CoToftenresortstospeculativeresponses,randomguessinginnearlyhalfoftheinstances,which\n",
      "leadstoexhaustionofoutputtokens. WhileGPT-3.5VoTeffectivelyminimizesrandomguesses,\n",
      "such occurrences become increasingly rare with GPT-4 CoT as the model size expands. On the\n",
      "otherhand,therelianceonrandomguessingintroducesunpredictabilityinperformancetrendsfor\n",
      "lesspowerfulmodels. Itsuggeststheirlimitationsinsustainingreliablereasoningprocessesacross\n",
      "differentdifficultylevels.\n",
      "\n",
      "CHUNK : 22\n",
      "DetailsonperformancetrendsareprovidedinAppendixD. 6 RelatedWorks\n",
      "SpatialReasoningoverText Spatialreasoningandspatiallanguageunderstanding[KPM20]in\n",
      "NLPdomainmainlyfocusonsemanticrepresentation[CBGG97,Bat10,HK11],spatialinformation\n",
      "extraction[RMK18,KVOM11],learningandreasoning[KM15,SLYA17,KFP19]. Recentadvance-\n",
      "mentshavefurtherexploredspatialreasoningwithinthecontextoflargelanguagemodels(LLMs). Toimprovemulti-hopspatialreasoningskillsoflanguagemodels,severalworks[MFNK21,MK22]\n",
      "proposedtopretrainlanguagemodelswithsyntheticdatasets. Anincreasingnumberofdatasetwere\n",
      "thendevelopedtocoversvarioustypeofspatialrelationsin2Dvisualscenes[WBC+15,SZL22],\n",
      "8\n",
      "\n",
      "\n",
      "CHUNK : 23\n",
      "VisualNavigation\n",
      "Natural-Language\n",
      "Settings VisualTiling\n",
      "RoutePlanning NextStep Navigation\n",
      "Prediction\n",
      "CompletingRate SuccRate\n",
      "GPT-3.5CoT 16.10 2.62 17.42 44.10 8.50\n",
      "GPT-3.5VoT 19.02 1.61 13.10 47.99 9.00\n",
      "LLAMA3-8BCoT 4.65 0 28.73 47.24 16.50\n",
      "LLAMA3-8BVoT 4.97 0.2 26.75 46.73 15.50\n",
      "LLAMA3-70BCoT 19.90 2.62 49.01 56.41 26.00\n",
      "LLAMA3-70BVoT 30.24 5.85 54.09 56.03 32.50\n",
      "Table 3: Performance of VoT in GPT-3.5 and LLAMA3 models. Underline denotes statistical\n",
      "significancewithp<0.05comparedtocorrespondingCoTbaselineusingtwo-samplez-test.\n",
      "\n",
      "CHUNK : 24\n",
      "geometricpatterns[Cho19]and3Dspatialinformation[AMKK21,HZC+23]. [FML+22]investi-\n",
      "gatedspatialreasoningcapabilitiesoftransformer-basedmodelsintheUIgroundingsetting. Onthe\n",
      "otherhand,someworksadoptedin-contextlearning,leveragingLLMsforgeneralpurposereasoning\n",
      "toconvertspatialinformationtologicforms[YIL23],orasageneralpatternmachineforsequence\n",
      "transformation[MXF+23]. Recently,severalworksfocusedonevaluatingspatialreasoningofLLMs\n",
      "ascognitivecapabilityonnavigation[YBL+23]andplanningtasks[MHV+24]amongvariousspatial\n",
      "structures. Whilemostexistingworksrelyonlinguisticsemanticsandverbalreasoning,andmight\n",
      "notalwaysnecessitatespatialawareness,weproposetoelicitmind’seyeofLLMsinspatialreasoning\n",
      "taskswithvariousformatsfromacognitiveperspective. TheVoTpromptinginducesLLMstocreate\n",
      "mentalimagesforvisualizingtheirinternalstatesandinformsubsequentreasoningstep. WorldModelsofLLMs WhiletherehavebeenmanytheoreticaldebatesaboutwhetherLLMs\n",
      "caneffectivelylearnaninternalworldmodelfromungroundedformalone[BHT+20,MGSS21],\n",
      "[LeC22]advocatedthatworldmodelsshouldrepresentperceptsandactionplansatmultiplelevels\n",
      "ofabstractionandmultipletimescales,withthecapabilityofplanning,predicting,andreasoning. [LWG+22]proposedtogroundLLMinthephysicalworldbyreasoningovertheexperimentalresults\n",
      "predictedbyexternalsimulation. [HGM+23]furtherleveragedLLMsasworldmodelstopredictthe\n",
      "subsequentstatesbyactionsimulation,givenpredefinedstatesandactionspertask. Ontheother\n",
      "hand,anincreasingnumberofstudiesfocusoninvestigatinginternalrepresentationsofLLMs.[PP22,\n",
      "AKH+21]showedthatbyutilizingin-contextlearning,LLMs’learnedrepresentationscanbemapped\n",
      "togroundedperceptualandconceptualstructureincolorandspatialdomains. Moreover,[GT23]\n",
      "and[NLW23]discoveredlinearrepresentationsofspace,timeandgamestateinspecificallytrained\n",
      "LLMs,whichareimportantfordynamiccausalworldmodels. Ourworkdoesnotprobetheinternal\n",
      "representations of specialized LLMs, nor does it depend on external simulation engine or state\n",
      "definitions. WedemonstrateLLMs’zero-shotcapabilityofrepresentingtheirpreceptsatanabstract\n",
      "level,predictingandtrackingtheinternalstatesovertimetogenerateactionplansinmulti-hopspatial\n",
      "reasoningtasks,whichpossiblymirrorsthecausalworldmodelwithinLLMs. 7 Conclusion\n",
      "ThisstudyintroducesVisualization-of-ThoughtPrompting(VoT),inspiredbythehumancognitive\n",
      "functionofvisualizingandmanipulatingmentalimagesthroughthemind’seye. Wehavedemon-\n",
      "stratedthatVoTenablesLLMstoexhibitthemechanismof\"themind’seye\",asevidencedbytheir\n",
      "performanceinmulti-hopspatialreasoningtasksandourcomprehensiveanalysisofthereasoning\n",
      "traces. Remarkably, VoTenableLLMstooutperformstate-of-the-artmultimodallargelanguage\n",
      "models(MLLMs)inthetestedvisualtasks. WhileVoTdemonstratesimpressiveefficacyinLLMs,\n",
      "thisemergentcapabilitytocreatementalimagestoenhancespatialreasoningresemblesthemind’s\n",
      "eyeprocess,suggestingitspromiseinMLLMs. BuildingonthesuccessofexperimentswithGPT-4,weplantoinvestigatehowVoTcanfutherelicit\n",
      "\"themind’seye\"inMLLMstoenhancetheirspatialawareness. Additionally,ourfutureeffortswill\n",
      "focusonautomaticdataaugmentationfromreal-worldscenarios,aimingtoidentifyeffectivemethods\n",
      "forlearninggeneralizedinternalrepresentationsofmentalimages. Thiswillfurtherimprovethe\n",
      "mind’seyeofLLMs,ultimatelycontributingtotheadvancementoftheircognitiveandreasoning\n",
      "abilities.\n",
      "\n",
      "CHUNK : 25\n",
      "9\n",
      "\n",
      "\n",
      "CHUNK : 26\n",
      "Limitations\n",
      "ThisworkonlyscratchesthesurfaceofspatialreasoningofLLMs. Bothmentalimagesandvisual\n",
      "statetrackingrelyontheemergentabilityofadvancedLLMs. Therefore,itmightcauseperformance\n",
      "deteriorationinlessadvancedlanguagemodelsormorechallengingtasks. Besides,duetothelimited\n",
      "dataexposureandalackofexplicitinstructiontuning, visualstatetrackingofcurrentLLMsare\n",
      "sensitivetoprompts. Forexample,whenexplicitlypromptedwith\"useascii-art\",thetrackingrate\n",
      "willsignificantlyincreasetherebyboostingperformance,whileremoving\"reasoning\"fromtheVoT\n",
      "promptwillcauseadecreaseoftrackingrate. Moreover,thementalimagestestedinourworkare\n",
      "limitedto2Dgrid. Tostrengththemind’seyeofLLMs,morediverseandcomplicatedrepresentation\n",
      "shouldbeexploredinthefuture,suchascomplexgeometricshapesandeven3Dsemanticsshownin\n",
      "Figure11inappendix. References\n",
      "[AKH+21] MostafaAbdou,ArturKulmizev,DanielHershcovich,StellaFrank,ElliePavlick,and\n",
      "AndersSøgaard. Canlanguagemodelsencodeperceptualstructurewithoutgrounding? acasestudyincolor. arXivpreprintarXiv:2109.06129,2021. [AMKK21] DaichAzuma,TaikiMiyanishi,ShuheiKurita,andMotoakiKawanabe. Scanqa: 3d\n",
      "questionansweringforspatialsceneunderstanding. 2022IEEE/CVFConferenceon\n",
      "ComputerVisionandPatternRecognition(CVPR),pages19107–19117,2021. [Bad92] AlanBaddeley. Workingmemory. Science,255(5044):556–559,1992. [Bat10] JohnABateman. Languageandspace: atwo-levelsemanticapproachbasedonprinci-\n",
      "plesofontologicalengineering. InternationalJournalofSpeechTechnology,13:29–48,\n",
      "2010. [BCE+23] SébastienBubeck,VarunChandrasekaran,RonenEldan,J.Gehrke,EricHorvitz,Ece\n",
      "Kamar,PeterLee,Y.Lee,Yuan-FangLi,ScottM.Lundberg,HarshaNori,H.Palangi,\n",
      "Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early\n",
      "experimentswithgpt-4. arXiv.org,2023. [BHT+20] YonatanBisk,AriHoltzman,JesseThomason,JacobAndreas,YoshuaBengio,Joyce\n",
      "Chai,MirellaLapata,AngelikiLazaridou,JonathanMay,AleksandrNisnevich,etal. Experiencegroundslanguage. arXivpreprintarXiv:2004.10151,2020. [BK18] NicholasBakerandPhilipJKellman. Abstractshaperepresentationinhumanvisual\n",
      "perception. JournalofExperimentalPsychology: General,147(9):1295,2018. [BMR+20] Tom B.\n",
      "\n",
      "CHUNK : 27\n",
      "Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla\n",
      "Dhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,Sandhini\n",
      "Agarwal,ArielHerbert-Voss,GretchenKrueger,T.Henighan,RewonChild,A.Ramesh,\n",
      "Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric\n",
      "Sigler,MateuszLitwin,S.Gray,BenjaminChess,JackClark,ChristopherBerner,Sam\n",
      "McCandlish, Alec Radford, I.\n",
      "\n",
      "CHUNK : 28\n",
      "Sutskever, and Dario Amodei. Language models are\n",
      "few-shotlearners. NeuralInformationProcessingSystems,2020. [CBGG97] AnthonyGCohn,BrandonBennett,JohnGooday,andNicholasMGotts. Representing\n",
      "andreasoningwithqualitativespatialrelationsaboutregions. InSpatialandtemporal\n",
      "reasoning,pages97–134.Springer,1997. [Cho19] FrançoisChollet. Onthemeasureofintelligence,2019. [ES03] NiklasEénandNiklasSörensson. Anextensiblesat-solver. InInternationalconference\n",
      "ontheoryandapplicationsofsatisfiabilitytesting,pages502–518.Springer,2003. [FML+22] R. Freedman, Joseph B.\n",
      "\n",
      "CHUNK : 29\n",
      "Mueller, Jack Ladwig, Steven Johnston, David McDonald,\n",
      "H.Wauck,RutaWheelock,andHayleyBorck. Asymbolicrepresentationofhuman\n",
      "postureforinterpretablelearningandreasoning. arXiv.org,2022. 10\n",
      "\n",
      "\n",
      "CHUNK : 30\n",
      "[GDB17] Mona M Garvert, Raymond J Dolan, and Timothy EJ Behrens. A map of abstract\n",
      "relational knowledge in the human hippocampal–entorhinal cortex. elife, 6:e17086,\n",
      "2017.\n",
      "\n",
      "CHUNK : 31\n",
      "[GN07] EugeneGoldbergandYakovNovikov. Berkmin: Afastandrobustsat-solver. Discrete\n",
      "AppliedMathematics,155(12):1549–1561,2007. [Gol66] Solomon W Golomb. Tiling with polyominoes. Journal of Combinatorial Theory,\n",
      "1(2):280–296,1966. [GT23] WesGurneeandMaxTegmark. Languagemodelsrepresentspaceandtime,2023. [HGM+23] ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,and\n",
      "ZhitingHu. Reasoningwithlanguagemodelisplanningwithworldmodel,2023. [HK11] JoanaHoisandOliverKutz. Towardslinguistically-groundedspatiallogics. InDagstuhl\n",
      "SeminarProceedings.SchlossDagstuhl-Leibniz-ZentrumfÃ1/4rInformatik,2011. [HZC+23] YiningHong,HaoyuZhen,PeihaoChen,ShuhongZheng,YilunDu,ZhenfangChen,\n",
      "andChuangGan. 3d-llm: Injectingthe3dworldintolargelanguagemodels,2023. [JSM+23] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\n",
      "draSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,Guillaume\n",
      "Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock,\n",
      "TevenLeScao,ThibautLavril,ThomasWang,TimothéeLacroix,andWilliamElSayed. Mistral7b,2023. [KC22] YunaKwakandClaytonECurtis. Unveilingtheabstractformatofmnemonicrepresen-\n",
      "tations. Neuron,110(11):1822–1828,2022. [KFP19] NikhilKrishnaswamy,ScottFriedman,andJamesPustejovsky.Combiningdeeplearning\n",
      "andqualitativespatialreasoningtolearncomplexstructuresfromsparseexampleswith\n",
      "noise. InProceedingsoftheAAAIConferenceonArtificialIntelligence, volume33,\n",
      "pages2911–2918,2019. [KGR+23] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Largelanguagemodelsarezero-shotreasoners,2023. [KM15] ParisaKordjamshidiandMarie-FrancineMoens. Globalmachinelearningforspatial\n",
      "ontologypopulation. JournalofWebSemantics,30:3–21,2015. [Knu00] DonaldEKnuth. Dancinglinks. arXivpreprintcs/0011047,2000.\n",
      "\n",
      "CHUNK : 32\n",
      "[KPM20] ParisaKordjamshidi,J.Pustejovsky,andMarie-FrancineMoens. Representation,learn-\n",
      "ingandreasoningonspatiallanguagefordownstreamnlptasks.ConferenceonEmpirical\n",
      "MethodsinNaturalLanguageProcessing,2020. [KVOM11] ParisaKordjamshidi,MartijnVanOtterlo,andMarie-FrancineMoens. Spatialrolela-\n",
      "beling:Towardsextractionofspatialrelationsfromnaturallanguage. ACMTransactions\n",
      "onSpeechandLanguageProcessing(TSLP),8(3):1–36,2011. [LB18] Brenden M.\n",
      "\n",
      "CHUNK : 33\n",
      "Lake and Marco Baroni. Generalization without systematicity: On the\n",
      "compositionalskillsofsequence-to-sequencerecurrentnetworks,2018. [LeC22] YannLeCun. Apathtowardsautonomousmachineintelligenceversion0.9.2,2022-06-\n",
      "27. OpenReview,62(1),2022. [LKH+22] XiangLorraineLi,AdhigunaKuncoro,JordanHoffmann,CypriendeMassond’Autume,\n",
      "Phil Blunsom, and Aida Nematzadeh. A systematic investigation of commonsense\n",
      "knowledgeinlargelanguagemodels,2022. [LWG+22] RuiboLiu,JasonWei,ShixiangShaneGu,Te-YenWu,SoroushVosoughi,ClaireCui,\n",
      "DennyZhou,andAndrewM.Dai. Mind’seye: Groundedlanguagemodelreasoning\n",
      "throughsimulation,2022. 11\n",
      "\n",
      "\n",
      "CHUNK : 34\n",
      "[MFNK21] Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjmashidi. Spartqa:Atextualquestionansweringbenchmarkforspatialreasoning. NorthAmerican\n",
      "ChapteroftheAssociationforComputationalLinguistics,2021. [MGSS21] WilliamMerrill,YoavGoldberg,RoySchwartz,andNoahASmith. Provablelimitations\n",
      "ofacquiringmeaningfromungroundedform: Whatwillfuturelanguagemodelsunder-\n",
      "stand? TransactionsoftheAssociationforComputationalLinguistics,9:1047–1060,\n",
      "2021. [MHV+24] Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Os-\n",
      "azuwa Ness, Nebojsa Jojic, Hamid Palangi, and Jonathan Larson. Evaluating\n",
      "cognitive maps and planning in large language models with cogeval. Arxiv:\n",
      "http://arxiv.org/abs/2309.15129v1,2024.\n",
      "\n",
      "CHUNK : 35\n",
      "[MK09] SamuelTMoultonandStephenMKosslyn. Imaginingpredictions: mentalimagery\n",
      "asmentalemulation. PhilosophicalTransactionsoftheRoyalSocietyB:Biological\n",
      "Sciences,364(1521):1273–1280,2009. [MK22] RoshanakMirzaeeandParisaKordjamshidi. Transferlearningwithsyntheticcorpora\n",
      "forspatialrolelabelingandreasoning. ConferenceonEmpiricalMethodsinNatural\n",
      "LanguageProcessing,2022. [MXF+23] SuvirMirchandani,FeiXia,PeteFlorence,BrianIchter,DannyDriess,MontserratGon-\n",
      "zalezArenas,KanishkaRao,DorsaSadigh,andAndyZeng. Largelanguagemodelsas\n",
      "generalpatternmachines,2023. [NLW23] NeelNanda,AndrewLee,andMartinWattenberg. Emergentlinearrepresentationsin\n",
      "worldmodelsofself-supervisedsequencemodels. arXivpreprintarXiv:2309.00941,\n",
      "2023. [OA+23] OpenAI,:,JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,\n",
      "FlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,Shyamal\n",
      "Anadkat, RedAvila, IgorBabuschkin, SuchirBalaji, ValerieBalcom, PaulBaltescu,\n",
      "HaimingBao,MoBavarian,JeffBelgum,IrwanBello,JakeBerdine,GabrielBernadett-\n",
      "Shapiro,ChristopherBerner,LennyBogdonoff,OlegBoiko,MadelaineBoyd,Anna-\n",
      "LuisaBrakman,GregBrockman,TimBrooks,MilesBrundage,KevinButton,Trevor\n",
      "Cai,RosieCampbell,AndrewCann,BrittanyCarey,ChelseaCarlson,RoryCarmichael,\n",
      "BrookeChan,CheChang,FotisChantzis,DerekChen,SullyChen,RubyChen,Jason\n",
      "Chen,MarkChen,BenChess,ChesterCho,CaseyChu,HyungWonChung,DaveCum-\n",
      "mings,JeremiahCurrier,YunxingDai,CoryDecareaux,ThomasDegry,NoahDeutsch,\n",
      "Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien\n",
      "Ecoffet,AttyEleti,TynaEloundou,DavidFarhi,LiamFedus,NikoFelix,SimónPosada\n",
      "Fishman,JustonForte,IsabellaFulford,LeoGao,ElieGeorges,ChristianGibson,Vik\n",
      "Goel,TarunGogineni,GabrielGoh,RaphaGontijo-Lopes,JonathanGordon,Morgan\n",
      "Grafstein,ScottGray,RyanGreene,JoshuaGross,ShixiangShaneGu,YufeiGuo,Chris\n",
      "Hallacy,JesseHan,JeffHarris,YuchenHe,MikeHeaton,JohannesHeidecke,Chris\n",
      "Hesse,AlanHickey,WadeHickey,PeterHoeschele,BrandonHoughton,KennyHsu,\n",
      "ShengliHu,XinHu,JoostHuizinga,ShantanuJain,ShawnJain,JoanneJang,Angela\n",
      "Jiang,RogerJiang,HaozhunJin,DennyJin,ShinoJomoto,BillieJonn,HeewooJun,\n",
      "TomerKaftan,ŁukaszKaiser,AliKamali,IngmarKanitscheider,NitishShirishKeskar,\n",
      "TabarakKhan,LoganKilpatrick,JongWookKim,ChristinaKim,YongjikKim,Hendrik\n",
      "Kirchner,JamieKiros,MattKnight,DanielKokotajlo,ŁukaszKondraciuk,Andrew\n",
      "Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael\n",
      "Lampe,IkaiLan,TeddyLee,JanLeike,JadeLeung,DanielLevy,ChakMingLi,Rachel\n",
      "Lim,MollyLin,StephanieLin,MateuszLitwin,TheresaLopez,RyanLowe,Patricia\n",
      "Lue,AnnaMakanju,KimMalfacini,SamManning,TodorMarkov,YanivMarkovski,\n",
      "BiancaMartin,KatieMayer,AndrewMayne,BobMcGrew,ScottMayerMcKinney,\n",
      "ChristineMcLeavey,PaulMcMillan,JakeMcNeil,DavidMedina,AalokMehta,Ja-\n",
      "cobMenick,LukeMetz,AndreyMishchenko,PamelaMishkin,VinnieMonaco,Evan\n",
      "Morikawa,DanielMossing,TongMu,MiraMurati,OlegMurk,DavidMély,Ashvin\n",
      "Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\n",
      "wooNoh,LongOuyang,CullenO’Keefe,JakubPachocki,AlexPaino,JoePalermo,\n",
      "12\n",
      "\n",
      "\n",
      "CHUNK : 36\n",
      "AshleyPantuliano,GiambattistaParascandolo,JoelParish,EmyParparita,AlexPas-\n",
      "sos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\n",
      "MichaelPetrov,HenriquePondedeOliveiraPinto,Michael,Pokorny,MichellePokrass,\n",
      "VitchyrPong,TollyPowell,AletheaPower,BorisPower,ElizabethProehl,RaulPuri,\n",
      "Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra\n",
      "Rimbach,CarlRoss,BobRotsted,HenriRoussez,NickRyder,MarioSaltarelli,Ted\n",
      "Sanders,ShibaniSanturkar,GirishSastry,HeatherSchmidt,DavidSchnurr,JohnSchul-\n",
      "man, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,\n",
      "Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina\n",
      "Slama,IanSohl,BenjaminSokolowsky,YangSong,NatalieStaudacher,FelipePetroski\n",
      "Such,NatalieSummers,IlyaSutskever,JieTang,NikolasTezak,MadeleineThomp-\n",
      "son, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley,\n",
      "JerryTworek,JuanFelipeCerónUribe,AndreaVallone,ArunVijayvergiya,Chelsea\n",
      "Voss,CarrollWainwright,JustinJayWang,AlvinWang,BenWang,JonathanWard,\n",
      "JasonWei,CJWeinmann,AkilaWelihinda,PeterWelinder,JiayiWeng,LilianWeng,\n",
      "MattWiethoff,DaveWillner,ClemensWinter,SamuelWolrich,HannahWong,Lauren\n",
      "Workman,SherwinWu,JeffWu,MichaelWu,KaiXiao,TaoXu,SarahYoo,Kevin\n",
      "Yu,QimingYuan,WojciechZaremba,RowanZellers,ChongZhang,MarvinZhang,\n",
      "ShengjiaZhao,TianhaoZheng,JuntangZhuang,WilliamZhuk,andBarretZoph. Gpt-4\n",
      "technicalreport,2023.\n",
      "\n",
      "CHUNK : 37\n",
      "[Ope23] OpenAI. Gpt-4v(ision)systemcard. 2023. [PP22] RomaPatelandElliePavlick. Mappinglanguagemodelstogroundedconceptualspaces. InInternationalConferenceonLearningRepresentations,2022. [RAB+20] LauraRuis,JacobAndreas,MarcoBaroni,DianeBouchacourt,andBrendenMLake. A\n",
      "benchmarkforsystematicgeneralizationingroundedlanguageunderstanding. Advances\n",
      "inneuralinformationprocessingsystems,33:19861–19872,2020. [Reg19] JohnRegehr. Explainingcodeusingasciiart,2019. [RFD+21] Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei Cheng, Dell Zhang, and\n",
      "AndreFreitas. Groundingnaturallanguageinstructions: Canlargelanguagemodels\n",
      "capturespatialinformation?,2021. [RKH+21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\n",
      "hiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,Gretchen\n",
      "Krueger,andIlyaSutskever. Learningtransferablevisualmodelsfromnaturallanguage\n",
      "supervision,2021. [RMK18] Taher Rahgooy, Umar Manzoor, and Parisa Kordjamshidi. Visually guided spatial\n",
      "relation extraction from text. In Proceedings of the 2018 Conference of the North\n",
      "AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage\n",
      "Technologies,Volume2(ShortPapers),pages788–794,2018. [SB14] AnthonyJSmithandJoannaJBryson.Alogicalapproachtobuildingdungeons:Answer\n",
      "setprogrammingforhierarchicalproceduralcontentgenerationinroguelikegames. In\n",
      "Proceedingsofthe50thAnniversaryConventionoftheAISB,2014.\n",
      "\n",
      "CHUNK : 38\n",
      "[SF72] RogerNShepardandChristineFeng. Achronometricstudyofmentalpaperfolding. Cognitivepsychology,3(2):228–243,1972. [She78] RogerNShepard. Thementalimage. Americanpsychologist,33(2):125,1978. [SLYA17] AlaneSuhr, MikeLewis, JamesYeh, andYoavArtzi. Acorpusofnaturallanguage\n",
      "forvisualreasoning. InReginaBarzilayandMin-YenKan,editors,Proceedingsofthe\n",
      "55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2: Short\n",
      "Papers),pages217–223,Vancouver,Canada,July2017.AssociationforComputational\n",
      "Linguistics. [SM71] RogerN.ShepardandJacquelineMetzler. Mentalrotationofthree-dimensionalobjects. Science,171:701–703,1971. 13\n",
      "\n",
      "\n",
      "CHUNK : 39\n",
      "[SMM21] HariniSampath,AliceMerrick,andAndrewMacvean. Accessibilityofcommandline\n",
      "interfaces. InProceedingsofthe2021CHIConferenceonHumanFactorsinComputing\n",
      "Systems,pages1–10,2021. [SZL22] Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. Stepgame: A new benchmark for\n",
      "robustmulti-hopspatialreasoningintexts. AAAIConferenceonArtificialIntelligence,\n",
      "2022. [TLI+23] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,\n",
      "TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,Aurelien\n",
      "Rodriguez,ArmandJoulin,EdouardGrave,andGuillaumeLample. Llama: Openand\n",
      "efficientfoundationlanguagemodels. arXiv.org,2023.\n",
      "\n",
      "CHUNK : 40\n",
      "[Tol48] EdwardCTolman. Cognitivemapsinratsandmen. Psychologicalreview,55(4):189,\n",
      "1948. [WBC+15] JasonWeston,AntoineBordes,SumitChopra,AlexanderM.Rush,BartvanMerriënboer,\n",
      "ArmandJoulin,andTomasMikolov. Towardsai-completequestionanswering: Asetof\n",
      "prerequisitetoytasks,2015. [YBL+23] Yutaro Yamada, Yihan Bao, Andrew K. Lampinen, Jungo Kasai, and Ilker Yildirim. Evaluatingspatialunderstandingoflargelanguagemodels,2023. [YIL23] ZhunYang,AdamIshay,andJoohyungLee. Couplinglargelanguagemodelswithlogic\n",
      "programmingforrobustandgeneralreasoningfromtext,2023. 14\n",
      "\n",
      "\n",
      "CHUNK : 41\n",
      "A SyntheticData\n",
      "A.1 VisualNavigation\n",
      "Asdepictedin1, givenaspecifick, theprocessofgeneratinga2Dnavigationmapiscomposed\n",
      "of3steps,whichareinstructiongeneration,instructionsimulation,maprendering. Ininstruction\n",
      "generationstep,weenumerateallpossibleinstructionsetsnavigatingfromthestartingpointtothe\n",
      "destination(e.gmoveup,thenmoveright). Duringthisstep,onlythedirectionofeachinstructionis\n",
      "determined,whilethemovingdistanceisundetermineduntilnextstep. Ininstructionsimulationstep,\n",
      "simulationisappliedinthe2Dcoordinatesystemwithorigin(0,0)asthestartingpoint. Toguarantee\n",
      "anuniqueanswerineachnavigationmap,themovingdistanceofeachinstructionisdynamically\n",
      "calculatedtoavoidoverlapping. Eachtimewhenanoverlappingisdetected,themovingdistance\n",
      "of previous instruction will be increased by 1 unit recursively until overlapping is resolved. As\n",
      "thedistanceisdetermined,thosecorrespondingpointsareaddedtothenavigatingpath. Afterall\n",
      "instructionsarecompleted,thefinalpointismarkedasthedestination. Inthemaprenderingstep,the\n",
      "boundingboxofthosepointsisadoptedandnormalizedtoa2Dsquaregrid. Thestartingpointand\n",
      "destinationaremarkedwithdedicatedsquares,andcellsalongthepatharemarkedbyemptysquares,\n",
      "whileotheruntouchedcellsaremarkedbyobstaclesquares. Algorithm1:NavigationMapGeneration\n",
      "Input :k\n",
      "Output:C ={s ,s ,...,s },C ={t ,t ,...,t },\n",
      "solution 1 2 n textual_map 1 2 n\n",
      "C ={v ,v ,...,v };wheren=2k+1\n",
      "visual_map 1 2 n\n",
      "1 dirs←[up,left,down,right]\n",
      "2 instruction_sets←gen_instruction(dirs,k)\n",
      "3 fordir_instructsininstruction_setsdo\n",
      "4 cur_pos,path_points,moves←initialize() // origin as the starting point\n",
      "5 fordirection in dir_instructsdo\n",
      "// instruction simulation\n",
      "6 ifnotvalidate_plan(cur_pos,direction,path_points)then\n",
      "7 increase_previous_move(cur_pos,moves,path_points)\n",
      "8 end\n",
      "9 cur_pos,path_points←step_forward(cur_pos,direction,path_points)\n",
      "10 moves←moves∪direction\n",
      "11 end\n",
      "12 s i ←moves\n",
      "13 t i,v i ←render_map(extract_bounding_box(path_points))\n",
      "14 C solution ←C solution∪s i\n",
      "15 C textual_map ←C textual_map∪t i\n",
      "16 C visual_map ←C visual_map∪v i\n",
      "17 end\n",
      "Sincethedirectionofeachnavigationinstructionisalternating,thereare4∗2k−1 = 2k+1 kinds\n",
      "ofspatialconfigurationsforak-hopnavigationmap. Duringtheimplementation,wesimplifythe\n",
      "recursiveimplementationwithanearlyquitwhenpathoverlappingcouldnotberesolvedwithinone\n",
      "iteration,themainconsiderationofwhichisthesizeofthemap.\n",
      "\n",
      "CHUNK : 42\n",
      "Sothenumberofgeneratedmapis\n",
      "slightlylowerthan2k+1asthenavigatingstepkincreases. A.2 VisualTiling\n",
      "Thedatagenerationprocesscomprises3stages,includingconfigurationgeneration,questiongen-\n",
      "erationandpolyominorendering. Intheconfigurationgenerationstage, togeneratevalidspatial\n",
      "configurationsofarectangleandthecorrespondingpolyominoset,weconvertatilingproblemto\n",
      "existingformalizedproblems. Oneoftheproblemsisanexactcoverproblemleveragingdancing\n",
      "linkalgorithm[Knu00],whichcouldbedescribedas: givenamatrixof0sand1s,findasetofrows\n",
      "containingexactlyone1ineachcolumn. Theconversionistoconstructamatrixof0sand1s,each\n",
      "rowofwhichrepresentsapossiblearrangementofplacingaspecificpolyominoinarectangle. As\n",
      "illustratedinEquation9,givenkpolyominopieces,andarectangleofnunitstobefilled,thefirstk\n",
      "15\n",
      "\n",
      "\n",
      "CHUNK : 43\n",
      "columnscomposeanone-hotvectorindicatingthecorrespondingpolyomino,andthelastncolumns\n",
      "aremarkedwith0or1dependingonwhetherthecorrespondingunitisfilledbythatpolyomino. Then\n",
      "findingasetofpolyominoarrangementsinarectangleequalstofindasetofrowscontainingexactly\n",
      "one1ineachcolumn. Anotheradaptableproblemisthebooleansatisfiabilityproblem(commonly\n",
      "knownasSAT),forwhichefficientsolversexist[ES03,GN07]. Atilingproblemcanbeconvertedto\n",
      "SATbyintroducingabooleanvariableforeachpossiblearrangementofeachpiece,andthenadding\n",
      "clausescomprisingofthosebooleanvariablesthatensureatleastonearrangementofeachpieceis\n",
      "achieved,whileavoidingconflictsbetweenarrangementsofonepieceortwodifferentpieces.\n",
      "\n",
      "CHUNK : 44\n",
      "Giventhesizeofarectangleandpolyominoestobefit,multiplecorrespondingsolutionsaregenerated\n",
      "byapplyingthosealgorithms. Theninthequestiongenerationstage, werandomlymaskseveral\n",
      "polyomino pieces in the rectangle, and generate a question answer(QA) pair for each masked\n",
      "polyomino. Finallytherectangleandeachpolyominopiecearerenderedwithemojisquares. C C C C\n",
      "1 k k+1 n+k\n",
      " 1 0 1 0 0 1 \n",
      "P1\n",
      "1 0 0 1 1 0\n",
      " \n",
      " \n",
      "  (9)\n",
      " \n",
      " \n",
      " 0 1 1 1 0 0 \n",
      "Pk\n",
      "0 1 0 0 1 1\n",
      "A.3 VisualDataRendering\n",
      "Aftergatheringthetextualdatasetof2Dsquaregrid,wegeneratethecorrespondingvisualdatasetby\n",
      "drawingtextontoanimage. Specificallyweadoptcoloremojisforafaircomparisonasthey’remore\n",
      "visualfriendlytoamultimodalmodel. A.4 DatasetDetails\n",
      "Datadistributionamongvariousdifficultylevelsforvisualnavigationtasksandvisualtilingtasksare\n",
      "providedinTable4and5.Itprovidesflexibledifficultycontrolacrossdifferenttasks.Forvisualtiling\n",
      "task,thedifficultyiscontrolledbythenumberofmaskedpolyominopieces. Asthenumberincreases,\n",
      "the more spatial arrangements LLMs need to consider. Regarding the visual navigation task, as\n",
      "illustratedinfigure2,weusethenumberofroadsktocontroldifficulty,whichiscorrespondingto\n",
      "thesizeofthemap. KStep Maskcount\n",
      "Task Total Total\n",
      "2 3 4 5 6 7 2 3\n",
      "RoutePlanning 8 16 32 64 128 248 496 Configuration 248 124 376\n",
      "NextStepPrediction 8 32 96 256 640 1488 2520 QAInstance 489 307 796\n",
      "Table4: Datadistributionofvisualnavigationdatasetwith Table 5: Details of visual tiling\n",
      "thetotalnavigatingstepofkindicatingdifficultylevel. The dataset. Some QA instances are\n",
      "reasonwhythenumberofgeneratedmapisslightlylower discardedwhenmultiplesolutions\n",
      "than2k+1fork >5isexplainedinAppendixA.1. existandallanswersarecorrect. B Examples\n",
      "For visual navigation and visual tiling tasks, the structured input template is comprised of task\n",
      "instruction,inputparametersandpromptofspecificsetting. B.1 VisualTasks\n",
      "TaskinstructionsandresponsesofeachvisualtaskundersettingGPT-4VoTareprovidedasfollowing:\n",
      "• RoutePlanningTaskinstructioninFigure6,responseinFigure12. 16\n",
      "\n",
      "\n",
      "CHUNK : 45\n",
      "• NextStepPredictionTaskinstructioninFigure6,responseinFigure13. • VisualTilingTaskinstructioninFigure7,responseinFigure14. Route Planning Next Step Prediction\n",
      "Navigation Task: for a provided map, is the home as starting point, is the Navigation Task: for a provided map, is the home as starting point, is the\n",
      "office as the destination. means the road, means the obstacle.\n",
      "\n",
      "CHUNK : 46\n",
      "There exists office as the destination. means the road, means the obstacle. There exists\n",
      "one and only one viable route for each map. Each step you choose a direction and one and only one viable route for each map. Each step you choose a direction and\n",
      "move to the end of the continuous road or the destination. move to the end of the continuous road or the destination. map: map:\n",
      "``` ```\n",
      "```\n",
      "``` Starting from , to navigate to , you made following movements:\n",
      "Starting from , provide the steps to navigate to . 1. Move right to the end of continuous road. 2. Move down to the end of continuous road. Visualize the state after each reasoning step. 3. Move left to the end of continuous road. What's the direction of next movement? A. Up\n",
      "B. Left\n",
      "C. Down\n",
      "D. Right\n",
      "Visualize the state after each reasoning step. Figure6: TaskInstructionofvisualnavigation. Task: given a set of polyominoes and corresponding Variation 2 fitting into its bounding box: Variation 3 fitting into its bounding box:\n",
      "variations of each polyomino, fit them into the empty ``` ```\n",
      "squares ( ) in the target rectangle without overlapping any\n",
      "existing polyominoes or going outside the rectangle. The\n",
      "variations allow only translation, not rotation or reflection. It's guaranteed that there always exists a solution. ```\n",
      "------------------------- -------------------------\n",
      "Target rectangle with 12 empty squares: Variations for Tetromino L:\n",
      "``` Variation 3 fitting into its bounding box: ```\n",
      "``` -------------------------\n",
      "To fit all the provided polyominoes into the empty squares\n",
      "( ), what's the correct variation of Tetromino T? A. 2\n",
      "``` B. 3\n",
      "Variation 7 fitting into its bounding box: C. Neither\n",
      "```\n",
      "Visualize the state after each reasoning step. ```\n",
      "Provided polyominoes:\n",
      "1. Tetromino I ( )\n",
      "2.\n",
      "\n",
      "CHUNK : 47\n",
      "Tetromino L ( )\n",
      "```\n",
      "3. Tetromino T ( )\n",
      "-------------------------\n",
      "-------------------------\n",
      "Variations for Tetromino T:\n",
      "Variations for Tetromino I:\n",
      "Variation 2 fitting into its bounding box:\n",
      "Variation 1 fitting into its bounding box:\n",
      "```\n",
      "```\n",
      "```\n",
      "```\n",
      "Figure7: TaskInstructionofvisualtiling. B.2 NaturalLanguageNavigation\n",
      "PromptExample Youhavebeengivena3by3squaregrid. Startingfromavertex,youwillmove\n",
      "alongtheedgesofthegrid. Initially,youarepositionedatthebottom-leftcornerofthegrid,where\n",
      "youwillfindatorch,thenyougoright,whereyouwillfindaninfantbed,thenyougoright,where\n",
      "youwillfindanAmericandipper. Thenyougoup,whereyouwillfindajay,thenyougoleft,where\n",
      "youwillfindaterrapin,thenyougoleft,whereyouwillfindamicrowaveoven. Thenyougoup,\n",
      "17\n",
      "\n",
      "\n",
      "CHUNK : 48\n",
      "Visualize the state after each reasoning step Let’s think step by step. First, let's look at the target rectangle and identify the empty squares ( ) First, let's look at the empty spaces in the target rectangle. We have a 4x1 space at the\n",
      "where we need to fit the polyominoes: bottom and three single squares above it. The Tetromino I ( ) can only fit in the 4x1\n",
      "space at the bottom, so we'll use Variation 2 of the Tetromino I to fill that space. Now, let's consider the Tetromino T ( ) variations. We have three single empty squares\n",
      "that are not in a straight line, so Variation 2 of the Tetromino T, which is a straight line\n",
      "We have two polyominoes to fit into this rectangle: Tetromino I ( ) and with a protrusion, will not fit. However, Variation 3 of the Tetromino T has a\n",
      "Tetromino T ( ). The Tetromino I can only fit in the bottom row since it's configuration that can fit into the three remaining empty squares with one square\n",
      "the only place with four consecutive empty squares, either horizontally or protruding to the side, which matches the layout of the empty squares. vertically. Let's place the Tetromino I ( ) in the bottom row:\n",
      "Therefore, the correct variation of TetrominoT to fit into the remaining empty squares is:\n",
      "B. 3\n",
      "The final configuration would look like this:\n",
      "….. We can see that Variation 2 of Tetromino T ( ) fits perfectly into the\n",
      "remaining empty squares, as it matches the T shape formed by the empty\n",
      "squares. Let's place Variation 2 of Tetromino T ( ) into the target rectangle:\n",
      "The correct variation of TetrominoT to fit into the empty squares is Variation 2. Answer: A.\n",
      "\n",
      "CHUNK : 49\n",
      "2\n",
      "(a)CommonbehaviorsofVoTandCoTpromptinginvisualtilingtask,withtheoveralltrackrateof87.1%and\n",
      "57.4%respectively. Visualize the state after each reasoning step Visualize the state after each reasoning step\n",
      "To navigate from to , we need to find the path that avoids obstacles To navigate from to on the provided map, follow these instructions:\n",
      "( ) and follows the roads ( ). Here's the step-by-step navigation: 1. Move down to the end of the continuous road.\n",
      "\n",
      "CHUNK : 50\n",
      "1. Move down from to the end of the continuous road. 2. Move right to the end of the continuous road. 3. Move up to reach the destination . Here's the visualization after each step:\n",
      "Starting point:\n",
      "After the move:\n",
      "After Step 1 (Move down):\n",
      "2. Move left from the current position to the office . After Step 2 (Move right):\n",
      "After the move:\n",
      "After Step 3 (Move up):\n",
      "The has now reached the , completing the navigation task. (b)Routeplanningtaskissensitivetoprompts.Afterdeletingtheword\"reasoning\"fromtheVoTprompt,final\n",
      "answerandstateisderivedwithoutconditioningonstatevisualizationinmanycases.Theleftexampleprovides\n",
      "acorrectsolutionwithimperfectintermediatevisualization,whiletherightexampleprovidesawrongsolution. Figure8: Examplecasesofvisualstatetrackingbehaviorsindifferenttasks. Weprovideside-by-side\n",
      "comparisonstoshowhowvisualstatetrackingisinfluencedbyprompt,whereboldtextsonthetop\n",
      "are prompts to compare. Case on the left exhibits visual state tracking in an interleaved manner,\n",
      "wherenextstateisconditionedonvisualizationofpreviousstates. Caseontherightdisentangles\n",
      "verbalreasoningandstatevisualization,whichleadstoadropintrackingrateandtaskperformance. 18\n",
      "\n",
      "\n",
      "CHUNK : 51\n",
      "whereyouwillfindabaseballplayer,thenyougoright,whereyouwillfindaharvestman,thenyou\n",
      "goright,whereyouwillfindaneckbrace. Nowyouhavealltheinformationonthemap. Youstartat\n",
      "thepositionwherethetorchislocated,thenyougorightbyonestep,thenyougorightbyonestep,\n",
      "thenyougoupbyonestep,thenyougoupbyonestep,thenyougoleftbyonestep,thenyougo\n",
      "downbyonestep,andthenyougodownbyonestep. Whatwillyoufind?\n",
      "\n",
      "CHUNK : 52\n",
      "ResponseExample SeeFigure15. C VisualStateTracking\n",
      "Asforwherethisemergentabilitystemsfrom,itmightderivefromtabulardata,citygridnaviga-\n",
      "tion,mazeexplorationrelatedcodingproblems[YBL+23]. Thesetasksinvolvesunderstandingand\n",
      "manipulating objects in a 2D square grid. Besides, we conjecture the exposure of ascii-art com-\n",
      "ments[Reg19]duringLLMs’codepre-trainingpossiblyenhancesthisgeneralizedability. Asafact\n",
      "tosupportthisconjecture,thevisualtilingtaskisdifferentfromnavigationtasksbecauseitrequires\n",
      "shapeunderstandingandspatialmanipulationability. Whiletabulardataandsquaregridnavigation\n",
      "data boost row-wise or column-wise attention, ascii-art supplements intricate spatial attention to\n",
      "understand and manipulate 2D shapes. Additionally, ascii-art in code comments is presented in\n",
      "various formats, one of which is interleaved ascii diagrams, natural language and programming\n",
      "language. It require LLMs to generate the interleaved mental images and text sequence, thereby\n",
      "enhancing spatial visualization ability and spatiotemporal causality. Interestingly in the natural\n",
      "languagenavigationtask,whenGPT-4ispromptedwith\"useascii-arttovisualize\",thecomplete\n",
      "trackingrateincreasesto98.5%(+78.5%),boostingtaskperformanceto62.5%(+3.5%). C.1 Ascii-artinCodeComments\n",
      "Ascii-artiscommonlyusedincodecommentstorepresentdatastructure,diagram,geometryand\n",
      "soon,whichcouldbenefitLLMs’spatialunderstandingandvisualizationcapability. Besides,it’s\n",
      "alsousedtoillustratehowanalgorithmworksorsimulateanoperation,wherereasoningtracesand\n",
      "correspondingvisualizationarepresentedinaninterleavedmanner.\n",
      "\n",
      "CHUNK : 53\n",
      "Belowareseveralexamplesin\n",
      "open-sourceprojects. • Spatial Causality:Double-ended queue in Rust, Scrolling web pages and tree rotation\n",
      "presenttripletsofpreviousvisualstate,instruction,andupdatedstateofinstructionfollow-\n",
      "ing. • TemporalCausality:Undosystemsfromemacsprovidesvarioustemporalstatesoftheundo\n",
      "systemwhenundooperationhappensindifferenttimelinesandcorrespondingvisualizations\n",
      "inaninterleavedmanner. Eachvisualizationreflectsthetemporalcasualityofthesystem\n",
      "state. Thiskindofinterleavedsequencetracksthesystemstateovertime,thusreflectingspatiotemporal\n",
      "casuality. D PerformanceTrendsAcrossLevels\n",
      "In this analysis, we examine performance trends across varying difficulty levels in the next-step\n",
      "prediction task for models utilizing either CoT or VoT methods. These trends are crucial for\n",
      "understandingtheinherentunpredictabilityassociatedwithrandomguessing. Askincreasesfrom2\n",
      "to7inak-stepnavigationmap,distinctperformancepatternsemergeamongdifferentmodels,as\n",
      "depictedinFigure9. LargerlanguagemodelssuchasGPT-4andLLAMA3-70Bdemonstrateamore\n",
      "predictabledecreaseinaccuracywithincreasingk. Thistrendindicatesarobustabilitytohandle\n",
      "progressivelychallengingtasks,despitetheoveralldecreaseinperformance. Detailedstatisticsare\n",
      "providedintable6. Incontrast, lesspowerfulmodelslikeGPT-3.5andLLAMA3-8Bexhibitan\n",
      "irregularperformancetrajectory. Thesemodelsshowvariableaccuracy,withsignificantfluctuations\n",
      "athigherdifficultylevels,suggestingarelianceonrandomguessing,particularlyunderconditionsof\n",
      "increasedtaskdifficulty. Thisbehaviorhighlightstheirlimitationsinsustainingreliablereasoning\n",
      "processesthroughmorecomplexscenarios. Furthermore,theVoTmethodseemstoofferamodest\n",
      "improvementinperformanceforthelesspowerfulmodels,particularlyinscenariosoflowerdifficulty. 19\n",
      "\n",
      "\n",
      "CHUNK : 54\n",
      "ThisobservationsuggeststhatVoTmightbeadvantageousforenhancingreliablereasoninginsimpler\n",
      "spatialreasoningtasks,potentiallycompensatingfortheinherentweaknessesofsmallerlanguage\n",
      "models. 100\n",
      "80\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "2 3 4 5 6 7\n",
      "K-stepMap\n",
      ")%(ycaruccA\n",
      "CoTModelsPerformanceTrend\n",
      "100\n",
      "80\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "2 3 4 5 6 7\n",
      "K-stepMap\n",
      ")%(ycaruccA\n",
      "VoTModelsPerformanceTrend\n",
      "GPT-4 GPT-3.5 LLAMA3-70B LLAMA3-8B\n",
      "Figure9:PerformanceTrendsofCoTandVoTModelsAcrossdifficultylevelsinnext-step-prediction\n",
      "task. Model K-stepMap MapCount CoTAccuracy(%) VoTAccuracy(%)\n",
      "2 8 75.00 75.00\n",
      "3 32 68.75 62.50\n",
      "GPT-4 4 96 60.42 68.75\n",
      "5 256 50.78 64.06\n",
      "6 640 52.34 55.16\n",
      "7 1488 45.30 52.69\n",
      "2 8 62.50 62.50\n",
      "3 32 68.75 65.63\n",
      "LLama3-70B 4 96 60.42 62.50\n",
      "5 256 56.25 57.42\n",
      "6 640 48.59 54.84\n",
      "7 1488 46.71 52.35\n",
      "Table6: CoTandVoTperformanceofadvancedmodelsinnext-step-predictiontaskacrossvarious\n",
      "difficulty levels. While performance drops as difficulty level increases, VoT method generally\n",
      "maintains a higher accuracy compared to CoT, highlighting its robustness in more challenging\n",
      "scenarios. E Casestudy\n",
      "Weconsidervisualstatetrackingsimilartospatiotemporalsimulation. Duringthesimulationinthose\n",
      "tasks,wediscoveredseveralinterestingbehaviorsofLLM. 1. Diverse visualization formats for state tracking: Nearly 30 different symbols found in the\n",
      "navigationtaskstotrackthenavigationprogress,includingmarkingthepath,markingthecurrent\n",
      "location. Amongthosediverserepresentations,LLMsucceededinsomechallengingcaseswhereit\n",
      "useddirectionalarrowemojistoindicateboththelocationandmovingdirectionateachstep. More\n",
      "examplescouldbefoundinAppendixE.1. 2. Inconsistencybetweenlanguageandvisualization: Thisiscommonlyobservedacrossalltasks. Duetothelimitedvisualizationcapability,sometimesLLMgeneratesaccuratelanguageinstruction\n",
      "butinaccuratevisualization. Andinothercases,LLMgenerateswronganswerseventhevisualization\n",
      "isgeneratedcorrectly,whichreflectsitslimitationofspatialunderstandingasdiscussedinprevious\n",
      "section. MoreexamplescouldbefoundinAppendixE.2.\n",
      "\n",
      "CHUNK : 55\n",
      "20\n",
      "\n",
      "\n",
      "CHUNK : 56\n",
      "3. Self-refinemechanism:Wefoundseveralcasesinvisualtilingtaskswherespatialhallucination\n",
      "happensduetotheinconsistencyorinaccuratevisualization.Subsequently,LLMrefineditsreasoning,\n",
      "resultinginanaccuratevisualizationandthecorrectionofthefinalanswer. Moreexamplescouldbe\n",
      "foundinAppendixE.3. E.1 mentalimagesforStateTracking\n",
      "Inthevisualnavigationtask,LLMadoptedvarioussymbolsandrepresentationstotrackthestateof\n",
      "navigationprogress. AsshowninFigure10,there’reseveraltrackingstyles.\n",
      "\n",
      "CHUNK : 57\n",
      "• Markthepath: adoptinganidenticalsymboltomarkcurrentlocationorpartofthepath. • Markpathanddirection: usingdirectionalarrowstomarkcurrentlocationandindicatethe\n",
      "movingdirectionsimultaneously,whichismorechallengingthansimplymarkingthepath. • Mark path with temporal steps: using numbers to demonstrate both temporal steps and\n",
      "currentlocation. • Remove road: turning roads into obstacles to avoid turning back, instead of adopting\n",
      "additionalsymbolstomarkthepath. E.2 InconsistencybetweenLanguageandVisualization\n",
      "Inthevisualtilingtask,twoinconsistentstepsarehighlightedinFigure16. Oneistheinconsistent\n",
      "visualizationwiththelanguageinstructionof\"placeVariation6ofTetrominoL\". Anotheristhe\n",
      "wrongdecisiontochose\"Variation2ofTetrominoI\"giventhevisualizationofthevalidstate. E.3 Self-refineMechanism\n",
      "WefoundvisualizationcouldenhanceLLM’sreasoningbyself-groundingandrefiningsubsequent\n",
      "reasoningstepsinsomecases. AsshowninFigure17,despitesuccessfullyidentifyingvariation1of\n",
      "tetrominoLasincorrectoption,GPT-4excludedthecorrectoptionofvariation6evenit’splaced\n",
      "accuratelyduetospatialhallucination(overlappingwithyellowpieces),whichledtoaimpossible\n",
      "solution. Thenitdetectedthemistakeandre-evaluatetheplacementofvariation6. Finallyitplaced\n",
      "thecorrectpieceintothetopleftcornerandvalidatedtheanswerbyfillingtheremainingspace. 21\n",
      "\n",
      "\n",
      "CHUNK : 58\n",
      "Use round pin Use checklist\n",
      "Original map 1. Move down 2. Move right Original map Move down\n",
      "Remove road to avoid turning back\n",
      "3. Move down 4. Move right 5. Move down Original map Move right\n",
      "Use arrows to reflect direction Use numbers for temporal steps\n",
      "Original map 1. Move right 2. Move down Original map 1. Move left 2. Move down\n",
      "3. Move right 4. Move down 5. Move left 3. Move right 4.\n",
      "\n",
      "CHUNK : 59\n",
      "Move down 5. Move right\n",
      "Figure10: Mentalimagesforstatetrackinginvisualnavigationtasks. Wefounddiversevisualization\n",
      "formatsinLLMstotrackthenavigationprogress,includingmarkingthepath,markingthecurrent\n",
      "location, marking directional steps, and marking temporal steps. More details are available in\n",
      "AppendixE.1\n",
      "22\n",
      "\n",
      "\n",
      "CHUNK : 60\n",
      "The ASCII art you've provided represents a three-dimensional cube with labeled vertices from 0 to 7. To unfold\n",
      "the front face (which appears to be the face connecting vertices 1, 2, 6, and 5), we would lay it flat. Assuming\n",
      "that the front face is the one with vertices 1, 2, 6, and 5, and we unfold it downwards, the net might look\n",
      "something like this:\n",
      "```\n",
      "0 3\n",
      "*-------*\n",
      "/| /|\n",
      "*-------* |\n",
      "4| | | |7\n",
      "Ascii cube: | *----- |-*\n",
      "``` |/ |/\n",
      "0 3 *-------*\n",
      "*-------* 1 2\n",
      "1 / | 2 /| ```\n",
      "*-------* | Now, let's unfold the front face (1, 2, 6, 5) downwards:\n",
      "| |4 | |7 ```\n",
      "| *-----|-* 0 3\n",
      "|/ | / *-------*\n",
      "*-------* / /|\n",
      "5 6 *-------* |\n",
      "``` 4| | |7\n",
      "Unfold the front face | | *\n",
      "| |/\n",
      "*-------*\n",
      "| |\n",
      "| |\n",
      "*-------*\n",
      "1 2\n",
      "```\n",
      "Here, the face with vertices 1, 2, 6, and 5 has been unfolded downwards. The vertical lines connecting the top\n",
      "face to the front face have now become horizontal, indicating that the front face is lying flat. The cube is now\n",
      "represented as a two-dimensional net, with the front face unfolded. Keep in mind that this is a simplified\n",
      "representation and that in an actual net, the faces would be connected at the edges without the gaps shown in the\n",
      "ASCII art. Figure11: Representationfor3DsemanticsandqualitativeresultsofGPT-4manipulatingthemental\n",
      "imagecorrectly. Starting from , let's navigate step by step to the .\n",
      "\n",
      "CHUNK : 61\n",
      "Step 3: Move left to the end of the continuous road. Step 6: Move down to reach the . Initial map: ``` ```\n",
      "```\n",
      "``` ```\n",
      "``` Step 4: Move down to the end of the continuous road. Step 1: Move right from to the end of the continuous road. ``` The steps to navigate from to are:\n",
      "```\n",
      "1. Right\n",
      "2. Down\n",
      "3. Left\n",
      "4. Down\n",
      "5. Left\n",
      "```\n",
      "Step 5: Move left to the end of the continuous road. 6. Down\n",
      "``` ``` We have now reached the destination, . Step 2: Move down to the end of the continuous road. ```\n",
      "```\n",
      "```\n",
      "Figure12: ResponsesofRoutePlanning\n",
      "23\n",
      "\n",
      "\n",
      "CHUNK : 62\n",
      "Let's visualize the state after each movement: 3. Move left to the end of the continuous road:\n",
      "Initial map: ```\n",
      "```\n",
      "``` ```\n",
      "1. Move right to the end of the continuous road: Now, the next movement options are:\n",
      "```\n",
      "A.Up - This would lead back to a previous position, which is not a viable route to the destination. B.\n",
      "\n",
      "CHUNK : 63\n",
      "Left - This is not possible as there is an obstacle ( ) immediately to the left. C. Down - This would lead to the destination ( ). D. Right - This is not possible as there is an obstacle ( ) immediately to the right. The correct direction for the next movement is:\n",
      "C. Down\n",
      "``` Moving down will take us to the destination, which is the office ( ). Here's the final visualization:\n",
      "2. Move down to the end of the continuous road: ```\n",
      "```\n",
      "```\n",
      "Figure13: ResponseofNextStepPrediction\n",
      "24\n",
      "\n",
      "\n",
      "CHUNK : 64\n",
      "Let's start by examining the empty spaces in the target rectangle and the provided\n",
      "polyominoes and their variations. We need to fit the Tetromino I, Tetromino L, and\n",
      "Tetromino T into the empty spaces without overlapping any existing polyominoes or going\n",
      "outside the rectangle. The target rectangle with 12 empty squares looks like this:\n",
      "```\n",
      "```\n",
      "We have two variations for Tetromino I ( ), which can either be placed vertically or\n",
      "horizontally.\n",
      "\n",
      "CHUNK : 65\n",
      "Given the configuration of the empty spaces, the only way to place the\n",
      "Tetromino I is horizontally at the top row, as the vertical placement would not fit anywhere. After placing Tetromino I ( ), the target rectangle looks like this:\n",
      "```\n",
      "```\n",
      "Next, we have two variations for Tetromino L ( ). We can see that Variation 3 of Tetromino\n",
      "L can fit in the bottom left corner, while Variation 7 would not fit anywhere without\n",
      "overlapping or going outside the rectangle. After placing Tetromino L ( ), the target rectangle looks like this:\n",
      "```\n",
      "(a)ResponseofVisualTiling-Page1\n",
      "25\n",
      "\n",
      "\n",
      "CHUNK : 66\n",
      "```\n",
      "Now, we have two variations for Tetromino T ( ). We need to fit this into the remaining\n",
      "empty spaces. Variation 2 of Tetromino T cannot fit because it would require two empty\n",
      "spaces next to each other vertically, which we do not have.\n",
      "\n",
      "CHUNK : 67\n",
      "However, Variation 3 of\n",
      "Tetromino T fits perfectly into the remaining space. After placing Tetromino T ( ), the target rectangle looks like this:\n",
      "```\n",
      "```\n",
      "The correct variation of Tetromino T that fits into the empty squares is:\n",
      "B. 3\n",
      "(b)ResponseofVisualTiling-Page2\n",
      "Figure14: ResponsesofVisualTiling\n",
      "26\n",
      "\n",
      "\n",
      "CHUNK : 68\n",
      "(a)ResponseofNaturalLanguageNavigation-Page1\n",
      "27\n",
      "\n",
      "\n",
      "CHUNK : 69\n",
      "(b)ResponseofNaturalLanguageNavigation-Page2\n",
      "Figure15: ResponsesofNaturalLanguageNavigation\n",
      "28\n",
      "\n",
      "\n",
      "CHUNK : 70\n",
      "(c)ResponseofNaturalLanguageNavigation-Page3\n",
      "Figure15: ResponsesofNaturalLanguageNavigation\n",
      "29\n",
      "\n",
      "\n",
      "CHUNK : 71\n",
      "First, let's look at the target rectangle and identify the empty squares ( ):\n",
      "```\n",
      "```\n",
      "We have two polyominoes to fit into this rectangle: Tetromino L ( ) and Tetromino I ( ). We need to use the variations provided to fill the empty squares without overlapping the\n",
      "existing polyominoes or going outside the rectangle. Let's start with Tetromino L ( ) because it has a unique shape that can fit into corners or\n",
      "protrusions.\n",
      "\n",
      "CHUNK : 72\n",
      "Looking at the variations of Tetromino L:\n",
      "Variation 1:\n",
      "```\n",
      "```\n",
      "Variation 6:\n",
      "```\n",
      "```\n",
      "We can see that Variation 6 of Tetromino L fits perfectly into the top left corner of the target\n",
      "rectangle, filling three empty squares vertically and turning to fill one horizontally. Let's\n",
      "place Variation 6 of Tetromino L into the target rectangle:\n",
      "```\n",
      "(a)Inconsistencybetweenlanguageandvisualization-Page1\n",
      "30\n",
      "\n",
      "\n",
      "CHUNK : 73\n",
      "```\n",
      "Now, we have four empty squares remaining in a straight line, which matches the shape of\n",
      "Tetromino I.\n",
      "\n",
      "CHUNK : 74\n",
      "Let's look at the variations of Tetromino I:\n",
      "Variation 1:\n",
      "```\n",
      "```\n",
      "Variation 2:\n",
      "```\n",
      "```\n",
      "Since the remaining empty squares are in a horizontal line, Variation 2 of Tetromino I will fit\n",
      "perfectly. Let's place Variation 2 of Tetromino I into the target rectangle:\n",
      "```\n",
      "```\n",
      "The correct variation of Tetromino I that fits into the target rectangle is:\n",
      "B. 2\n",
      "(b)Inconsistencybetweenlanguageandvisualization-Page2\n",
      "Figure16: Inconsistencybetweenlanguageandvisualization\n",
      "31\n",
      "\n",
      "\n",
      "CHUNK : 75\n",
      "Let's start by visualizing the target rectangle with the empty squares ( ) and the existing\n",
      "polyominoes:\n",
      "```\n",
      "```\n",
      "We have two polyominoes to fit into this rectangle: Tetromino L ( ) and Tetromino I ( ). We need to determine which variation of Tetromino L will fit without overlapping any\n",
      "existing polyominoes or going outside the rectangle. Let's consider the variations for Tetromino L:\n",
      "Variation 1:\n",
      "```\n",
      "```\n",
      "Variation 6:\n",
      "```\n",
      "```\n",
      "We also have the variations for Tetromino I, but since it's a straight line, it can only fit in one\n",
      "way in the remaining space after placing Tetromino L.\n",
      "\n",
      "CHUNK : 76\n",
      "Let's try to fit Variation 1 of Tetromino L first:\n",
      "```\n",
      "(a)ExamplecaseofSelf-refine-Page1\n",
      "32\n",
      "\n",
      "\n",
      "CHUNK : 77\n",
      "```\n",
      "As we can see, Variation 1 of Tetromino L does not fit because it overlaps with the existing\n",
      "yellow ( ) polyomino. Now let's try Variation 6 of Tetromino L:\n",
      "```\n",
      "```\n",
      "Variation 6 of Tetromino L does not fit either because it goes outside the rectangle. Since neither Variation 1 nor Variation 6 of Tetromino L fits into the target rectangle without\n",
      "overlapping existing polyominoes or going outside the rectangle, the correct answer is:\n",
      "C. Neither\n",
      "However, there seems to be a mistake because the task guarantees that there always\n",
      "exists a solution.\n",
      "\n",
      "CHUNK : 78\n",
      "Let's re-evaluate the placement of Tetromino L. We can see that the Tetromino L must fit in the top left corner because that's the only place\n",
      "with enough space for its shape. Let's try placing Variation 1 of Tetromino L in the top left\n",
      "corner:\n",
      "```\n",
      "(b)ExamplecaseofSelf-refine-Page2\n",
      "Figure17: ExamplecaseofSelf-refine\n",
      "33\n",
      "\n",
      "\n",
      "CHUNK : 79\n",
      "```\n",
      "This time, Variation 1 of Tetromino L fits perfectly in the top left corner. Now, we can place\n",
      "the Tetromino I in the remaining space:\n",
      "```\n",
      "```\n",
      "The Tetromino I fits perfectly in the remaining space using Variation 1 (vertical placement). Therefore, the correct variation of Tetromino L that fits into the target rectangle is:\n",
      "A. 1\n",
      "The initial attempt to place Variation 1 of Tetromino L was incorrect because it was not\n",
      "placed in the top left corner.\n",
      "\n",
      "CHUNK : 80\n",
      "Upon correcting the placement, we find that Variation 1 does\n",
      "indeed fit, and the Tetromino I can be placed afterward to complete the puzzle. (c)ExamplecaseofSelf-refine-Page3\n",
      "Figure17: ExamplecaseofSelf-refine\n",
      "34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check number of chunks created\n",
    "print(\"Number of chunks created: \", len(documents))\n",
    "\n",
    " # Printing first few chunks\n",
    "for i in range(len(documents)):\n",
    "  print()\n",
    "  print(f\"CHUNK : {i+1}\")\n",
    "  print(documents[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "rMmSdcrNrPfI"
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Instantiate the embedding model\n",
    "embedder = HuggingFaceEmbeddings()\n",
    "# Create the vector store\n",
    "vector = FAISS.from_documents(documents, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Qye96zx2rhJR"
   },
   "outputs": [],
   "source": [
    "retriever = vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ygnH7_E_sToH"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQ-mBGbIsSOv"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"API-KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xKJRU2lmztBH",
    "outputId": "1c635a45-277f-411e-c51c-afc92cdff406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "1. Use the following pieces of context to provide a detailed explanation in response to the student's query.\n",
      "2. If you don't know the answer, say \"I don't know\" without making up an answer.\n",
      "3. Make sure to break down the explanation in a clear, student-friendly manner, using simple language. Aim to help the student understand the topic better.\n",
      "Context: Context:\n",
      "content:Weusep todenoteapre-trainedLMwithparametersθ,x,y,ztodenotealanguagesequence,and\n",
      "θ\n",
      "vtodenoteavisualizationsequenceintextform. Inamulti-hopspatialreasoningtaskwithinputx,\n",
      "CoTpromptinggeneratesaseriesofintermediatestepsz ,··· ,z ,eachstepz ∼p (z |x,z )\n",
      "1 n i θ i 1···i−1\n",
      "is sampled sequentially, followed by the output y ∼ p (y|x,z ). As shown in Figure 1, VoT\n",
      "θ 1···n\n",
      "promptingenhancesthisprocessbyaddingavisuospatialsketchpadtoeachintermediatestepz ,\n",
      "i\n",
      "thenthesubsequentstepz issampledconditionedonpriorstepsz andvisualizationsv . i+1 1···i 1···i\n",
      "As defined in the Equation 5 and 6, it forms interleaved reasoning traces and visualizations. A\n",
      "qualitativecomparisonbetweenoutputsofVoTandCoTisprovidedinFigure8ainappendix. v ∼p (v |prompt ,x,z ,v ) (5)\n",
      "i θ i VoT 1···i 1···i−1\n",
      "4\n",
      "\n",
      "source:llm-research-paper.pdf\n",
      "\n",
      "Context:\n",
      "content:5 Analysis\n",
      "Asexplainedinsection3,oneofthecoreaspectsofVoTliesinenablingLLMswithvisualstate\n",
      "tracking. Duringtheexperiments,itwasobservedthatGPT-4CoToccasionallyexhibitedthisreason-\n",
      "ingpatternacrossseveraltaskswithexceptionofrouteplanning. Besides,incorrectvisualizationsof\n",
      "VoTarecommonlyobservedinmodeloutputs. Inthissection,ouranalysisofVoTprimarilyfocuses\n",
      "onthreequestions: (1)Dovisualstatetrackingbehaviorsdifferamongpromptingmethods? (2)How\n",
      "visualizationsenhancefinalanswers? (3)CanVoTbenefitlesspowerfullanguagemodels? 5.1 Dovisualstatetrackingbehaviorsdifferamongpromptingmethods? Foreachmodeloutput,weextractthesequenceofvisualizationssampledpriortogeneratingthefinal\n",
      "answeranddiscardanyvisualizationsgeneratedthereafter. Thenwecomparethesequencelengthl\n",
      "v\n",
      "withthenumberofreasoningstepsl . WecalculateComplete\n",
      "Tracking(cid:80)n(l\n",
      "==l )/nwhena\n",
      "s i v s\n",
      "visualizationv correspondstoeachstates . Similarly,wecalculatethePartial Trackingmetric\n",
      "i i\n",
      "as\n",
      "(cid:80)n(l\n",
      "> 0)/n whenat leastone visualization ispresent beforethefinal answerisgenerated. i v\n",
      "Figure 5 shows the significant differences between these settings. In the GPT-4 CoT setting, it\n",
      "demonstratednoticeabletrackingrateacrossalmostalltasksexceptrouteplanning. Thisobservation\n",
      "impliesthatLLMsinherentlyexhibitthecapabilityofvisualstatetrackingwhenspatiotemporal\n",
      "simulationisintegraltoreasoning. Ontheotherhand,thevisualstatetrackingbehaviorissensitivetopromptstovaryingdegrees. As\n",
      "showcasedinFigure8inappendix,afterremoving\"reasoning\"fromthepromptofVoT,thevisualiza-\n",
      "tionsaresampledafterGPT-4generatesthewronganswer. Consequently,explicitlypromptingLLMs\n",
      "tovisualizetheirreasoningtraceswithVoTmarkedlyimprovesthevisualtrackingrate,thereby\n",
      "enhancing overall performance. The potential contribution of code pre-training to this emergent\n",
      "capabilityisfurtherexploredinAppendixC. 100 96.2 92.9 87.1\n",
      "59.3 57.4\n",
      "50\n",
      "18.1 20\n",
      "0 1.20 5 0.50\n",
      "Route NextStep Visual Natural\n",
      "PlanningPrediction Tiling language\n",
      "Navigation\n",
      "%\n",
      "GPT-4VoT GPT-4CoT GPT-4w/oViz\n",
      "100 99.2 95.391 87.4\n",
      "80.5\n",
      "59.2\n",
      "50\n",
      "30.5\n",
      "18.3\n",
      "0 1.80 5.5 0\n",
      "Route NextStep Visual Natural\n",
      "PlanningPrediction Tiling language\n",
      "Navigation\n",
      "(a)Completetrackingrate\n",
      "%\n",
      "GPT-4VoT GPT-4CoT GPT-4w/oViz\n",
      "(b)Partialtrackingrate\n",
      "Figure5: trackingrateofdifferentsettingsacrossalltasksk. 5.2 Howvisualizationsenhancefinalanswers?\n",
      "source:llm-research-paper.pdf\n",
      "\n",
      "Context:\n",
      "content:Place L 3. Place T\n",
      "Figure4: ExamplesofVoTpromptinginthreetasks,whereLLMgenerates2Dgridsastext-form\n",
      "mentalimages. Thegeneratedreasoningtracesandvisualizationsformaninterleavedsequenceto\n",
      "trackthestateovertime. The2Dgridsintheinputandresponsesarecomposedofspecialcharacters. FullresponsescouldbefoundinAppendixB. z ∼p (z |prompt ,x,z ,v ) (6)\n",
      "i+1 θ i+1 VoT 1···i 1···i\n",
      "ThisreasoningparadigmenablesLLMswithvisualstatetracking. Weintroducetheconceptofa\n",
      "state,denotedass =[x,z ,v ]representingapartialsolutionatstepiwiththeinput,the\n",
      "i 1···i 1···i−1\n",
      "sequenceofintermediatestepsz andthesequenceofvisualizationsv . 1···i 1···i−1\n",
      "v ∼p (v |prompt ,x,z ,v )\n",
      "i θ i VoT 1···i 1···i−1\n",
      "(7)\n",
      "∼p (v |prompt ,s )\n",
      "θ i VoT i\n",
      "As shown in Equation 7, visual state tracking is implemented by generating the visualization v\n",
      "i\n",
      "asrepresentationoftheinternalstates aftereachreasoningstepz (e.g.v couldbeagridofthe\n",
      "i i i\n",
      "navigationmapmarkedwithpathorafilledrectangle).Groundedbythevisualstatetrackingsequence,\n",
      "thesubsequentstateisderivedbys ∼p (s |prompt ,x,s ,v ). Thismechanismallows\n",
      "i+1 θ i+1 VoT i i\n",
      "forthederivationofsubsequentstates,reflectingspatiotemporalcausalityandenhancingthespatial\n",
      "reasoningcapabilitiesofLLMsinagroundedcontext. 4 Experiment\n",
      "4.1 Setup\n",
      "Forthevisualtaskswhereacounterpartimageexistsforeachtextinput,weconductadditionalexper-\n",
      "imentswithamultimodalmodel. Specifically,weadoptGPT-4[OA+23]andGPT-4Vision[Ope23]\n",
      "via Azure OpenAI API as they’re state of the art LLM and multimodal model respectively. API\n",
      "settingsaretemperature0asgreedydecodingandtopp1,withmodelversionsof1106-previewand\n",
      "vision-preview. Forallexperimentsweadoptzero-shotprompting. DependingonwhethertheLLMisexplicitlypromptedtovisualizeintermediatesteps,weexperiment\n",
      "withthreesettingsofGPT-4,includingzero-shotCoTprompting(GPT-4CoT),GPT-4w/oVizwhere\n",
      "visualizationisexplicitlydisabledduringreasoning,andVoTprompting(GPT-4VoT).Additional\n",
      "settingofGPT-4VisionwithcounterpartimageinputisGPT-4VCoT.Promptsareasfollowing:\n",
      "5\n",
      "\n",
      "source:llm-research-paper.pdf\n",
      "Question: Explain VoT prompting in basic terms\n",
      "Detailed Explanation: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Imagine you're solving a puzzle, like figuring out the shortest route on a map.  You might think through the steps in your head, maybe drawing little sketches to help you visualize the path.  That's similar to what a large language model (LLM) does with \"Chain of Thought\" (CoT) prompting.  It thinks through the problem step-by-step, but it only keeps track of those steps in its internal workings – we don't see the \"sketches\".\n",
      "\n",
      "VoT (Visual Chain of Thought) prompting is like giving that LLM a sketchbook.  With VoT, as the LLM solves the problem step-by-step, it also creates little visual representations (the \"sketches\") of its reasoning process.  These visualizations are text-based, like a simple grid or diagram described in words, and are shown alongside each step of the solution.  So, we get to see the internal visualization the LLM is using, making it easier to understand how it arrived at its answer.\n",
      "\n",
      "In short, VoT improves upon CoT by adding visual \"notes\" or \"sketches\" that the LLM creates as it solves problems involving spatial reasoning (things related to location, direction, and space). These visualizations are integrated with the reasoning process, helping the LLM to track its progress and arrive at a more accurate solution.  It's like giving the LLM a visual memory to improve its spatial reasoning skills.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.schema import Generation, LLMResult, Document\n",
    "from typing import Optional, List, Any\n",
    "from pydantic import Field\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "\n",
    "# Define the GenAIWrapper\n",
    "class GenAIWrapper(BaseLLM):\n",
    "    model: Any = Field(...)  # Explicitly define the model field\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \"\"\"Initialize the wrapper with the underlying GenAI model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Simplified single-shot call method.\"\"\"\n",
    "        response = self.model.generate_text(prompt)\n",
    "        return response.text  # Ensure this returns the text output\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Handles batched prompts and returns results in LangChain's standard format.\"\"\"\n",
    "        generations = []\n",
    "        for prompt in prompts:\n",
    "            response = self.model.generate_content(prompt)  # Adjust the method name if necessary\n",
    "            generations.append(Generation(text=response.text))\n",
    "\n",
    "        # Wrap results in LangChain's LLMResult object\n",
    "        return LLMResult(generations=[generations])\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"genai\"\n",
    "\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# Wrap the model using the LangChain-compatible wrapper\n",
    "wrapped_model = GenAIWrapper(model=model)\n",
    "\n",
    "# Create prompts and chains\n",
    "prompt = \"\"\"\n",
    "1. Use the following pieces of context to provide a detailed explanation in response to the student's query.\n",
    "2. If you don't know the answer, say \"I don't know\" without making up an answer.\n",
    "3. Make sure to break down the explanation in a clear, student-friendly manner, using simple language. Aim to help the student understand the topic better.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Detailed Explanation: \"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
    "llm_chain = LLMChain(llm=wrapped_model, prompt=QA_CHAIN_PROMPT, verbose=True)\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\", \"source\"],\n",
    "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
    ")\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "qa = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Test the QA chain\n",
    "response = qa.invoke({\"query\": \"Explain VoT prompting in basic terms\"})\n",
    "print(response[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
